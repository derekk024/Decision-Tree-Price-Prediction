{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "555d3702-eb1e-4d07-8383-9cbff701d7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/derek/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import time\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#manually set data\n",
    "notebook_name = \"Prediction Overlap N Events.ipynb\"\n",
    "VIXvar = ''\n",
    "maxfeatures = 5 #'sqrt'\n",
    "GSPCvar = 'all - og'\n",
    "#all predictors - the original\n",
    "shrtcomment = ''\n",
    "NYICDXQQQvar = ''\n",
    "XLKvar = 'OC'\n",
    "benchmark = '^GSPC'\n",
    "theticker = 'IYW' #^GSPC\n",
    "predictionsnum = 'N/A'\n",
    "longcomment = ''\n",
    "\n",
    "#settings\n",
    "dt = 0.63\n",
    "iterations = 10\n",
    "number_of_estimators = 1600\n",
    "minimum_of_samples_split = 150\n",
    "startnumber = 3000\n",
    "stepnumber = 220\n",
    "startdateunformatted = '1950-01-01'\n",
    "predictiondayslists1 = []\n",
    "\n",
    "#prep\n",
    "starting_time = pd.to_datetime(startdateunformatted).date()\n",
    "today = datetime.now()\n",
    "tomorrow = today + timedelta(days=1)\n",
    "tomorrow_str = tomorrow.strftime('%Y-%m-%d')\n",
    "ending_time = tomorrow_str\n",
    "tomorrow_str\n",
    "results = []\n",
    "predictionfrequency = []\n",
    "avgpredictionchange = []\n",
    "avgpredictionchangesd = []\n",
    "avgweightedpredictionchange = []\n",
    "avgweightedpredictionchangesd = []\n",
    "avgweightedbenchmarkchange = []\n",
    "avgweightedbenchmarkchangesd = []\n",
    "pd.set_option('display.max_rows', 400)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "#data\n",
    "# Fetch S&P 500 data\n",
    "sp500 = yf.Ticker(theticker)\n",
    "sp500 = sp500.history(period=\"max\")\n",
    "\n",
    "# Ensure the index is a DatetimeIndex\n",
    "if not isinstance(sp500.index, pd.DatetimeIndex):\n",
    "    sp500.index = pd.to_datetime(sp500.index)\n",
    "\n",
    "# Normalize the dates (remove the time component)\n",
    "sp500.index = sp500.index.normalize()\n",
    "\n",
    "# Drop columns if needed\n",
    "if \"Dividends\" in sp500.columns:\n",
    "    del sp500[\"Dividends\"]\n",
    "if \"Stock Splits\" in sp500.columns:\n",
    "    del sp500[\"Stock Splits\"]\n",
    "sp500.index = sp500.index.date\n",
    "display(sp500)\n",
    "sp500[\"Tomorrow\"] = sp500[\"Close\"].shift(-1)\n",
    "######what change were predicting\n",
    "sp500[\"Target\"] = (sp500[\"Close\"] > sp500[\"Open\"]).astype(int).shift(-1)\n",
    "#####\n",
    "sp500 = sp500.loc[starting_time:].copy()\n",
    "display(sp500)\n",
    "volatility = yf.download(tickers = '^VIX', start = starting_time, end = ending_time)\n",
    "volatility = volatility[['Open', 'Adj Close', 'High', 'Low']]\n",
    "volatility = volatility.rename(columns={'Open': 'VOpen', 'Adj Close': 'VAdj Close', 'High': 'VHigh', 'Low': 'VLow'})\n",
    "volatility.index = volatility.index.normalize()\n",
    "volatility.index = volatility.index.date\n",
    "sp500 = pd.concat([sp500, volatility], axis=1)\n",
    "USD = yf.download(tickers = '^NYICDX', start = starting_time, end = ending_time)\n",
    "USD = USD[['Open', 'Adj Close', 'High', 'Low']]\n",
    "USD = USD.rename(columns={'Open': 'USDOpen', 'Adj Close': 'USDAdj Close', 'High': 'USDHigh', 'Low': 'USDLow'})\n",
    "USD.index = USD.index.normalize()\n",
    "USD.index = USD.index.date\n",
    "sp500 = pd.concat([sp500, USD], axis=1)\n",
    "#QQQ = yf.download(tickers = 'QQQ', start = starting_time, end = ending_time)\n",
    "#QQQ = QQQ[['Open', 'Adj Close', 'High', 'Low']]\n",
    "#QQQ = QQQ.rename(columns={'Open': 'QQQOpen', 'Adj Close': 'QQQAdj Close', 'High': 'QQQHigh', 'Low': 'QQQLow'})\n",
    "#QQQ.index = QQQ.index.normalize()\n",
    "#QQQ.index = QQQ.index.date\n",
    "#sp500 = pd.concat([sp500, QQQ], axis=1)\n",
    "tech = yf.download(tickers = 'XLK', start = starting_time, end = ending_time)\n",
    "tech = tech[['Open', 'Adj Close', 'High', 'Low']]\n",
    "tech = tech.rename(columns={'Open': 'techOpen', 'Adj Close': 'techAdj Close', 'High': 'techHigh', 'Low': 'techLow'})\n",
    "tech.index = tech.index.normalize()\n",
    "tech.index = tech.index.date\n",
    "sp500 = pd.concat([sp500, tech], axis=1)\n",
    "new_row = pd.DataFrame([[0] * len(sp500.columns)], columns=sp500.columns, index=[pd.to_datetime(tomorrow_str)])\n",
    "new_row.index = new_row.index.normalize()\n",
    "new_row.index = new_row.index.date\n",
    "sp500 = pd.concat([sp500, new_row])\n",
    "train = sp500.iloc[:-100]\n",
    "test = sp500.iloc[-100:]\n",
    "\n",
    "#old\n",
    "predictors = [\"Close\", \"Volume\", \"Open\", \"High\"]\n",
    "#, \"Low\", \"VOpen\", \"VAdj Close\", \"VHigh\", \"VLow\"\n",
    "#predictors = [\"Close\", \"Volume\", \"Open\", \"High\", \"Low\"]\n",
    "\n",
    "#The backtest function evaluates a model by training it on historical data \n",
    "#and testing it in increments. It iterates over the data, training the model\n",
    "#on a growing set of past data and testing it on the next segment. \n",
    "#The results are collected and combined into a single DataFrame for analysis.\n",
    "def backtest(data, model, predictors, start=startnumber, step=stepnumber):\n",
    "    all_predictions = []\n",
    "\n",
    "    for i in range(start, data.shape[0], step):\n",
    "        train = data.iloc[0:i].copy()\n",
    "        test = data.iloc[i:(i+step)].copy()\n",
    "        predictions = predict(train, test, predictors, model)\n",
    "        all_predictions.append(predictions)\n",
    "    \n",
    "    return pd.concat(all_predictions)\n",
    "\n",
    "horizons = [2,5,60,250,1000]\n",
    "new_predictors = []\n",
    "\n",
    "for horizon in horizons:\n",
    "    rolling_averages = sp500.rolling(horizon).mean()\n",
    "    \n",
    "    ratio_column = f\"Close_Ratio_{horizon}\"\n",
    "    sp500[ratio_column] = sp500[\"Close\"] / rolling_averages[\"Close\"]\n",
    "    \n",
    "    trend_column = f\"Trend_{horizon}\"\n",
    "    sp500[trend_column] = sp500.shift(1).rolling(horizon).sum()[\"Target\"]\n",
    "    \n",
    "    new_predictors+= [ratio_column, trend_column]\n",
    "\n",
    "sp500 = sp500.dropna(subset=sp500.columns[sp500.columns != \"Tomorrow\"])\n",
    "\n",
    "additional_predictors = [\"techAdj Close\", \"techOpen\"]# \"VOpen\", \"VAdj Close\"] #\"VAdj Close\"]#, \"QQQOpen\", \"QQQAdj Close\"] #[\"VAdj Close\", \"VOpen\", \"VHigh\", \"VLow\"]\n",
    "#\"USDHigh\", \"USDLow\", \"USDAdj Close\", \"USDOpen\", ,  \"techLow\",\"techHigh\", \n",
    "#additional_predictors = [\"Close\", \"Volume\", \"Open\", \"High\", \"Low\"] #[]#\n",
    "new_predictors.extend(additional_predictors)\n",
    "\n",
    "def predict(train, test, predictors, model):\n",
    "        model.fit(train[predictors], train[\"Target\"])\n",
    "        preds = model.predict_proba(test[predictors])[:,1]\n",
    "        preds[preds >=dt] = 1\n",
    "        preds[preds <dt] = 0\n",
    "        preds = pd.Series(preds, index=test.index, name=\"Predictions\")\n",
    "        combined = pd.concat([test[\"Target\"], preds], axis=1)\n",
    "        return combined\n",
    "\n",
    "start_time = time.time()\n",
    "##\n",
    "benchmarkdata = yf.Ticker(benchmark)\n",
    "benchmarkdata = benchmarkdata.history(period=\"max\")\n",
    "### the way change in what time is recorded\n",
    "benchmarkdata[\"BMChange\"] = ((benchmarkdata[\"Close\"] - benchmarkdata[\"Open\"])/benchmarkdata[\"Close\"]).shift(-1)\n",
    "###\n",
    "if not isinstance(benchmarkdata.index, pd.DatetimeIndex):\n",
    "    benchmarkdata.index = pd.to_datetime(benchmarkdata.index)\n",
    "\n",
    "benchmarkdata.index = benchmarkdata.index.normalize()\n",
    "benchmarkdata.index = benchmarkdata.index.date\n",
    "benchmarkdata = benchmarkdata.drop(benchmarkdata.index[1])\n",
    "print(benchmarkdata)\n",
    "#\n",
    "for i in range (iterations):\n",
    "    predictionchange = []\n",
    "    weightedpredictionchange = []\n",
    "    weightedbmchange = []\n",
    "    model = RandomForestClassifier(n_estimators=number_of_estimators, max_features=maxfeatures, min_samples_split=minimum_of_samples_split, random_state=i)\n",
    "    predictions = backtest(sp500, model, new_predictors)\n",
    "    #display(f\"Iteration {i + 1}:\")\n",
    "    #display(predictions)\n",
    "    #display(predictions[\"Predictions\"].value_counts())\n",
    "    #display(predictions[\"Target\"].value_counts() / predictions.shape[0])\n",
    "    #display(np.round(precision_score(predictions[\"Target\"], predictions[\"Predictions\"]), decimals=4))\n",
    "    results.append(precision_score(predictions[\"Target\"], predictions[\"Predictions\"]))\n",
    "    \n",
    "    \n",
    "    #predictions already happened what happens after is obtaining data\n",
    "    #newdata\n",
    "    spy = yf.Ticker(theticker)\n",
    "    spy = spy.history(period=\"max\")\n",
    "    #\n",
    "    if not isinstance(spy.index, pd.DatetimeIndex):\n",
    "        spy.index = pd.to_datetime(spy.index)\n",
    "        #\n",
    "    spy.index = spy.index.normalize()\n",
    "    del spy[\"Dividends\"]\n",
    "    del spy[\"Stock Splits\"]\n",
    "    del spy[\"Volume\"]\n",
    "    del spy[\"High\"]\n",
    "    del spy[\"Low\"]\n",
    "    #### what time for change\n",
    "    spy[\"Change\"] = (spy[\"Close\"] - spy[\"Open\"]).shift(-1)\n",
    "    #####\n",
    "    spy['Open']\n",
    "    del spy['Close']\n",
    "    spy.index = spy.index.date\n",
    "    spy = spy.drop(spy.index[1])\n",
    "    display(spy)\n",
    "    total_sum = predictions[\"Predictions\"].value_counts().sum()\n",
    "    preddays1 = []\n",
    "    for idx, value in predictions[\"Predictions\"].items():\n",
    "        if value != 0:\n",
    "            preddays1.append(idx)\n",
    "    predictiondayslists1.append(preddays1)\n",
    "    display(total_sum)\n",
    "    spy = spy.iloc[-total_sum:]\n",
    "    predictionfrequency.append(predictions[\"Predictions\"].sum() / len(spy))\n",
    "    \n",
    "    predictions = predictions.iloc[-total_sum:]    #???total_sum\n",
    "    change_column = spy['Change'].tolist()\n",
    "    spy_column = spy['Open'].tolist()\n",
    "    \n",
    "    #display(change_column)\n",
    "    predictions['Change'] = change_column\n",
    "    predictions['Weight'] = spy_column\n",
    "    display(predictions)\n",
    "    combination = predictions\n",
    "    plt.figure(figsize=(10, 6))  # Adjust the figure size as needed\n",
    "    plt.plot(predictions.index, predictions['Predictions'], marker='o', linestyle='-')\n",
    "    plt.title('Predictions Over Time')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Predictions')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    #display(spy)\n",
    "    def new_row(row):\n",
    "        if row['Predictions'] == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return row['Change']\n",
    "    predictions['UpdatedChange'] = predictions.apply(new_row, axis=1)\n",
    "    predictions['WeightedChange'] = predictions['UpdatedChange'] / predictions['Weight']\n",
    "    display(predictions)\n",
    "    last_200_rows = predictions.tail(400)\n",
    "    #display(last_200_rows)\n",
    "    for value in predictions['UpdatedChange']:\n",
    "        if value != 0:\n",
    "            predictionchange.append(value)    \n",
    "    avgpredictionchange.append(np.mean(predictionchange))\n",
    "    avgpredictionchangesd.append(np.std(predictionchange))\n",
    "    for value in predictions['WeightedChange']:\n",
    "        if value != 0:\n",
    "            weightedpredictionchange.append(value)    \n",
    "    avgweightedpredictionchange.append(np.mean(weightedpredictionchange))\n",
    "    avgweightedpredictionchangesd.append(np.std(weightedpredictionchange))\n",
    "    ###benchmark stuff\n",
    "    display(combination)\n",
    "    combination[\"BMChange\"] = benchmarkdata[\"BMChange\"]\n",
    "    for idx, value in predictions[\"UpdatedChange\"].items():\n",
    "        if value != 0:\n",
    "            weightedbmchange.append(predictions[\"BMChange\"].loc[idx])\n",
    "        else:\n",
    "            predictions.at[idx, \"BMChange\"] = 0\n",
    "    # Filter out the non-zero elements\n",
    "    combination[\"BMChange\"] = combination[\"WeightedChange\"] - combination[\"BMChange\"]\n",
    "    weightedbmchange = combination[\"BMChange\"][combination[\"BMChange\"] != 0]\n",
    "\n",
    "# Convert the filtered series to a list\n",
    "    weightedbmchange = weightedbmchange.tolist()\n",
    "\n",
    "    avgweightedbenchmarkchange.append(np.mean(weightedbmchange))\n",
    "    avgweightedbenchmarkchangesd.append(np.std(weightedbmchange))\n",
    "    display(combination)\n",
    "\n",
    "\n",
    "print(type(combination))\n",
    "print(type(benchmarkdata))\n",
    "print(benchmarkdata)\n",
    "\n",
    "display(results)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "display(np.round((elapsed_time / 60), decimals=2))\n",
    "display(np.round(((elapsed_time / 60) / iterations), decimals=2))\n",
    "\n",
    "display(np.round(np.mean(avgpredictionchange), decimals=2))\n",
    "display(np.round(np.mean(avgpredictionchangesd), decimals=4))\n",
    "display(np.round(np.mean(avgweightedpredictionchange), decimals=6))\n",
    "display(np.round(np.mean(avgweightedpredictionchangesd), decimals=6))\n",
    "\n",
    "display(np.round(np.mean(predictionfrequency), decimals=4))\n",
    "display(np.round(np.mean(results), decimals=4))\n",
    "display(np.round(np.std(results), decimals=4))\n",
    "\n",
    "dates_when_predictions_is_1 = predictions.index[predictions['Predictions'] == 1].tolist()\n",
    "\n",
    "#print(f\"All dates when 'predictions' is equal to 1: {dates_when_predictions_is_1}\")\n",
    "\n",
    "\n",
    "# Load the CSV file\n",
    "filename = \"/Users/derek/Downloads/DT AI Transformed Data.csv\"\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "# Define your 24 variables (ordered list of values to add)\n",
    "new_data = [\n",
    "    notebook_name, number_of_estimators, startdateunformatted, dt, VIXvar, minimum_of_samples_split,\n",
    "    maxfeatures, GSPCvar, shrtcomment, iterations, NYICDXQQQvar, XLKvar,\n",
    "    startnumber, stepnumber, theticker, np.round(np.mean(results), decimals=4), np.round(np.std(results), decimals=4), np.round(((elapsed_time / 60) / iterations), decimals=2),\n",
    "    np.round((elapsed_time / 60), decimals=2), np.round(np.mean(predictionfrequency), decimals = 4), np.round(np.mean(avgpredictionchange), decimals=2), np.round(np.mean(avgpredictionchangesd), decimals=4), np.round(np.mean(avgweightedpredictionchange), decimals=6), np.round(np.mean(avgweightedpredictionchangesd), decimals=6), np.round(np.mean(avgweightedbenchmarkchange), decimals = 6), np.round(np.mean(avgweightedbenchmarkchangesd), decimals=6), longcomment\n",
    "]\n",
    "\n",
    "# Find the first empty row\n",
    "# Here we assume that the row is considered empty if all columns are NaN or empty strings\n",
    "empty_row_index = df.index[df.isnull().all(axis=1) | (df == '').all(axis=1)].tolist()\n",
    "if empty_row_index:\n",
    "    # If there are empty rows, use the first one\n",
    "    first_empty_row = empty_row_index[0]\n",
    "else:\n",
    "    # If no empty rows, append a new row at the end\n",
    "    first_empty_row = len(df)\n",
    "\n",
    "# Ensure the length of new_data matches the number of columns\n",
    "if len(new_data) != len(df.columns):\n",
    "    raise ValueError(\"Number of new data values does not match number of columns.\")\n",
    "\n",
    "# Add the new data to the determined row\n",
    "df.loc[first_empty_row] = new_data\n",
    "\n",
    "# Save the updated DataFrame back to the CSV file\n",
    "df.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Data successfully added to row {first_empty_row + 1} of the CSV file.\")\n",
    "\n",
    "#print(f\"Number of columns in DataFrame: {len(df.columns)}\")\n",
    "#print(f\"Number of values in new_data: {len(new_data)}\")\n",
    "\n",
    "#print(benchmarkdata)\n",
    "\n",
    "#print(np.mean(avgweightedbenchmarkchange))\n",
    "#print(np.mean(avgweightedbenchmarkchangesd)) \n",
    "\n",
    "#imports\n",
    "import time\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#manually set data\n",
    "notebook_name = \"Decision Tree Testing Consistency Open Close 3.ipynb\"\n",
    "VIXvar = ''\n",
    "maxfeatures = 5 #'sqrt'\n",
    "GSPCvar = 'all - og'\n",
    "#all predictors - the original\n",
    "shrtcomment = ''\n",
    "NYICDXQQQvar = ''\n",
    "XLKvar = 'OC'\n",
    "benchmark = '^GSPC'\n",
    "theticker = 'QQQ' #^GSPC\n",
    "predictionsnum = 'N/A'\n",
    "longcomment = ''\n",
    "\n",
    "#settings\n",
    "dt = 0.6\n",
    "\n",
    "number_of_estimators = 1600\n",
    "minimum_of_samples_split = 150\n",
    "startnumber = 3000\n",
    "stepnumber = 220\n",
    "startdateunformatted = '1950-01-01'\n",
    "predictiondayslists2 = []\n",
    "\n",
    "#prep\n",
    "starting_time = pd.to_datetime(startdateunformatted).date()\n",
    "today = datetime.now()\n",
    "tomorrow = today + timedelta(days=1)\n",
    "tomorrow_str = tomorrow.strftime('%Y-%m-%d')\n",
    "ending_time = tomorrow_str\n",
    "tomorrow_str\n",
    "results = []\n",
    "predictionfrequency = []\n",
    "avgpredictionchange = []\n",
    "avgpredictionchangesd = []\n",
    "avgweightedpredictionchange = []\n",
    "avgweightedpredictionchangesd = []\n",
    "avgweightedbenchmarkchange = []\n",
    "avgweightedbenchmarkchangesd = []\n",
    "pd.set_option('display.max_rows', 400)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "#data\n",
    "# Fetch S&P 500 data\n",
    "sp500 = yf.Ticker(theticker)\n",
    "sp500 = sp500.history(period=\"max\")\n",
    "\n",
    "# Ensure the index is a DatetimeIndex\n",
    "if not isinstance(sp500.index, pd.DatetimeIndex):\n",
    "    sp500.index = pd.to_datetime(sp500.index)\n",
    "\n",
    "# Normalize the dates (remove the time component)\n",
    "sp500.index = sp500.index.normalize()\n",
    "\n",
    "# Drop columns if needed\n",
    "if \"Dividends\" in sp500.columns:\n",
    "    del sp500[\"Dividends\"]\n",
    "if \"Stock Splits\" in sp500.columns:\n",
    "    del sp500[\"Stock Splits\"]\n",
    "sp500.index = sp500.index.date\n",
    "display(sp500)\n",
    "sp500[\"Tomorrow\"] = sp500[\"Close\"].shift(-1)\n",
    "######what change were predicting\n",
    "sp500[\"Target\"] = (sp500[\"Close\"] > sp500[\"Open\"]).astype(int).shift(-1)\n",
    "#####\n",
    "sp500 = sp500.loc[starting_time:].copy()\n",
    "display(sp500)\n",
    "volatility = yf.download(tickers = '^VIX', start = starting_time, end = ending_time)\n",
    "volatility = volatility[['Open', 'Adj Close', 'High', 'Low']]\n",
    "volatility = volatility.rename(columns={'Open': 'VOpen', 'Adj Close': 'VAdj Close', 'High': 'VHigh', 'Low': 'VLow'})\n",
    "volatility.index = volatility.index.normalize()\n",
    "volatility.index = volatility.index.date\n",
    "sp500 = pd.concat([sp500, volatility], axis=1)\n",
    "USD = yf.download(tickers = '^NYICDX', start = starting_time, end = ending_time)\n",
    "USD = USD[['Open', 'Adj Close', 'High', 'Low']]\n",
    "USD = USD.rename(columns={'Open': 'USDOpen', 'Adj Close': 'USDAdj Close', 'High': 'USDHigh', 'Low': 'USDLow'})\n",
    "USD.index = USD.index.normalize()\n",
    "USD.index = USD.index.date\n",
    "sp500 = pd.concat([sp500, USD], axis=1)\n",
    "#QQQ = yf.download(tickers = 'QQQ', start = starting_time, end = ending_time)\n",
    "#QQQ = QQQ[['Open', 'Adj Close', 'High', 'Low']]\n",
    "#QQQ = QQQ.rename(columns={'Open': 'QQQOpen', 'Adj Close': 'QQQAdj Close', 'High': 'QQQHigh', 'Low': 'QQQLow'})\n",
    "#QQQ.index = QQQ.index.normalize()\n",
    "#QQQ.index = QQQ.index.date\n",
    "#sp500 = pd.concat([sp500, QQQ], axis=1)\n",
    "tech = yf.download(tickers = 'XLK', start = starting_time, end = ending_time)\n",
    "tech = tech[['Open', 'Adj Close', 'High', 'Low']]\n",
    "tech = tech.rename(columns={'Open': 'techOpen', 'Adj Close': 'techAdj Close', 'High': 'techHigh', 'Low': 'techLow'})\n",
    "tech.index = tech.index.normalize()\n",
    "tech.index = tech.index.date\n",
    "sp500 = pd.concat([sp500, tech], axis=1)\n",
    "new_row = pd.DataFrame([[0] * len(sp500.columns)], columns=sp500.columns, index=[pd.to_datetime(tomorrow_str)])\n",
    "new_row.index = new_row.index.normalize()\n",
    "new_row.index = new_row.index.date\n",
    "sp500 = pd.concat([sp500, new_row])\n",
    "train = sp500.iloc[:-100]\n",
    "test = sp500.iloc[-100:]\n",
    "\n",
    "#old\n",
    "predictors = [\"Close\", \"Volume\", \"Open\", \"High\"]\n",
    "#, \"Low\", \"VOpen\", \"VAdj Close\", \"VHigh\", \"VLow\"\n",
    "#predictors = [\"Close\", \"Volume\", \"Open\", \"High\", \"Low\"]\n",
    "\n",
    "#The backtest function evaluates a model by training it on historical data \n",
    "#and testing it in increments. It iterates over the data, training the model\n",
    "#on a growing set of past data and testing it on the next segment. \n",
    "#The results are collected and combined into a single DataFrame for analysis.\n",
    "def backtest(data, model, predictors, start=startnumber, step=stepnumber):\n",
    "    all_predictions = []\n",
    "\n",
    "    for i in range(start, data.shape[0], step):\n",
    "        train = data.iloc[0:i].copy()\n",
    "        test = data.iloc[i:(i+step)].copy()\n",
    "        predictions = predict(train, test, predictors, model)\n",
    "        all_predictions.append(predictions)\n",
    "    \n",
    "    return pd.concat(all_predictions)\n",
    "\n",
    "horizons = [2,5,60,250,1000]\n",
    "new_predictors = []\n",
    "\n",
    "for horizon in horizons:\n",
    "    rolling_averages = sp500.rolling(horizon).mean()\n",
    "    \n",
    "    ratio_column = f\"Close_Ratio_{horizon}\"\n",
    "    sp500[ratio_column] = sp500[\"Close\"] / rolling_averages[\"Close\"]\n",
    "    \n",
    "    trend_column = f\"Trend_{horizon}\"\n",
    "    sp500[trend_column] = sp500.shift(1).rolling(horizon).sum()[\"Target\"]\n",
    "    \n",
    "    new_predictors+= [ratio_column, trend_column]\n",
    "\n",
    "sp500 = sp500.dropna(subset=sp500.columns[sp500.columns != \"Tomorrow\"])\n",
    "\n",
    "additional_predictors = [\"techAdj Close\", \"techOpen\"]# \"VOpen\", \"VAdj Close\"] #\"VAdj Close\"]#, \"QQQOpen\", \"QQQAdj Close\"] #[\"VAdj Close\", \"VOpen\", \"VHigh\", \"VLow\"]\n",
    "#\"USDHigh\", \"USDLow\", \"USDAdj Close\", \"USDOpen\", ,  \"techLow\",\"techHigh\", \n",
    "#additional_predictors = [\"Close\", \"Volume\", \"Open\", \"High\", \"Low\"] #[]#\n",
    "new_predictors.extend(additional_predictors)\n",
    "\n",
    "def predict(train, test, predictors, model):\n",
    "        model.fit(train[predictors], train[\"Target\"])\n",
    "        preds = model.predict_proba(test[predictors])[:,1]\n",
    "        preds[preds >=dt] = 1\n",
    "        preds[preds <dt] = 0\n",
    "        preds = pd.Series(preds, index=test.index, name=\"Predictions\")\n",
    "        combined = pd.concat([test[\"Target\"], preds], axis=1)\n",
    "        return combined\n",
    "\n",
    "start_time = time.time()\n",
    "##\n",
    "benchmarkdata = yf.Ticker(benchmark)\n",
    "benchmarkdata = benchmarkdata.history(period=\"max\")\n",
    "### the way change in what time is recorded\n",
    "benchmarkdata[\"BMChange\"] = ((benchmarkdata[\"Close\"] - benchmarkdata[\"Open\"])/benchmarkdata[\"Close\"]).shift(-1)\n",
    "###\n",
    "if not isinstance(benchmarkdata.index, pd.DatetimeIndex):\n",
    "    benchmarkdata.index = pd.to_datetime(benchmarkdata.index)\n",
    "\n",
    "benchmarkdata.index = benchmarkdata.index.normalize()\n",
    "benchmarkdata.index = benchmarkdata.index.date\n",
    "benchmarkdata = benchmarkdata.drop(benchmarkdata.index[1])\n",
    "print(benchmarkdata)\n",
    "#\n",
    "for i in range (iterations):\n",
    "    predictionchange = []\n",
    "    weightedpredictionchange = []\n",
    "    weightedbmchange = []\n",
    "    model = RandomForestClassifier(n_estimators=number_of_estimators, max_features=maxfeatures, min_samples_split=minimum_of_samples_split, random_state=i)\n",
    "    predictions = backtest(sp500, model, new_predictors)\n",
    "    #display(f\"Iteration {i + 1}:\")\n",
    "    #display(predictions)\n",
    "    #display(predictions[\"Predictions\"].value_counts())\n",
    "    #display(predictions[\"Target\"].value_counts() / predictions.shape[0])\n",
    "    #display(np.round(precision_score(predictions[\"Target\"], predictions[\"Predictions\"]), decimals=4))\n",
    "    results.append(precision_score(predictions[\"Target\"], predictions[\"Predictions\"]))\n",
    "    \n",
    "    \n",
    "    #predictions already happened what happens after is obtaining data\n",
    "    #newdata\n",
    "    spy = yf.Ticker(theticker)\n",
    "    spy = spy.history(period=\"max\")\n",
    "    #\n",
    "    if not isinstance(spy.index, pd.DatetimeIndex):\n",
    "        spy.index = pd.to_datetime(spy.index)\n",
    "        #\n",
    "    spy.index = spy.index.normalize()\n",
    "    del spy[\"Dividends\"]\n",
    "    del spy[\"Stock Splits\"]\n",
    "    del spy[\"Volume\"]\n",
    "    del spy[\"High\"]\n",
    "    del spy[\"Low\"]\n",
    "    #### what time for change\n",
    "    spy[\"Change\"] = (spy[\"Close\"] - spy[\"Open\"]).shift(-1)\n",
    "    #####\n",
    "    spy['Open']\n",
    "    del spy['Close']\n",
    "    spy.index = spy.index.date\n",
    "    spy = spy.drop(spy.index[1])\n",
    "    display(spy)\n",
    "    total_sum = predictions[\"Predictions\"].value_counts().sum()\n",
    "    preddays2 = []\n",
    "    for idx, value in predictions[\"Predictions\"].items():\n",
    "        if value != 0:\n",
    "            preddays2.append(idx)\n",
    "    predictiondayslists2.append(preddays2)\n",
    "    display(total_sum)\n",
    "    spy = spy.iloc[-total_sum:]\n",
    "    predictionfrequency.append(predictions[\"Predictions\"].sum() / len(spy))\n",
    "    \n",
    "    predictions = predictions.iloc[-total_sum:]    #???total_sum\n",
    "    change_column = spy['Change'].tolist()\n",
    "    spy_column = spy['Open'].tolist()\n",
    "    \n",
    "    #display(change_column)\n",
    "    predictions['Change'] = change_column\n",
    "    predictions['Weight'] = spy_column\n",
    "    display(predictions)\n",
    "    combination = predictions\n",
    "    plt.figure(figsize=(10, 6))  # Adjust the figure size as needed\n",
    "    plt.plot(predictions.index, predictions['Predictions'], marker='o', linestyle='-')\n",
    "    plt.title('Predictions Over Time')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Predictions')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    #display(spy)\n",
    "    def new_row(row):\n",
    "        if row['Predictions'] == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return row['Change']\n",
    "    predictions['UpdatedChange'] = predictions.apply(new_row, axis=1)\n",
    "    predictions['WeightedChange'] = predictions['UpdatedChange'] / predictions['Weight']\n",
    "    display(predictions)\n",
    "    last_200_rows = predictions.tail(400)\n",
    "    #display(last_200_rows)\n",
    "    for value in predictions['UpdatedChange']:\n",
    "        if value != 0:\n",
    "            predictionchange.append(value)    \n",
    "    avgpredictionchange.append(np.mean(predictionchange))\n",
    "    avgpredictionchangesd.append(np.std(predictionchange))\n",
    "    for value in predictions['WeightedChange']:\n",
    "        if value != 0:\n",
    "            weightedpredictionchange.append(value)    \n",
    "    avgweightedpredictionchange.append(np.mean(weightedpredictionchange))\n",
    "    avgweightedpredictionchangesd.append(np.std(weightedpredictionchange))\n",
    "    ###benchmark stuff\n",
    "    display(combination)\n",
    "    combination[\"BMChange\"] = benchmarkdata[\"BMChange\"]\n",
    "    for idx, value in predictions[\"UpdatedChange\"].items():\n",
    "        if value != 0:\n",
    "            weightedbmchange.append(predictions[\"BMChange\"].loc[idx])\n",
    "        else:\n",
    "            predictions.at[idx, \"BMChange\"] = 0\n",
    "    # Filter out the non-zero elements\n",
    "    combination[\"BMChange\"] = combination[\"WeightedChange\"] - combination[\"BMChange\"]\n",
    "    weightedbmchange = combination[\"BMChange\"][combination[\"BMChange\"] != 0]\n",
    "\n",
    "# Convert the filtered series to a list\n",
    "    weightedbmchange = weightedbmchange.tolist()\n",
    "\n",
    "    avgweightedbenchmarkchange.append(np.mean(weightedbmchange))\n",
    "    avgweightedbenchmarkchangesd.append(np.std(weightedbmchange))\n",
    "    display(combination)\n",
    "\n",
    "\n",
    "print(type(combination))\n",
    "print(type(benchmarkdata))\n",
    "print(benchmarkdata)\n",
    "\n",
    "display(results)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "display(np.round((elapsed_time / 60), decimals=2))\n",
    "display(np.round(((elapsed_time / 60) / iterations), decimals=2))\n",
    "\n",
    "display(np.round(np.mean(avgpredictionchange), decimals=2))\n",
    "display(np.round(np.mean(avgpredictionchangesd), decimals=4))\n",
    "display(np.round(np.mean(avgweightedpredictionchange), decimals=6))\n",
    "display(np.round(np.mean(avgweightedpredictionchangesd), decimals=6))\n",
    "\n",
    "display(np.round(np.mean(predictionfrequency), decimals=4))\n",
    "display(np.round(np.mean(results), decimals=4))\n",
    "display(np.round(np.std(results), decimals=4))\n",
    "\n",
    "dates_when_predictions_is_1 = predictions.index[predictions['Predictions'] == 1].tolist()\n",
    "\n",
    "#print(f\"All dates when 'predictions' is equal to 1: {dates_when_predictions_is_1}\")\n",
    "\n",
    "\n",
    "# Load the CSV file\n",
    "filename = \"/Users/derek/Downloads/DT AI Transformed Data.csv\"\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "# Define your 24 variables (ordered list of values to add)\n",
    "new_data = [\n",
    "    notebook_name, number_of_estimators, startdateunformatted, dt, VIXvar, minimum_of_samples_split,\n",
    "    maxfeatures, GSPCvar, shrtcomment, iterations, NYICDXQQQvar, XLKvar,\n",
    "    startnumber, stepnumber, theticker, np.round(np.mean(results), decimals=4), np.round(np.std(results), decimals=4), np.round(((elapsed_time / 60) / iterations), decimals=2),\n",
    "    np.round((elapsed_time / 60), decimals=2), np.round(np.mean(predictionfrequency), decimals = 4), np.round(np.mean(avgpredictionchange), decimals=2), np.round(np.mean(avgpredictionchangesd), decimals=4), np.round(np.mean(avgweightedpredictionchange), decimals=6), np.round(np.mean(avgweightedpredictionchangesd), decimals=6), np.round(np.mean(avgweightedbenchmarkchange), decimals = 6), np.round(np.mean(avgweightedbenchmarkchangesd), decimals=6), longcomment\n",
    "]\n",
    "\n",
    "# Find the first empty row\n",
    "# Here we assume that the row is considered empty if all columns are NaN or empty strings\n",
    "empty_row_index = df.index[df.isnull().all(axis=1) | (df == '').all(axis=1)].tolist()\n",
    "if empty_row_index:\n",
    "    # If there are empty rows, use the first one\n",
    "    first_empty_row = empty_row_index[0]\n",
    "else:\n",
    "    # If no empty rows, append a new row at the end\n",
    "    first_empty_row = len(df)\n",
    "\n",
    "# Ensure the length of new_data matches the number of columns\n",
    "if len(new_data) != len(df.columns):\n",
    "    raise ValueError(\"Number of new data values does not match number of columns.\")\n",
    "\n",
    "# Add the new data to the determined row\n",
    "df.loc[first_empty_row] = new_data\n",
    "\n",
    "# Save the updated DataFrame back to the CSV file\n",
    "df.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Data successfully added to row {first_empty_row + 1} of the CSV file.\")\n",
    "\n",
    "#print(f\"Number of columns in DataFrame: {len(df.columns)}\")\n",
    "#print(f\"Number of values in new_data: {len(new_data)}\")\n",
    "\n",
    "#print(benchmarkdata)\n",
    "\n",
    "#print(np.mean(avgweightedbenchmarkchange))\n",
    "#print(np.mean(avgweightedbenchmarkchangesd)) \n",
    "\n",
    "#imports\n",
    "import time\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#manually set data\n",
    "notebook_name = \"Decision Tree Testing Consistency Open Close 3.ipynb\"\n",
    "VIXvar = ''\n",
    "maxfeatures = 5 #'sqrt'\n",
    "GSPCvar = 'all - og'\n",
    "#all predictors - the original\n",
    "shrtcomment = ''\n",
    "NYICDXQQQvar = ''\n",
    "XLKvar = 'OC'\n",
    "benchmark = '^GSPC'\n",
    "theticker = 'VGT' #^GSPC\n",
    "predictionsnum = 'N/A'\n",
    "longcomment = ''\n",
    "\n",
    "#settings\n",
    "dt = 0.625\n",
    "\n",
    "number_of_estimators = 1600\n",
    "minimum_of_samples_split = 150\n",
    "startnumber = 3000\n",
    "stepnumber = 220\n",
    "startdateunformatted = '1950-01-01'\n",
    "predictiondayslists3 = []\n",
    "\n",
    "#prep\n",
    "starting_time = pd.to_datetime(startdateunformatted).date()\n",
    "today = datetime.now()\n",
    "tomorrow = today + timedelta(days=1)\n",
    "tomorrow_str = tomorrow.strftime('%Y-%m-%d')\n",
    "ending_time = tomorrow_str\n",
    "tomorrow_str\n",
    "results = []\n",
    "predictionfrequency = []\n",
    "avgpredictionchange = []\n",
    "avgpredictionchangesd = []\n",
    "avgweightedpredictionchange = []\n",
    "avgweightedpredictionchangesd = []\n",
    "avgweightedbenchmarkchange = []\n",
    "avgweightedbenchmarkchangesd = []\n",
    "pd.set_option('display.max_rows', 400)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "#data\n",
    "# Fetch S&P 500 data\n",
    "sp500 = yf.Ticker(theticker)\n",
    "sp500 = sp500.history(period=\"max\")\n",
    "\n",
    "# Ensure the index is a DatetimeIndex\n",
    "if not isinstance(sp500.index, pd.DatetimeIndex):\n",
    "    sp500.index = pd.to_datetime(sp500.index)\n",
    "\n",
    "# Normalize the dates (remove the time component)\n",
    "sp500.index = sp500.index.normalize()\n",
    "\n",
    "# Drop columns if needed\n",
    "if \"Dividends\" in sp500.columns:\n",
    "    del sp500[\"Dividends\"]\n",
    "if \"Stock Splits\" in sp500.columns:\n",
    "    del sp500[\"Stock Splits\"]\n",
    "sp500.index = sp500.index.date\n",
    "display(sp500)\n",
    "sp500[\"Tomorrow\"] = sp500[\"Close\"].shift(-1)\n",
    "######what change were predicting\n",
    "sp500[\"Target\"] = (sp500[\"Close\"] > sp500[\"Open\"]).astype(int).shift(-1)\n",
    "#####\n",
    "sp500 = sp500.loc[starting_time:].copy()\n",
    "display(sp500)\n",
    "volatility = yf.download(tickers = '^VIX', start = starting_time, end = ending_time)\n",
    "volatility = volatility[['Open', 'Adj Close', 'High', 'Low']]\n",
    "volatility = volatility.rename(columns={'Open': 'VOpen', 'Adj Close': 'VAdj Close', 'High': 'VHigh', 'Low': 'VLow'})\n",
    "volatility.index = volatility.index.normalize()\n",
    "volatility.index = volatility.index.date\n",
    "sp500 = pd.concat([sp500, volatility], axis=1)\n",
    "USD = yf.download(tickers = '^NYICDX', start = starting_time, end = ending_time)\n",
    "USD = USD[['Open', 'Adj Close', 'High', 'Low']]\n",
    "USD = USD.rename(columns={'Open': 'USDOpen', 'Adj Close': 'USDAdj Close', 'High': 'USDHigh', 'Low': 'USDLow'})\n",
    "USD.index = USD.index.normalize()\n",
    "USD.index = USD.index.date\n",
    "sp500 = pd.concat([sp500, USD], axis=1)\n",
    "#QQQ = yf.download(tickers = 'QQQ', start = starting_time, end = ending_time)\n",
    "#QQQ = QQQ[['Open', 'Adj Close', 'High', 'Low']]\n",
    "#QQQ = QQQ.rename(columns={'Open': 'QQQOpen', 'Adj Close': 'QQQAdj Close', 'High': 'QQQHigh', 'Low': 'QQQLow'})\n",
    "#QQQ.index = QQQ.index.normalize()\n",
    "#QQQ.index = QQQ.index.date\n",
    "#sp500 = pd.concat([sp500, QQQ], axis=1)\n",
    "tech = yf.download(tickers = 'XLK', start = starting_time, end = ending_time)\n",
    "tech = tech[['Open', 'Adj Close', 'High', 'Low']]\n",
    "tech = tech.rename(columns={'Open': 'techOpen', 'Adj Close': 'techAdj Close', 'High': 'techHigh', 'Low': 'techLow'})\n",
    "tech.index = tech.index.normalize()\n",
    "tech.index = tech.index.date\n",
    "sp500 = pd.concat([sp500, tech], axis=1)\n",
    "new_row = pd.DataFrame([[0] * len(sp500.columns)], columns=sp500.columns, index=[pd.to_datetime(tomorrow_str)])\n",
    "new_row.index = new_row.index.normalize()\n",
    "new_row.index = new_row.index.date\n",
    "sp500 = pd.concat([sp500, new_row])\n",
    "train = sp500.iloc[:-100]\n",
    "test = sp500.iloc[-100:]\n",
    "\n",
    "#old\n",
    "predictors = [\"Close\", \"Volume\", \"Open\", \"High\"]\n",
    "#, \"Low\", \"VOpen\", \"VAdj Close\", \"VHigh\", \"VLow\"\n",
    "#predictors = [\"Close\", \"Volume\", \"Open\", \"High\", \"Low\"]\n",
    "\n",
    "#The backtest function evaluates a model by training it on historical data \n",
    "#and testing it in increments. It iterates over the data, training the model\n",
    "#on a growing set of past data and testing it on the next segment. \n",
    "#The results are collected and combined into a single DataFrame for analysis.\n",
    "def backtest(data, model, predictors, start=startnumber, step=stepnumber):\n",
    "    all_predictions = []\n",
    "\n",
    "    for i in range(start, data.shape[0], step):\n",
    "        train = data.iloc[0:i].copy()\n",
    "        test = data.iloc[i:(i+step)].copy()\n",
    "        predictions = predict(train, test, predictors, model)\n",
    "        all_predictions.append(predictions)\n",
    "    \n",
    "    return pd.concat(all_predictions)\n",
    "\n",
    "horizons = [2,5,60,250,1000]\n",
    "new_predictors = []\n",
    "\n",
    "for horizon in horizons:\n",
    "    rolling_averages = sp500.rolling(horizon).mean()\n",
    "    \n",
    "    ratio_column = f\"Close_Ratio_{horizon}\"\n",
    "    sp500[ratio_column] = sp500[\"Close\"] / rolling_averages[\"Close\"]\n",
    "    \n",
    "    trend_column = f\"Trend_{horizon}\"\n",
    "    sp500[trend_column] = sp500.shift(1).rolling(horizon).sum()[\"Target\"]\n",
    "    \n",
    "    new_predictors+= [ratio_column, trend_column]\n",
    "\n",
    "sp500 = sp500.dropna(subset=sp500.columns[sp500.columns != \"Tomorrow\"])\n",
    "\n",
    "additional_predictors = [\"techAdj Close\", \"techOpen\"]# \"VOpen\", \"VAdj Close\"] #\"VAdj Close\"]#, \"QQQOpen\", \"QQQAdj Close\"] #[\"VAdj Close\", \"VOpen\", \"VHigh\", \"VLow\"]\n",
    "#\"USDHigh\", \"USDLow\", \"USDAdj Close\", \"USDOpen\", ,  \"techLow\",\"techHigh\", \n",
    "#additional_predictors = [\"Close\", \"Volume\", \"Open\", \"High\", \"Low\"] #[]#\n",
    "new_predictors.extend(additional_predictors)\n",
    "\n",
    "def predict(train, test, predictors, model):\n",
    "        model.fit(train[predictors], train[\"Target\"])\n",
    "        preds = model.predict_proba(test[predictors])[:,1]\n",
    "        preds[preds >=dt] = 1\n",
    "        preds[preds <dt] = 0\n",
    "        preds = pd.Series(preds, index=test.index, name=\"Predictions\")\n",
    "        combined = pd.concat([test[\"Target\"], preds], axis=1)\n",
    "        return combined\n",
    "\n",
    "start_time = time.time()\n",
    "##\n",
    "benchmarkdata = yf.Ticker(benchmark)\n",
    "benchmarkdata = benchmarkdata.history(period=\"max\")\n",
    "### the way change in what time is recorded\n",
    "benchmarkdata[\"BMChange\"] = ((benchmarkdata[\"Close\"] - benchmarkdata[\"Open\"])/benchmarkdata[\"Close\"]).shift(-1)\n",
    "###\n",
    "if not isinstance(benchmarkdata.index, pd.DatetimeIndex):\n",
    "    benchmarkdata.index = pd.to_datetime(benchmarkdata.index)\n",
    "\n",
    "benchmarkdata.index = benchmarkdata.index.normalize()\n",
    "benchmarkdata.index = benchmarkdata.index.date\n",
    "benchmarkdata = benchmarkdata.drop(benchmarkdata.index[1])\n",
    "print(benchmarkdata)\n",
    "#\n",
    "for i in range (iterations):\n",
    "    predictionchange = []\n",
    "    weightedpredictionchange = []\n",
    "    weightedbmchange = []\n",
    "    model = RandomForestClassifier(n_estimators=number_of_estimators, max_features=maxfeatures, min_samples_split=minimum_of_samples_split, random_state=i)\n",
    "    predictions = backtest(sp500, model, new_predictors)\n",
    "    #display(f\"Iteration {i + 1}:\")\n",
    "    #display(predictions)\n",
    "    #display(predictions[\"Predictions\"].value_counts())\n",
    "    #display(predictions[\"Target\"].value_counts() / predictions.shape[0])\n",
    "    #display(np.round(precision_score(predictions[\"Target\"], predictions[\"Predictions\"]), decimals=4))\n",
    "    results.append(precision_score(predictions[\"Target\"], predictions[\"Predictions\"]))\n",
    "    \n",
    "    \n",
    "    #predictions already happened what happens after is obtaining data\n",
    "    #newdata\n",
    "    spy = yf.Ticker(theticker)\n",
    "    spy = spy.history(period=\"max\")\n",
    "    #\n",
    "    if not isinstance(spy.index, pd.DatetimeIndex):\n",
    "        spy.index = pd.to_datetime(spy.index)\n",
    "        #\n",
    "    spy.index = spy.index.normalize()\n",
    "    del spy[\"Dividends\"]\n",
    "    del spy[\"Stock Splits\"]\n",
    "    del spy[\"Volume\"]\n",
    "    del spy[\"High\"]\n",
    "    del spy[\"Low\"]\n",
    "    #### what time for change\n",
    "    spy[\"Change\"] = (spy[\"Close\"] - spy[\"Open\"]).shift(-1)\n",
    "    #####\n",
    "    spy['Open']\n",
    "    del spy['Close']\n",
    "    spy.index = spy.index.date\n",
    "    spy = spy.drop(spy.index[1])\n",
    "    display(spy)\n",
    "    total_sum = predictions[\"Predictions\"].value_counts().sum()\n",
    "    preddays3 = []\n",
    "    for idx, value in predictions[\"Predictions\"].items():\n",
    "        if value != 0:\n",
    "            preddays3.append(idx)\n",
    "    predictiondayslists3.append(preddays3)\n",
    "    preddays3 = []\n",
    "    display(total_sum)\n",
    "    spy = spy.iloc[-total_sum:]\n",
    "    predictionfrequency.append(predictions[\"Predictions\"].sum() / len(spy))\n",
    "    \n",
    "    predictions = predictions.iloc[-total_sum:]    #???total_sum\n",
    "    change_column = spy['Change'].tolist()\n",
    "    spy_column = spy['Open'].tolist()\n",
    "    \n",
    "    #display(change_column)\n",
    "    predictions['Change'] = change_column\n",
    "    predictions['Weight'] = spy_column\n",
    "    display(predictions)\n",
    "    combination = predictions\n",
    "    plt.figure(figsize=(10, 6))  # Adjust the figure size as needed\n",
    "    plt.plot(predictions.index, predictions['Predictions'], marker='o', linestyle='-')\n",
    "    plt.title('Predictions Over Time')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Predictions')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    #display(spy)\n",
    "    def new_row(row):\n",
    "        if row['Predictions'] == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return row['Change']\n",
    "    predictions['UpdatedChange'] = predictions.apply(new_row, axis=1)\n",
    "    predictions['WeightedChange'] = predictions['UpdatedChange'] / predictions['Weight']\n",
    "    display(predictions)\n",
    "    last_200_rows = predictions.tail(400)\n",
    "    #display(last_200_rows)\n",
    "    for value in predictions['UpdatedChange']:\n",
    "        if value != 0:\n",
    "            predictionchange.append(value)    \n",
    "    avgpredictionchange.append(np.mean(predictionchange))\n",
    "    avgpredictionchangesd.append(np.std(predictionchange))\n",
    "    for value in predictions['WeightedChange']:\n",
    "        if value != 0:\n",
    "            weightedpredictionchange.append(value)    \n",
    "    avgweightedpredictionchange.append(np.mean(weightedpredictionchange))\n",
    "    avgweightedpredictionchangesd.append(np.std(weightedpredictionchange))\n",
    "    ###benchmark stuff\n",
    "    display(combination)\n",
    "    combination[\"BMChange\"] = benchmarkdata[\"BMChange\"]\n",
    "    for idx, value in predictions[\"UpdatedChange\"].items():\n",
    "        if value != 0:\n",
    "            weightedbmchange.append(predictions[\"BMChange\"].loc[idx])\n",
    "        else:\n",
    "            predictions.at[idx, \"BMChange\"] = 0\n",
    "    # Filter out the non-zero elements\n",
    "    combination[\"BMChange\"] = combination[\"WeightedChange\"] - combination[\"BMChange\"]\n",
    "    weightedbmchange = combination[\"BMChange\"][combination[\"BMChange\"] != 0]\n",
    "\n",
    "# Convert the filtered series to a list\n",
    "    weightedbmchange = weightedbmchange.tolist()\n",
    "\n",
    "    avgweightedbenchmarkchange.append(np.mean(weightedbmchange))\n",
    "    avgweightedbenchmarkchangesd.append(np.std(weightedbmchange))\n",
    "    display(combination)\n",
    "\n",
    "\n",
    "print(type(combination))\n",
    "print(type(benchmarkdata))\n",
    "print(benchmarkdata)\n",
    "\n",
    "display(results)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "display(np.round((elapsed_time / 60), decimals=2))\n",
    "display(np.round(((elapsed_time / 60) / iterations), decimals=2))\n",
    "\n",
    "display(np.round(np.mean(avgpredictionchange), decimals=2))\n",
    "display(np.round(np.mean(avgpredictionchangesd), decimals=4))\n",
    "display(np.round(np.mean(avgweightedpredictionchange), decimals=6))\n",
    "display(np.round(np.mean(avgweightedpredictionchangesd), decimals=6))\n",
    "\n",
    "display(np.round(np.mean(predictionfrequency), decimals=4))\n",
    "display(np.round(np.mean(results), decimals=4))\n",
    "display(np.round(np.std(results), decimals=4))\n",
    "\n",
    "dates_when_predictions_is_1 = predictions.index[predictions['Predictions'] == 1].tolist()\n",
    "\n",
    "#print(f\"All dates when 'predictions' is equal to 1: {dates_when_predictions_is_1}\")\n",
    "\n",
    "\n",
    "# Load the CSV file\n",
    "filename = \"/Users/derek/Downloads/DT AI Transformed Data.csv\"\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "# Define your 24 variables (ordered list of values to add)\n",
    "new_data = [\n",
    "    notebook_name, number_of_estimators, startdateunformatted, dt, VIXvar, minimum_of_samples_split,\n",
    "    maxfeatures, GSPCvar, shrtcomment, iterations, NYICDXQQQvar, XLKvar,\n",
    "    startnumber, stepnumber, theticker, np.round(np.mean(results), decimals=4), np.round(np.std(results), decimals=4), np.round(((elapsed_time / 60) / iterations), decimals=2),\n",
    "    np.round((elapsed_time / 60), decimals=2), np.round(np.mean(predictionfrequency), decimals = 4), np.round(np.mean(avgpredictionchange), decimals=2), np.round(np.mean(avgpredictionchangesd), decimals=4), np.round(np.mean(avgweightedpredictionchange), decimals=6), np.round(np.mean(avgweightedpredictionchangesd), decimals=6), np.round(np.mean(avgweightedbenchmarkchange), decimals = 6), np.round(np.mean(avgweightedbenchmarkchangesd), decimals=6), longcomment\n",
    "]\n",
    "\n",
    "# Find the first empty row\n",
    "# Here we assume that the row is considered empty if all columns are NaN or empty strings\n",
    "empty_row_index = df.index[df.isnull().all(axis=1) | (df == '').all(axis=1)].tolist()\n",
    "if empty_row_index:\n",
    "    # If there are empty rows, use the first one\n",
    "    first_empty_row = empty_row_index[0]\n",
    "else:\n",
    "    # If no empty rows, append a new row at the end\n",
    "    first_empty_row = len(df)\n",
    "\n",
    "# Ensure the length of new_data matches the number of columns\n",
    "if len(new_data) != len(df.columns):\n",
    "    raise ValueError(\"Number of new data values does not match number of columns.\")\n",
    "\n",
    "# Add the new data to the determined row\n",
    "df.loc[first_empty_row] = new_data\n",
    "\n",
    "# Save the updated DataFrame back to the CSV file\n",
    "df.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Data successfully added to row {first_empty_row + 1} of the CSV file.\")\n",
    "\n",
    "#print(f\"Number of columns in DataFrame: {len(df.columns)}\")\n",
    "#print(f\"Number of values in new_data: {len(new_data)}\")\n",
    "\n",
    "#print(benchmarkdata)\n",
    "\n",
    "#print(np.mean(avgweightedbenchmarkchange))\n",
    "#print(np.mean(avgweightedbenchmarkchangesd)) \n",
    "\n",
    "#imports\n",
    "import time\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#manually set data\n",
    "notebook_name = \"Decision Tree Testing Consistency Open Close 3 past tech.ipynb\"\n",
    "VIXMSUSFNvar = '/OC'\n",
    "maxfeatures = 5 #'sqrt'\n",
    "GSPCvar = 'all - og'\n",
    "#all predictors - the original\n",
    "shrtcomment = ''\n",
    "NYICDXQQQvar = ''\n",
    "XLKvar = ''\n",
    "benchmark = '^GSPC'\n",
    "theticker = 'QQQ' #^GSPC\n",
    "predictionsnum = 'N/A'\n",
    "longcomment = ''\n",
    "\n",
    "#settings\n",
    "dt = 0.6\n",
    "\n",
    "number_of_estimators = 1600\n",
    "minimum_of_samples_split = 150\n",
    "startnumber = 3000\n",
    "stepnumber = 220\n",
    "startdateunformatted = '1950-01-01'\n",
    "predictiondayslists4 = []\n",
    "\n",
    "#prep\n",
    "starting_time = pd.to_datetime(startdateunformatted).date()\n",
    "today = datetime.now()\n",
    "tomorrow = today + timedelta(days=1)\n",
    "tomorrow_str = tomorrow.strftime('%Y-%m-%d')\n",
    "ending_time = tomorrow_str\n",
    "tomorrow_str\n",
    "results = []\n",
    "predictionfrequency = []\n",
    "avgpredictionchange = []\n",
    "avgpredictionchangesd = []\n",
    "avgweightedpredictionchange = []\n",
    "avgweightedpredictionchangesd = []\n",
    "avgweightedbenchmarkchange = []\n",
    "avgweightedbenchmarkchangesd = []\n",
    "pd.set_option('display.max_rows', 400)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "#data\n",
    "# Fetch S&P 500 data\n",
    "sp500 = yf.Ticker(theticker)\n",
    "sp500 = sp500.history(period=\"max\")\n",
    "\n",
    "# Ensure the index is a DatetimeIndex\n",
    "if not isinstance(sp500.index, pd.DatetimeIndex):\n",
    "    sp500.index = pd.to_datetime(sp500.index)\n",
    "\n",
    "# Normalize the dates (remove the time component)\n",
    "sp500.index = sp500.index.normalize()\n",
    "\n",
    "# Drop columns if needed\n",
    "if \"Dividends\" in sp500.columns:\n",
    "    del sp500[\"Dividends\"]\n",
    "if \"Stock Splits\" in sp500.columns:\n",
    "    del sp500[\"Stock Splits\"]\n",
    "sp500.index = sp500.index.date\n",
    "display(sp500)\n",
    "sp500[\"Tomorrow\"] = sp500[\"Close\"].shift(-1)\n",
    "######what change were predicting\n",
    "sp500[\"Target\"] = (sp500[\"Close\"] > sp500[\"Open\"]).astype(int).shift(-1)\n",
    "#####\n",
    "sp500 = sp500.loc[starting_time:].copy()\n",
    "display(sp500)\n",
    "volatility = yf.download(tickers = '^VIX', start = starting_time, end = ending_time)\n",
    "volatility = volatility[['Open', 'Adj Close', 'High', 'Low']]\n",
    "volatility = volatility.rename(columns={'Open': 'VOpen', 'Adj Close': 'VAdj Close', 'High': 'VHigh', 'Low': 'VLow'})\n",
    "volatility.index = volatility.index.normalize()\n",
    "volatility.index = volatility.index.date\n",
    "sp500 = pd.concat([sp500, volatility], axis=1)\n",
    "USD = yf.download(tickers = '^NYICDX', start = starting_time, end = ending_time)\n",
    "USD = USD[['Open', 'Adj Close', 'High', 'Low']]\n",
    "USD = USD.rename(columns={'Open': 'USDOpen', 'Adj Close': 'USDAdj Close', 'High': 'USDHigh', 'Low': 'USDLow'})\n",
    "USD.index = USD.index.normalize()\n",
    "USD.index = USD.index.date\n",
    "sp500 = pd.concat([sp500, USD], axis=1)\n",
    "#QQQ = yf.download(tickers = 'QQQ', start = starting_time, end = ending_time)\n",
    "#QQQ = QQQ[['Open', 'Adj Close', 'High', 'Low']]\n",
    "#QQQ = QQQ.rename(columns={'Open': 'QQQOpen', 'Adj Close': 'QQQAdj Close', 'High': 'QQQHigh', 'Low': 'QQQLow'})\n",
    "#QQQ.index = QQQ.index.normalize()\n",
    "#QQQ.index = QQQ.index.date\n",
    "#sp500 = pd.concat([sp500, QQQ], axis=1)\n",
    "tech = yf.download(tickers = 'XLK', start = starting_time, end = ending_time)\n",
    "tech = tech[['Open', 'Adj Close', 'High', 'Low']]\n",
    "tech = tech.rename(columns={'Open': 'techOpen', 'Adj Close': 'techAdj Close', 'High': 'techHigh', 'Low': 'techLow'})\n",
    "tech.index = tech.index.normalize()\n",
    "tech.index = tech.index.date\n",
    "sp500 = pd.concat([sp500, tech], axis=1)\n",
    "msusfn = yf.download(tickers = 'IXF', start = starting_time, end = ending_time)\n",
    "msusfn = msusfn[['Open', 'Adj Close', 'High', 'Low']]\n",
    "msusfn = msusfn.rename(columns={'Open': 'msusfnOpen', 'Adj Close': 'msusfnAdj Close', 'High': 'msusfnHigh', 'Low': 'msusfnLow'})\n",
    "msusfn.index = msusfn.index.normalize()\n",
    "msusfn.index = msusfn.index.date\n",
    "sp500 = pd.concat([sp500, msusfn], axis=1)\n",
    "new_row = pd.DataFrame([[0] * len(sp500.columns)], columns=sp500.columns, index=[pd.to_datetime(tomorrow_str)])\n",
    "new_row.index = new_row.index.normalize()\n",
    "new_row.index = new_row.index.date\n",
    "sp500 = pd.concat([sp500, new_row])\n",
    "train = sp500.iloc[:-100]\n",
    "test = sp500.iloc[-100:]\n",
    "\n",
    "#old\n",
    "predictors = [\"Close\", \"Volume\", \"Open\", \"High\"]\n",
    "#, \"Low\", \"VOpen\", \"VAdj Close\", \"VHigh\", \"VLow\"\n",
    "#predictors = [\"Close\", \"Volume\", \"Open\", \"High\", \"Low\"]\n",
    "\n",
    "#The backtest function evaluates a model by training it on historical data \n",
    "#and testing it in increments. It iterates over the data, training the model\n",
    "#on a growing set of past data and testing it on the next segment. \n",
    "#The results are collected and combined into a single DataFrame for analysis.\n",
    "def backtest(data, model, predictors, start=startnumber, step=stepnumber):\n",
    "    all_predictions = []\n",
    "\n",
    "    for i in range(start, data.shape[0], step):\n",
    "        train = data.iloc[0:i].copy()\n",
    "        test = data.iloc[i:(i+step)].copy()\n",
    "        predictions = predict(train, test, predictors, model)\n",
    "        all_predictions.append(predictions)\n",
    "    \n",
    "    return pd.concat(all_predictions)\n",
    "\n",
    "horizons = [2,5,60,250,1000]\n",
    "new_predictors = []\n",
    "\n",
    "for horizon in horizons:\n",
    "    rolling_averages = sp500.rolling(horizon).mean()\n",
    "    \n",
    "    ratio_column = f\"Close_Ratio_{horizon}\"\n",
    "    sp500[ratio_column] = sp500[\"Close\"] / rolling_averages[\"Close\"]\n",
    "    \n",
    "    trend_column = f\"Trend_{horizon}\"\n",
    "    sp500[trend_column] = sp500.shift(1).rolling(horizon).sum()[\"Target\"]\n",
    "    \n",
    "    new_predictors+= [ratio_column, trend_column]\n",
    "\n",
    "sp500 = sp500.dropna(subset=sp500.columns[sp500.columns != \"Tomorrow\"])\n",
    "\n",
    "additional_predictors = [\"msusfnOpen\", \"msusfnAdj Close\"]\n",
    "    #\"USDAdj Close\", \"USDOpen\"] #  \"techAdj Close\", \"techOpen\"]# \"VOpen\", \"VAdj Close\"] #\"VAdj Close\"]#, \"QQQOpen\", \"QQQAdj Close\"] #[\"VAdj Close\", \"VOpen\", \"VHigh\", \"VLow\"]\n",
    "#\"USDHigh\", \"USDLow\", \"USDAdj Close\", \"USDOpen\", ,  \"techLow\",\"techHigh\", \n",
    "#additional_predictors = [\"Close\", \"Volume\", \"Open\", \"High\", \"Low\"] #[]#\n",
    "new_predictors.extend(additional_predictors)\n",
    "\n",
    "def predict(train, test, predictors, model):\n",
    "        model.fit(train[predictors], train[\"Target\"])\n",
    "        preds = model.predict_proba(test[predictors])[:,1]\n",
    "        preds[preds >=dt] = 1\n",
    "        preds[preds <dt] = 0\n",
    "        preds = pd.Series(preds, index=test.index, name=\"Predictions\")\n",
    "        combined = pd.concat([test[\"Target\"], preds], axis=1)\n",
    "        return combined\n",
    "\n",
    "start_time = time.time()\n",
    "##\n",
    "benchmarkdata = yf.Ticker(benchmark)\n",
    "benchmarkdata = benchmarkdata.history(period=\"max\")\n",
    "### the way change in what time is recorded\n",
    "benchmarkdata[\"BMChange\"] = ((benchmarkdata[\"Close\"] - benchmarkdata[\"Open\"])/benchmarkdata[\"Close\"]).shift(-1)\n",
    "###\n",
    "if not isinstance(benchmarkdata.index, pd.DatetimeIndex):\n",
    "    benchmarkdata.index = pd.to_datetime(benchmarkdata.index)\n",
    "\n",
    "benchmarkdata.index = benchmarkdata.index.normalize()\n",
    "benchmarkdata.index = benchmarkdata.index.date\n",
    "benchmarkdata = benchmarkdata.drop(benchmarkdata.index[1])\n",
    "print(benchmarkdata)\n",
    "#\n",
    "for i in range (iterations):\n",
    "    predictionchange = []\n",
    "    weightedpredictionchange = []\n",
    "    weightedbmchange = []\n",
    "    model = RandomForestClassifier(n_estimators=number_of_estimators, max_features=maxfeatures, min_samples_split=minimum_of_samples_split, random_state=i)\n",
    "    predictions = backtest(sp500, model, new_predictors)\n",
    "    #display(f\"Iteration {i + 1}:\")\n",
    "    #display(predictions)\n",
    "    #display(predictions[\"Predictions\"].value_counts())\n",
    "    #display(predictions[\"Target\"].value_counts() / predictions.shape[0])\n",
    "    #display(np.round(precision_score(predictions[\"Target\"], predictions[\"Predictions\"]), decimals=4))\n",
    "    results.append(precision_score(predictions[\"Target\"], predictions[\"Predictions\"]))\n",
    "    \n",
    "    \n",
    "    #predictions already happened what happens after is obtaining data\n",
    "    #newdata\n",
    "    spy = yf.Ticker(theticker)\n",
    "    spy = spy.history(period=\"max\")\n",
    "    #\n",
    "    if not isinstance(spy.index, pd.DatetimeIndex):\n",
    "        spy.index = pd.to_datetime(spy.index)\n",
    "        #\n",
    "    spy.index = spy.index.normalize()\n",
    "    del spy[\"Dividends\"]\n",
    "    del spy[\"Stock Splits\"]\n",
    "    del spy[\"Volume\"]\n",
    "    del spy[\"High\"]\n",
    "    del spy[\"Low\"]\n",
    "    #### what time for change\n",
    "    spy[\"Change\"] = (spy[\"Close\"] - spy[\"Open\"]).shift(-1)\n",
    "    #####\n",
    "    spy['Open']\n",
    "    del spy['Close']\n",
    "    spy.index = spy.index.date\n",
    "    spy = spy.drop(spy.index[1])\n",
    "    display(spy)\n",
    "    total_sum = predictions[\"Predictions\"].value_counts().sum()\n",
    "    preddays4 = []\n",
    "    for idx, value in predictions[\"Predictions\"].items():\n",
    "        if value != 0:\n",
    "            preddays4.append(idx)\n",
    "    predictiondayslists4.append(preddays4)\n",
    "    preddays4 = []\n",
    "    display(total_sum)\n",
    "    spy = spy.iloc[-total_sum:]\n",
    "    predictionfrequency.append(predictions[\"Predictions\"].sum() / len(spy))\n",
    "    \n",
    "    predictions = predictions.iloc[-total_sum:]    #???total_sum\n",
    "    change_column = spy['Change'].tolist()\n",
    "    spy_column = spy['Open'].tolist()\n",
    "    #display(change_column)\n",
    "    predictions['Change'] = change_column\n",
    "    predictions['Weight'] = spy_column\n",
    "    display(predictions)\n",
    "    combination = predictions\n",
    "    plt.figure(figsize=(10, 6))  # Adjust the figure size as needed\n",
    "    plt.plot(predictions.index, predictions['Predictions'], marker='o', linestyle='-')\n",
    "    plt.title('Predictions Over Time')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Predictions')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    #display(spy)\n",
    "    def new_row(row):\n",
    "        if row['Predictions'] == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return row['Change']\n",
    "    predictions['UpdatedChange'] = predictions.apply(new_row, axis=1)\n",
    "    predictions['WeightedChange'] = predictions['UpdatedChange'] / predictions['Weight']\n",
    "    display(predictions)\n",
    "    last_200_rows = predictions.tail(400)\n",
    "    #display(last_200_rows)\n",
    "    for value in predictions['UpdatedChange']:\n",
    "        if value != 0:\n",
    "            predictionchange.append(value)    \n",
    "    avgpredictionchange.append(np.mean(predictionchange))\n",
    "    avgpredictionchangesd.append(np.std(predictionchange))\n",
    "    for value in predictions['WeightedChange']:\n",
    "        if value != 0:\n",
    "            weightedpredictionchange.append(value)    \n",
    "    avgweightedpredictionchange.append(np.mean(weightedpredictionchange))\n",
    "    avgweightedpredictionchangesd.append(np.std(weightedpredictionchange))\n",
    "    ###benchmark stuff\n",
    "    display(combination)\n",
    "    combination[\"BMChange\"] = benchmarkdata[\"BMChange\"]\n",
    "    for idx, value in predictions[\"UpdatedChange\"].items():\n",
    "        if value != 0:\n",
    "            weightedbmchange.append(predictions[\"BMChange\"].loc[idx])\n",
    "        else:\n",
    "            predictions.at[idx, \"BMChange\"] = 0\n",
    "    # Filter out the non-zero elements\n",
    "    combination[\"BMChange\"] = combination[\"WeightedChange\"] - combination[\"BMChange\"]\n",
    "    weightedbmchange = combination[\"BMChange\"][combination[\"BMChange\"] != 0]\n",
    "\n",
    "# Convert the filtered series to a list\n",
    "    weightedbmchange = weightedbmchange.tolist()\n",
    "\n",
    "    avgweightedbenchmarkchange.append(np.mean(weightedbmchange))\n",
    "    avgweightedbenchmarkchangesd.append(np.std(weightedbmchange))\n",
    "    display(combination)\n",
    "\n",
    "\n",
    "print(type(combination))\n",
    "print(type(benchmarkdata))\n",
    "print(benchmarkdata)\n",
    "\n",
    "display(results)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "display(np.round((elapsed_time / 60), decimals=2))\n",
    "display(np.round(((elapsed_time / 60) / iterations), decimals=2))\n",
    "\n",
    "display(np.round(np.mean(avgpredictionchange), decimals=2))\n",
    "display(np.round(np.mean(avgpredictionchangesd), decimals=4))\n",
    "display(np.round(np.mean(avgweightedpredictionchange), decimals=6))\n",
    "display(np.round(np.mean(avgweightedpredictionchangesd), decimals=6))\n",
    "\n",
    "display(np.round(np.mean(predictionfrequency), decimals=4))\n",
    "display(np.round(np.mean(results), decimals=4))\n",
    "display(np.round(np.std(results), decimals=4))\n",
    "\n",
    "dates_when_predictions_is_1 = predictions.index[predictions['Predictions'] == 1].tolist()\n",
    "\n",
    "#print(f\"All dates when 'predictions' is equal to 1: {dates_when_predictions_is_1}\")\n",
    "\n",
    "\n",
    "# Load the CSV file\n",
    "filename = \"/Users/derek/Downloads/DT AI Transformed Data.csv\"\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "# Define your 24 variables (ordered list of values to add)\n",
    "new_data = [\n",
    "    notebook_name, number_of_estimators, startdateunformatted, dt, VIXMSUSFNvar, minimum_of_samples_split,\n",
    "    maxfeatures, GSPCvar, shrtcomment, iterations, NYICDXQQQvar, XLKvar,\n",
    "    startnumber, stepnumber, theticker, np.round(np.mean(results), decimals=4), np.round(np.std(results), decimals=4), np.round(((elapsed_time / 60) / iterations), decimals=2),\n",
    "    np.round((elapsed_time / 60), decimals=2), np.round(np.mean(predictionfrequency), decimals = 4), np.round(np.mean(avgpredictionchange), decimals=2), np.round(np.mean(avgpredictionchangesd), decimals=4), np.round(np.mean(avgweightedpredictionchange), decimals=6), np.round(np.mean(avgweightedpredictionchangesd), decimals=6), np.round(np.mean(avgweightedbenchmarkchange), decimals = 6), np.round(np.mean(avgweightedbenchmarkchangesd), decimals=6), longcomment\n",
    "]\n",
    "\n",
    "# Find the first empty row\n",
    "# Here we assume that the row is considered empty if all columns are NaN or empty strings\n",
    "empty_row_index = df.index[df.isnull().all(axis=1) | (df == '').all(axis=1)].tolist()\n",
    "if empty_row_index:\n",
    "    # If there are empty rows, use the first one\n",
    "    first_empty_row = empty_row_index[0]\n",
    "else:\n",
    "    # If no empty rows, append a new row at the end\n",
    "    first_empty_row = len(df)\n",
    "\n",
    "# Ensure the length of new_data matches the number of columns\n",
    "if len(new_data) != len(df.columns):\n",
    "    raise ValueError(\"Number of new data values does not match number of columns.\")\n",
    "\n",
    "# Add the new data to the determined row\n",
    "df.loc[first_empty_row] = new_data\n",
    "\n",
    "# Save the updated DataFrame back to the CSV file\n",
    "df.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Data successfully added to row {first_empty_row + 1} of the CSV file.\")\n",
    "\n",
    "print(f\"Number of columns in DataFrame: {len(df.columns)}\")\n",
    "print(f\"Number of values in new_data: {len(new_data)}\")\n",
    "\n",
    "print(benchmarkdata)\n",
    "\n",
    "print(np.mean(avgweightedbenchmarkchange))\n",
    "print(np.mean(avgweightedbenchmarkchangesd)) \n",
    "\n",
    "\n",
    "\n",
    "#imports\n",
    "import time\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#manually set data\n",
    "notebook_name = \"Decision Tree Testing Consistency Open Close 3 past tech.ipynb\"\n",
    "VIXMSUSFNvar = '/OC'\n",
    "maxfeatures = 5 #'sqrt'\n",
    "GSPCvar = 'all - og'\n",
    "#all predictors - the original\n",
    "shrtcomment = ''\n",
    "NYICDXQQQvar = ''\n",
    "XLKvar = ''\n",
    "benchmark = '^GSPC'\n",
    "theticker = 'EEM' #^GSPC\n",
    "predictionsnum = 'N/A'\n",
    "longcomment = ''\n",
    "\n",
    "#settings\n",
    "dt = 0.6\n",
    "\n",
    "number_of_estimators = 1600\n",
    "minimum_of_samples_split = 150\n",
    "startnumber = 3000\n",
    "stepnumber = 220\n",
    "startdateunformatted = '1950-01-01'\n",
    "predictiondayslists5 = []\n",
    "\n",
    "#prep\n",
    "starting_time = pd.to_datetime(startdateunformatted).date()\n",
    "today = datetime.now()\n",
    "tomorrow = today + timedelta(days=1)\n",
    "tomorrow_str = tomorrow.strftime('%Y-%m-%d')\n",
    "ending_time = tomorrow_str\n",
    "tomorrow_str\n",
    "results = []\n",
    "predictionfrequency = []\n",
    "avgpredictionchange = []\n",
    "avgpredictionchangesd = []\n",
    "avgweightedpredictionchange = []\n",
    "avgweightedpredictionchangesd = []\n",
    "avgweightedbenchmarkchange = []\n",
    "avgweightedbenchmarkchangesd = []\n",
    "pd.set_option('display.max_rows', 400)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "#data\n",
    "# Fetch S&P 500 data\n",
    "sp500 = yf.Ticker(theticker)\n",
    "sp500 = sp500.history(period=\"max\")\n",
    "\n",
    "# Ensure the index is a DatetimeIndex\n",
    "if not isinstance(sp500.index, pd.DatetimeIndex):\n",
    "    sp500.index = pd.to_datetime(sp500.index)\n",
    "\n",
    "# Normalize the dates (remove the time component)\n",
    "sp500.index = sp500.index.normalize()\n",
    "\n",
    "# Drop columns if needed\n",
    "if \"Dividends\" in sp500.columns:\n",
    "    del sp500[\"Dividends\"]\n",
    "if \"Stock Splits\" in sp500.columns:\n",
    "    del sp500[\"Stock Splits\"]\n",
    "sp500.index = sp500.index.date\n",
    "display(sp500)\n",
    "sp500[\"Tomorrow\"] = sp500[\"Close\"].shift(-1)\n",
    "######what change were predicting\n",
    "sp500[\"Target\"] = (sp500[\"Close\"] > sp500[\"Open\"]).astype(int).shift(-1)\n",
    "#####\n",
    "sp500 = sp500.loc[starting_time:].copy()\n",
    "display(sp500)\n",
    "volatility = yf.download(tickers = '^VIX', start = starting_time, end = ending_time)\n",
    "volatility = volatility[['Open', 'Adj Close', 'High', 'Low']]\n",
    "volatility = volatility.rename(columns={'Open': 'VOpen', 'Adj Close': 'VAdj Close', 'High': 'VHigh', 'Low': 'VLow'})\n",
    "volatility.index = volatility.index.normalize()\n",
    "volatility.index = volatility.index.date\n",
    "sp500 = pd.concat([sp500, volatility], axis=1)\n",
    "USD = yf.download(tickers = '^NYICDX', start = starting_time, end = ending_time)\n",
    "USD = USD[['Open', 'Adj Close', 'High', 'Low']]\n",
    "USD = USD.rename(columns={'Open': 'USDOpen', 'Adj Close': 'USDAdj Close', 'High': 'USDHigh', 'Low': 'USDLow'})\n",
    "USD.index = USD.index.normalize()\n",
    "USD.index = USD.index.date\n",
    "sp500 = pd.concat([sp500, USD], axis=1)\n",
    "#QQQ = yf.download(tickers = 'QQQ', start = starting_time, end = ending_time)\n",
    "#QQQ = QQQ[['Open', 'Adj Close', 'High', 'Low']]\n",
    "#QQQ = QQQ.rename(columns={'Open': 'QQQOpen', 'Adj Close': 'QQQAdj Close', 'High': 'QQQHigh', 'Low': 'QQQLow'})\n",
    "#QQQ.index = QQQ.index.normalize()\n",
    "#QQQ.index = QQQ.index.date\n",
    "#sp500 = pd.concat([sp500, QQQ], axis=1)\n",
    "tech = yf.download(tickers = 'XLK', start = starting_time, end = ending_time)\n",
    "tech = tech[['Open', 'Adj Close', 'High', 'Low']]\n",
    "tech = tech.rename(columns={'Open': 'techOpen', 'Adj Close': 'techAdj Close', 'High': 'techHigh', 'Low': 'techLow'})\n",
    "tech.index = tech.index.normalize()\n",
    "tech.index = tech.index.date\n",
    "sp500 = pd.concat([sp500, tech], axis=1)\n",
    "msusfn = yf.download(tickers = 'IXF', start = starting_time, end = ending_time)\n",
    "msusfn = msusfn[['Open', 'Adj Close', 'High', 'Low']]\n",
    "msusfn = msusfn.rename(columns={'Open': 'msusfnOpen', 'Adj Close': 'msusfnAdj Close', 'High': 'msusfnHigh', 'Low': 'msusfnLow'})\n",
    "msusfn.index = msusfn.index.normalize()\n",
    "msusfn.index = msusfn.index.date\n",
    "sp500 = pd.concat([sp500, msusfn], axis=1)\n",
    "new_row = pd.DataFrame([[0] * len(sp500.columns)], columns=sp500.columns, index=[pd.to_datetime(tomorrow_str)])\n",
    "new_row.index = new_row.index.normalize()\n",
    "new_row.index = new_row.index.date\n",
    "sp500 = pd.concat([sp500, new_row])\n",
    "train = sp500.iloc[:-100]\n",
    "test = sp500.iloc[-100:]\n",
    "\n",
    "#old\n",
    "predictors = [\"Close\", \"Volume\", \"Open\", \"High\"]\n",
    "#, \"Low\", \"VOpen\", \"VAdj Close\", \"VHigh\", \"VLow\"\n",
    "#predictors = [\"Close\", \"Volume\", \"Open\", \"High\", \"Low\"]\n",
    "\n",
    "#The backtest function evaluates a model by training it on historical data \n",
    "#and testing it in increments. It iterates over the data, training the model\n",
    "#on a growing set of past data and testing it on the next segment. \n",
    "#The results are collected and combined into a single DataFrame for analysis.\n",
    "def backtest(data, model, predictors, start=startnumber, step=stepnumber):\n",
    "    all_predictions = []\n",
    "\n",
    "    for i in range(start, data.shape[0], step):\n",
    "        train = data.iloc[0:i].copy()\n",
    "        test = data.iloc[i:(i+step)].copy()\n",
    "        predictions = predict(train, test, predictors, model)\n",
    "        all_predictions.append(predictions)\n",
    "    \n",
    "    return pd.concat(all_predictions)\n",
    "\n",
    "horizons = [2,5,60,250,1000]\n",
    "new_predictors = []\n",
    "\n",
    "for horizon in horizons:\n",
    "    rolling_averages = sp500.rolling(horizon).mean()\n",
    "    \n",
    "    ratio_column = f\"Close_Ratio_{horizon}\"\n",
    "    sp500[ratio_column] = sp500[\"Close\"] / rolling_averages[\"Close\"]\n",
    "    \n",
    "    trend_column = f\"Trend_{horizon}\"\n",
    "    sp500[trend_column] = sp500.shift(1).rolling(horizon).sum()[\"Target\"]\n",
    "    \n",
    "    new_predictors+= [ratio_column, trend_column]\n",
    "\n",
    "sp500 = sp500.dropna(subset=sp500.columns[sp500.columns != \"Tomorrow\"])\n",
    "\n",
    "additional_predictors = [\"msusfnOpen\", \"msusfnAdj Close\"]\n",
    "    #\"USDAdj Close\", \"USDOpen\"] #  \"techAdj Close\", \"techOpen\"]# \"VOpen\", \"VAdj Close\"] #\"VAdj Close\"]#, \"QQQOpen\", \"QQQAdj Close\"] #[\"VAdj Close\", \"VOpen\", \"VHigh\", \"VLow\"]\n",
    "#\"USDHigh\", \"USDLow\", \"USDAdj Close\", \"USDOpen\", ,  \"techLow\",\"techHigh\", \n",
    "#additional_predictors = [\"Close\", \"Volume\", \"Open\", \"High\", \"Low\"] #[]#\n",
    "new_predictors.extend(additional_predictors)\n",
    "\n",
    "def predict(train, test, predictors, model):\n",
    "        model.fit(train[predictors], train[\"Target\"])\n",
    "        preds = model.predict_proba(test[predictors])[:,1]\n",
    "        preds[preds >=dt] = 1\n",
    "        preds[preds <dt] = 0\n",
    "        preds = pd.Series(preds, index=test.index, name=\"Predictions\")\n",
    "        combined = pd.concat([test[\"Target\"], preds], axis=1)\n",
    "        return combined\n",
    "\n",
    "start_time = time.time()\n",
    "##\n",
    "benchmarkdata = yf.Ticker(benchmark)\n",
    "benchmarkdata = benchmarkdata.history(period=\"max\")\n",
    "### the way change in what time is recorded\n",
    "benchmarkdata[\"BMChange\"] = ((benchmarkdata[\"Close\"] - benchmarkdata[\"Open\"])/benchmarkdata[\"Close\"]).shift(-1)\n",
    "###\n",
    "if not isinstance(benchmarkdata.index, pd.DatetimeIndex):\n",
    "    benchmarkdata.index = pd.to_datetime(benchmarkdata.index)\n",
    "\n",
    "benchmarkdata.index = benchmarkdata.index.normalize()\n",
    "benchmarkdata.index = benchmarkdata.index.date\n",
    "benchmarkdata = benchmarkdata.drop(benchmarkdata.index[1])\n",
    "print(benchmarkdata)\n",
    "#\n",
    "for i in range (iterations):\n",
    "    predictionchange = []\n",
    "    weightedpredictionchange = []\n",
    "    weightedbmchange = []\n",
    "    model = RandomForestClassifier(n_estimators=number_of_estimators, max_features=maxfeatures, min_samples_split=minimum_of_samples_split, random_state=i)\n",
    "    predictions = backtest(sp500, model, new_predictors)\n",
    "    #display(f\"Iteration {i + 1}:\")\n",
    "    #display(predictions)\n",
    "    #display(predictions[\"Predictions\"].value_counts())\n",
    "    #display(predictions[\"Target\"].value_counts() / predictions.shape[0])\n",
    "    #display(np.round(precision_score(predictions[\"Target\"], predictions[\"Predictions\"]), decimals=4))\n",
    "    results.append(precision_score(predictions[\"Target\"], predictions[\"Predictions\"]))\n",
    "    \n",
    "    \n",
    "    #predictions already happened what happens after is obtaining data\n",
    "    #newdata\n",
    "    spy = yf.Ticker(theticker)\n",
    "    spy = spy.history(period=\"max\")\n",
    "    #\n",
    "    if not isinstance(spy.index, pd.DatetimeIndex):\n",
    "        spy.index = pd.to_datetime(spy.index)\n",
    "        #\n",
    "    spy.index = spy.index.normalize()\n",
    "    del spy[\"Dividends\"]\n",
    "    del spy[\"Stock Splits\"]\n",
    "    del spy[\"Volume\"]\n",
    "    del spy[\"High\"]\n",
    "    del spy[\"Low\"]\n",
    "    #### what time for change\n",
    "    spy[\"Change\"] = (spy[\"Close\"] - spy[\"Open\"]).shift(-1)\n",
    "    #####\n",
    "    spy['Open']\n",
    "    del spy['Close']\n",
    "    spy.index = spy.index.date\n",
    "    spy = spy.drop(spy.index[1])\n",
    "    display(spy)\n",
    "    total_sum = predictions[\"Predictions\"].value_counts().sum()\n",
    "    preddays5 = []\n",
    "    for idx, value in predictions[\"Predictions\"].items():\n",
    "        if value != 0:\n",
    "            preddays5.append(idx)\n",
    "    predictiondayslists5.append(preddays5)\n",
    "    preddays5 = []\n",
    "    display(total_sum)\n",
    "    spy = spy.iloc[-total_sum:]\n",
    "    predictionfrequency.append(predictions[\"Predictions\"].sum() / len(spy))\n",
    "    \n",
    "    predictions = predictions.iloc[-total_sum:]    #???total_sum\n",
    "    change_column = spy['Change'].tolist()\n",
    "    spy_column = spy['Open'].tolist()\n",
    "    #display(change_column)\n",
    "    predictions['Change'] = change_column\n",
    "    predictions['Weight'] = spy_column\n",
    "    display(predictions)\n",
    "    combination = predictions\n",
    "    plt.figure(figsize=(10, 6))  # Adjust the figure size as needed\n",
    "    plt.plot(predictions.index, predictions['Predictions'], marker='o', linestyle='-')\n",
    "    plt.title('Predictions Over Time')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Predictions')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    #display(spy)\n",
    "    def new_row(row):\n",
    "        if row['Predictions'] == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return row['Change']\n",
    "    predictions['UpdatedChange'] = predictions.apply(new_row, axis=1)\n",
    "    predictions['WeightedChange'] = predictions['UpdatedChange'] / predictions['Weight']\n",
    "    display(predictions)\n",
    "    last_200_rows = predictions.tail(400)\n",
    "    #display(last_200_rows)\n",
    "    for value in predictions['UpdatedChange']:\n",
    "        if value != 0:\n",
    "            predictionchange.append(value)    \n",
    "    avgpredictionchange.append(np.mean(predictionchange))\n",
    "    avgpredictionchangesd.append(np.std(predictionchange))\n",
    "    for value in predictions['WeightedChange']:\n",
    "        if value != 0:\n",
    "            weightedpredictionchange.append(value)    \n",
    "    avgweightedpredictionchange.append(np.mean(weightedpredictionchange))\n",
    "    avgweightedpredictionchangesd.append(np.std(weightedpredictionchange))\n",
    "    ###benchmark stuff\n",
    "    display(combination)\n",
    "    combination[\"BMChange\"] = benchmarkdata[\"BMChange\"]\n",
    "    for idx, value in predictions[\"UpdatedChange\"].items():\n",
    "        if value != 0:\n",
    "            weightedbmchange.append(predictions[\"BMChange\"].loc[idx])\n",
    "        else:\n",
    "            predictions.at[idx, \"BMChange\"] = 0\n",
    "    # Filter out the non-zero elements\n",
    "    combination[\"BMChange\"] = combination[\"WeightedChange\"] - combination[\"BMChange\"]\n",
    "    weightedbmchange = combination[\"BMChange\"][combination[\"BMChange\"] != 0]\n",
    "\n",
    "# Convert the filtered series to a list\n",
    "    weightedbmchange = weightedbmchange.tolist()\n",
    "\n",
    "    avgweightedbenchmarkchange.append(np.mean(weightedbmchange))\n",
    "    avgweightedbenchmarkchangesd.append(np.std(weightedbmchange))\n",
    "    display(combination)\n",
    "\n",
    "\n",
    "print(type(combination))\n",
    "print(type(benchmarkdata))\n",
    "print(benchmarkdata)\n",
    "\n",
    "display(results)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "display(np.round((elapsed_time / 60), decimals=2))\n",
    "display(np.round(((elapsed_time / 60) / iterations), decimals=2))\n",
    "\n",
    "display(np.round(np.mean(avgpredictionchange), decimals=2))\n",
    "display(np.round(np.mean(avgpredictionchangesd), decimals=4))\n",
    "display(np.round(np.mean(avgweightedpredictionchange), decimals=6))\n",
    "display(np.round(np.mean(avgweightedpredictionchangesd), decimals=6))\n",
    "\n",
    "display(np.round(np.mean(predictionfrequency), decimals=4))\n",
    "display(np.round(np.mean(results), decimals=4))\n",
    "display(np.round(np.std(results), decimals=4))\n",
    "\n",
    "dates_when_predictions_is_1 = predictions.index[predictions['Predictions'] == 1].tolist()\n",
    "\n",
    "#print(f\"All dates when 'predictions' is equal to 1: {dates_when_predictions_is_1}\")\n",
    "\n",
    "\n",
    "# Load the CSV file\n",
    "filename = \"/Users/derek/Downloads/DT AI Transformed Data.csv\"\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "# Define your 24 variables (ordered list of values to add)\n",
    "new_data = [\n",
    "    notebook_name, number_of_estimators, startdateunformatted, dt, VIXMSUSFNvar, minimum_of_samples_split,\n",
    "    maxfeatures, GSPCvar, shrtcomment, iterations, NYICDXQQQvar, XLKvar,\n",
    "    startnumber, stepnumber, theticker, np.round(np.mean(results), decimals=4), np.round(np.std(results), decimals=4), np.round(((elapsed_time / 60) / iterations), decimals=2),\n",
    "    np.round((elapsed_time / 60), decimals=2), np.round(np.mean(predictionfrequency), decimals = 4), np.round(np.mean(avgpredictionchange), decimals=2), np.round(np.mean(avgpredictionchangesd), decimals=4), np.round(np.mean(avgweightedpredictionchange), decimals=6), np.round(np.mean(avgweightedpredictionchangesd), decimals=6), np.round(np.mean(avgweightedbenchmarkchange), decimals = 6), np.round(np.mean(avgweightedbenchmarkchangesd), decimals=6), longcomment\n",
    "]\n",
    "\n",
    "# Find the first empty row\n",
    "# Here we assume that the row is considered empty if all columns are NaN or empty strings\n",
    "empty_row_index = df.index[df.isnull().all(axis=1) | (df == '').all(axis=1)].tolist()\n",
    "if empty_row_index:\n",
    "    # If there are empty rows, use the first one\n",
    "    first_empty_row = empty_row_index[0]\n",
    "else:\n",
    "    # If no empty rows, append a new row at the end\n",
    "    first_empty_row = len(df)\n",
    "\n",
    "# Ensure the length of new_data matches the number of columns\n",
    "if len(new_data) != len(df.columns):\n",
    "    raise ValueError(\"Number of new data values does not match number of columns.\")\n",
    "\n",
    "# Add the new data to the determined row\n",
    "df.loc[first_empty_row] = new_data\n",
    "\n",
    "# Save the updated DataFrame back to the CSV file\n",
    "df.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Data successfully added to row {first_empty_row + 1} of the CSV file.\")\n",
    "\n",
    "print(f\"Number of columns in DataFrame: {len(df.columns)}\")\n",
    "print(f\"Number of values in new_data: {len(new_data)}\")\n",
    "\n",
    "print(benchmarkdata)\n",
    "\n",
    "#imports\n",
    "import time\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#manually set data\n",
    "notebook_name = \"Decision Tree Testing Consistency Open Close 4 (BM Target)\"\n",
    "VIXvar = ''\n",
    "maxfeatures = 5 #'sqrt'\n",
    "GSPCvar = 'all - og'\n",
    "#all predictors - the original\n",
    "shrtcomment = ''\n",
    "NYICDXQQQvar = ''\n",
    "XLKvar = 'OC'\n",
    "benchmark = '^GSPC'\n",
    "theticker = 'IYW' #^GSPC\n",
    "predictionsnum = 'N/A'\n",
    "longcomment = ''\n",
    "\n",
    "#settings\n",
    "dt = 0.64\n",
    "#iterations = 10\n",
    "number_of_estimators = 1600\n",
    "minimum_of_samples_split = 150\n",
    "startnumber = 3000\n",
    "stepnumber = 220\n",
    "startdateunformatted = '1950-01-01'\n",
    "\n",
    "#prep\n",
    "starting_time = pd.to_datetime(startdateunformatted).date()\n",
    "today = datetime.now()\n",
    "tomorrow = today + timedelta(days=1)\n",
    "tomorrow_str = tomorrow.strftime('%Y-%m-%d')\n",
    "ending_time = tomorrow_str\n",
    "tomorrow_str\n",
    "results = []\n",
    "predictionfrequency = []\n",
    "avgpredictionchange = []\n",
    "avgpredictionchangesd = []\n",
    "avgweightedpredictionchange = []\n",
    "avgweightedpredictionchangesd = []\n",
    "avgweightedbenchmarkchange = []\n",
    "avgweightedbenchmarkchangesd = []\n",
    "predictiondayslists6 = []\n",
    "pd.set_option('display.max_rows', 400)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "#setting up benchmark\n",
    "##\n",
    "benchmarkdata = yf.Ticker(benchmark)\n",
    "benchmarkdata = benchmarkdata.history(period=\"max\")\n",
    "### the way change in what time is recorded\n",
    "#change type\n",
    "benchmarkdata[\"BMChange\"] = ((benchmarkdata[\"Close\"] - benchmarkdata[\"Open\"]) / benchmarkdata[\"Open\"])\n",
    "if not isinstance(benchmarkdata.index, pd.DatetimeIndex):\n",
    "    benchmarkdata.index = pd.to_datetime(benchmarkdata.index)\n",
    "benchmarkdata.index = benchmarkdata.index.normalize()\n",
    "benchmarkdata.index = benchmarkdata.index.date\n",
    "benchmarkdata = benchmarkdata.drop(benchmarkdata.index[1])\n",
    "print(benchmarkdata)\n",
    "\n",
    "#data\n",
    "# Fetch S&P 500 data\n",
    "sp500 = yf.Ticker(theticker)\n",
    "sp500 = sp500.history(period=\"max\")\n",
    "\n",
    "# Ensure the index is a DatetimeIndex\n",
    "if not isinstance(sp500.index, pd.DatetimeIndex):\n",
    "    sp500.index = pd.to_datetime(sp500.index)\n",
    "\n",
    "# Normalize the dates (remove the time component)\n",
    "sp500.index = sp500.index.normalize()\n",
    "\n",
    "# Drop columns if needed\n",
    "if \"Dividends\" in sp500.columns:\n",
    "    del sp500[\"Dividends\"]\n",
    "if \"Stock Splits\" in sp500.columns:\n",
    "    del sp500[\"Stock Splits\"]\n",
    "sp500.index = sp500.index.date\n",
    "display(sp500)\n",
    "sp500[\"Tomorrow\"] = sp500[\"Close\"].shift(-1)\n",
    "######what change were predicting\n",
    "sp500[\"BMChange\"] = (benchmarkdata[\"Close\"] - benchmarkdata[\"Open\"])/benchmarkdata[\"Open\"]\n",
    "sp500[\"Target\"] = (sp500[\"BMChange\"] > ((sp500[\"Close\"] - sp500[\"Open\"])/sp500[\"Open\"])).astype(int).shift(-1)\n",
    "benchmarkdata[\"BMChange\"] = benchmarkdata[\"BMChange\"].shift(-1)\n",
    "sp500 = sp500.drop(columns=['BMChange'])\n",
    "#sp500[\"Target\"] = (sp500[\"Close\"] > sp500[\"Open\"]).astype(int).shift(-1)\n",
    "#####\n",
    "sp500 = sp500.loc[starting_time:].copy()\n",
    "display(sp500)\n",
    "volatility = yf.download(tickers = '^VIX', start = starting_time, end = ending_time)\n",
    "volatility = volatility[['Open', 'Adj Close', 'High', 'Low']]\n",
    "volatility = volatility.rename(columns={'Open': 'VOpen', 'Adj Close': 'VAdj Close', 'High': 'VHigh', 'Low': 'VLow'})\n",
    "volatility.index = volatility.index.normalize()\n",
    "volatility.index = volatility.index.date\n",
    "sp500 = pd.concat([sp500, volatility], axis=1)\n",
    "USD = yf.download(tickers = '^NYICDX', start = starting_time, end = ending_time)\n",
    "USD = USD[['Open', 'Adj Close', 'High', 'Low']]\n",
    "USD = USD.rename(columns={'Open': 'USDOpen', 'Adj Close': 'USDAdj Close', 'High': 'USDHigh', 'Low': 'USDLow'})\n",
    "USD.index = USD.index.normalize()\n",
    "USD.index = USD.index.date\n",
    "sp500 = pd.concat([sp500, USD], axis=1)\n",
    "#QQQ = yf.download(tickers = 'QQQ', start = starting_time, end = ending_time)\n",
    "#QQQ = QQQ[['Open', 'Adj Close', 'High', 'Low']]\n",
    "#QQQ = QQQ.rename(columns={'Open': 'QQQOpen', 'Adj Close': 'QQQAdj Close', 'High': 'QQQHigh', 'Low': 'QQQLow'})\n",
    "#QQQ.index = QQQ.index.normalize()\n",
    "#QQQ.index = QQQ.index.date\n",
    "#sp500 = pd.concat([sp500, QQQ], axis=1)\n",
    "tech = yf.download(tickers = 'XLK', start = starting_time, end = ending_time)\n",
    "tech = tech[['Open', 'Adj Close', 'High', 'Low']]\n",
    "tech = tech.rename(columns={'Open': 'techOpen', 'Adj Close': 'techAdj Close', 'High': 'techHigh', 'Low': 'techLow'})\n",
    "tech.index = tech.index.normalize()\n",
    "tech.index = tech.index.date\n",
    "sp500 = pd.concat([sp500, tech], axis=1)\n",
    "new_row = pd.DataFrame([[0] * len(sp500.columns)], columns=sp500.columns, index=[pd.to_datetime(tomorrow_str)])\n",
    "new_row.index = new_row.index.normalize()\n",
    "new_row.index = new_row.index.date\n",
    "sp500 = pd.concat([sp500, new_row])\n",
    "sp500 = sp500.drop(sp500.index[-1])\n",
    "train = sp500.iloc[:-100]\n",
    "test = sp500.iloc[-100:]\n",
    "\n",
    "#old\n",
    "predictors = [\"Close\", \"Volume\", \"Open\", \"High\"]\n",
    "#, \"Low\", \"VOpen\", \"VAdj Close\", \"VHigh\", \"VLow\"\n",
    "#predictors = [\"Close\", \"Volume\", \"Open\", \"High\", \"Low\"]\n",
    "\n",
    "#The backtest function evaluates a model by training it on historical data \n",
    "#and testing it in increments. It iterates over the data, training the model\n",
    "#on a growing set of past data and testing it on the next segment. \n",
    "#The results are collected and combined into a single DataFrame for analysis.\n",
    "def backtest(data, model, predictors, start=startnumber, step=stepnumber):\n",
    "    all_predictions = []\n",
    "\n",
    "    for i in range(start, data.shape[0], step):\n",
    "        train = data.iloc[0:i].copy()\n",
    "        test = data.iloc[i:(i+step)].copy()\n",
    "        predictions = predict(train, test, predictors, model)\n",
    "        all_predictions.append(predictions)\n",
    "    \n",
    "    return pd.concat(all_predictions)\n",
    "\n",
    "horizons = [2,5,60,250,1000]\n",
    "new_predictors = []\n",
    "\n",
    "for horizon in horizons:\n",
    "    rolling_averages = sp500.rolling(horizon).mean()\n",
    "    \n",
    "    ratio_column = f\"Close_Ratio_{horizon}\"\n",
    "    sp500[ratio_column] = sp500[\"Close\"] / rolling_averages[\"Close\"]\n",
    "    \n",
    "    trend_column = f\"Trend_{horizon}\"\n",
    "    sp500[trend_column] = sp500.shift(1).rolling(horizon).sum()[\"Target\"]\n",
    "    \n",
    "    new_predictors+= [ratio_column, trend_column]\n",
    "\n",
    "sp500 = sp500.dropna(subset=sp500.columns[sp500.columns != \"Tomorrow\"])\n",
    "\n",
    "additional_predictors = [\"techAdj Close\", \"techOpen\"]# \"VOpen\", \"VAdj Close\"] #\"VAdj Close\"]#, \"QQQOpen\", \"QQQAdj Close\"] #[\"VAdj Close\", \"VOpen\", \"VHigh\", \"VLow\"]\n",
    "#\"USDHigh\", \"USDLow\", \"USDAdj Close\", \"USDOpen\", ,  \"techLow\",\"techHigh\", \n",
    "#additional_predictors = [\"Close\", \"Volume\", \"Open\", \"High\", \"Low\"] #[]#\n",
    "new_predictors.extend(additional_predictors)\n",
    "\n",
    "def predict(train, test, predictors, model):\n",
    "        model.fit(train[predictors], train[\"Target\"])\n",
    "        preds = model.predict_proba(test[predictors])[:,1]\n",
    "        preds[preds >=dt] = 1\n",
    "        preds[preds <dt] = 0\n",
    "        preds = pd.Series(preds, index=test.index, name=\"Predictions\")\n",
    "        combined = pd.concat([test[\"Target\"], preds], axis=1)\n",
    "        return combined\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "#\n",
    "for i in range (iterations):\n",
    "    predictionchange = []\n",
    "    weightedpredictionchange = []\n",
    "    weightedbmchange = []\n",
    "    model = RandomForestClassifier(n_estimators=number_of_estimators, max_features=maxfeatures, min_samples_split=minimum_of_samples_split, random_state=i)\n",
    "    predictions = backtest(sp500, model, new_predictors)\n",
    "    #display(f\"Iteration {i + 1}:\")\n",
    "    #display(predictions)\n",
    "    #display(predictions[\"Predictions\"].value_counts())\n",
    "    #display(predictions[\"Target\"].value_counts() / predictions.shape[0])\n",
    "    #display(np.round(precision_score(predictions[\"Target\"], predictions[\"Predictions\"]), decimals=4))\n",
    "    results.append(precision_score(predictions[\"Target\"], predictions[\"Predictions\"]))\n",
    "    \n",
    "    \n",
    "    #predictions already happened what happens after is obtaining data\n",
    "    #newdata\n",
    "    spy = yf.Ticker(theticker)\n",
    "    spy = spy.history(period=\"max\")\n",
    "    #\n",
    "    if not isinstance(spy.index, pd.DatetimeIndex):\n",
    "        spy.index = pd.to_datetime(spy.index)\n",
    "        #\n",
    "    spy.index = spy.index.normalize()\n",
    "    del spy[\"Dividends\"]\n",
    "    del spy[\"Stock Splits\"]\n",
    "    del spy[\"Volume\"]\n",
    "    del spy[\"High\"]\n",
    "    del spy[\"Low\"]\n",
    "    #### what time for change\n",
    "    spy[\"Change\"] = (spy[\"Close\"] - spy[\"Open\"]).shift(-1)\n",
    "    #####\n",
    "    spy['Open']\n",
    "    del spy['Close']\n",
    "    spy.index = spy.index.date\n",
    "    spy = spy.drop(spy.index[1])\n",
    "    display(spy)\n",
    "    total_sum = predictions[\"Predictions\"].value_counts().sum()\n",
    "    preddays6 = []\n",
    "    for idx, value in predictions[\"Predictions\"].items():\n",
    "        if value != 0:\n",
    "            preddays6.append(idx)\n",
    "    predictiondayslists6.append(preddays6)\n",
    "    preddays6 = []\n",
    "    display(total_sum)\n",
    "    spy = spy.iloc[-total_sum:]\n",
    "    predictionfrequency.append(predictions[\"Predictions\"].sum() / len(spy))\n",
    "    \n",
    "    predictions = predictions.iloc[-total_sum:]    #???total_sum\n",
    "    change_column = spy['Change'].tolist()\n",
    "    spy_column = spy['Open'].tolist()\n",
    "    #display(change_column)\n",
    "    predictions['Change'] = change_column\n",
    "    predictions['Weight'] = spy_column\n",
    "    display(predictions)\n",
    "    combination = predictions\n",
    "    plt.figure(figsize=(10, 6))  # Adjust the figure size as needed\n",
    "    plt.plot(predictions.index, predictions['Predictions'], marker='o', linestyle='-')\n",
    "    plt.title('Predictions Over Time')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Predictions')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    #display(spy)\n",
    "    def new_row(row):\n",
    "        if row['Predictions'] == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return row['Change']\n",
    "    predictions['UpdatedChange'] = predictions.apply(new_row, axis=1)\n",
    "    predictions['WeightedChange'] = predictions['UpdatedChange'] / predictions['Weight']\n",
    "    display(predictions)\n",
    "    last_200_rows = predictions.tail(400)\n",
    "    #display(last_200_rows)\n",
    "    for value in predictions['UpdatedChange']:\n",
    "        if value != 0:\n",
    "            predictionchange.append(value)    \n",
    "    avgpredictionchange.append(np.mean(predictionchange))\n",
    "    avgpredictionchangesd.append(np.std(predictionchange))\n",
    "    for value in predictions['WeightedChange']:\n",
    "        if value != 0:\n",
    "            weightedpredictionchange.append(value)    \n",
    "    avgweightedpredictionchange.append(np.mean(weightedpredictionchange))\n",
    "    avgweightedpredictionchangesd.append(np.std(weightedpredictionchange))\n",
    "    ###benchmark stuff\n",
    "    display(combination)\n",
    "    combination[\"BMChange\"] = benchmarkdata[\"BMChange\"]\n",
    "    for idx, value in predictions[\"UpdatedChange\"].items():\n",
    "        if value != 0:\n",
    "            weightedbmchange.append(predictions[\"BMChange\"].loc[idx])\n",
    "        else:\n",
    "            predictions.at[idx, \"BMChange\"] = 0\n",
    "    # Filter out the non-zero elements\n",
    "    combination[\"BMChange\"] = combination[\"WeightedChange\"] - combination[\"BMChange\"]\n",
    "    weightedbmchange = combination[\"BMChange\"][combination[\"BMChange\"] != 0]\n",
    "\n",
    "# Convert the filtered series to a list\n",
    "    weightedbmchange = weightedbmchange.tolist()\n",
    "\n",
    "    avgweightedbenchmarkchange.append(np.mean(weightedbmchange))\n",
    "    avgweightedbenchmarkchangesd.append(np.std(weightedbmchange))\n",
    "    display(combination)\n",
    "\n",
    "\n",
    "print(type(combination))\n",
    "print(type(benchmarkdata))\n",
    "print(benchmarkdata)\n",
    "\n",
    "display(results)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "display(np.round((elapsed_time / 60), decimals=2))\n",
    "display(np.round(((elapsed_time / 60) / iterations), decimals=2))\n",
    "\n",
    "display(np.round(np.mean(avgpredictionchange), decimals=2))\n",
    "display(np.round(np.mean(avgpredictionchangesd), decimals=4))\n",
    "display(np.round(np.mean(avgweightedpredictionchange), decimals=6))\n",
    "display(np.round(np.mean(avgweightedpredictionchangesd), decimals=6))\n",
    "\n",
    "display(np.round(np.mean(predictionfrequency), decimals=4))\n",
    "display(np.round(np.mean(results), decimals=4))\n",
    "display(np.round(np.std(results), decimals=4))\n",
    "\n",
    "dates_when_predictions_is_1 = predictions.index[predictions['Predictions'] == 1].tolist()\n",
    "\n",
    "#print(f\"All dates when 'predictions' is equal to 1: {dates_when_predictions_is_1}\")\n",
    "\n",
    "\n",
    "# Load the CSV file\n",
    "filename = \"/Users/derek/Downloads/DT AI Transformed Data.csv\"\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "# Define your 24 variables (ordered list of values to add)\n",
    "new_data = [\n",
    "    notebook_name, number_of_estimators, startdateunformatted, dt, VIXvar, minimum_of_samples_split,\n",
    "    maxfeatures, GSPCvar, shrtcomment, iterations, NYICDXQQQvar, XLKvar,\n",
    "    startnumber, stepnumber, theticker, np.round(np.mean(results), decimals=4), np.round(np.std(results), decimals=4), np.round(((elapsed_time / 60) / iterations), decimals=2),\n",
    "    np.round((elapsed_time / 60), decimals=2), np.round(np.mean(predictionfrequency), decimals = 4), np.round(np.mean(avgpredictionchange), decimals=2), np.round(np.mean(avgpredictionchangesd), decimals=4), np.round(np.mean(avgweightedpredictionchange), decimals=6), np.round(np.mean(avgweightedpredictionchangesd), decimals=6), np.round(np.mean(avgweightedbenchmarkchange), decimals = 6), np.round(np.mean(avgweightedbenchmarkchangesd), decimals=6), longcomment\n",
    "]\n",
    "\n",
    "# Find the first empty row\n",
    "# Here we assume that the row is considered empty if all columns are NaN or empty strings\n",
    "empty_row_index = df.index[df.isnull().all(axis=1) | (df == '').all(axis=1)].tolist()\n",
    "if empty_row_index:\n",
    "    # If there are empty rows, use the first one\n",
    "    first_empty_row = empty_row_index[0]\n",
    "else:\n",
    "    # If no empty rows, append a new row at the end\n",
    "    first_empty_row = len(df)\n",
    "\n",
    "# Ensure the length of new_data matches the number of columns\n",
    "if len(new_data) != len(df.columns):\n",
    "    raise ValueError(\"Number of new data values does not match number of columns.\")\n",
    "\n",
    "# Add the new data to the determined row\n",
    "df.loc[first_empty_row] = new_data\n",
    "\n",
    "# Save the updated DataFrame back to the CSV file\n",
    "df.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Data successfully added to row {first_empty_row + 1} of the CSV file.\")\n",
    "\n",
    "print(f\"Number of columns in DataFrame: {len(df.columns)}\")\n",
    "print(f\"Number of values in new_data: {len(new_data)}\")\n",
    "\n",
    "print(benchmarkdata)\n",
    "\n",
    "print(np.mean(avgweightedbenchmarkchange))\n",
    "print(np.mean(avgweightedbenchmarkchangesd)) \n",
    "\n",
    "\n",
    "\n",
    "#imports\n",
    "import time\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#manually set data\n",
    "notebook_name = \"Decision Tree Testing Consistency Open Close 4 (BM Target)\"\n",
    "VIXvar = '/OC'\n",
    "maxfeatures = 5 #'sqrt'\n",
    "GSPCvar = 'all - og'\n",
    "#all predictors - the original\n",
    "shrtcomment = ''\n",
    "NYICDXQQQvar = ''\n",
    "XLKvar = ''\n",
    "benchmark = '^GSPC'\n",
    "theticker = 'EEM' #^GSPC\n",
    "predictionsnum = 'N/A'\n",
    "longcomment = ''\n",
    "\n",
    "#settings\n",
    "dt = 0.64\n",
    "\n",
    "number_of_estimators = 1600\n",
    "minimum_of_samples_split = 150\n",
    "startnumber = 3000\n",
    "stepnumber = 220\n",
    "startdateunformatted = '1950-01-01'\n",
    "\n",
    "#prep\n",
    "starting_time = pd.to_datetime(startdateunformatted).date()\n",
    "today = datetime.now()\n",
    "tomorrow = today + timedelta(days=1)\n",
    "tomorrow_str = tomorrow.strftime('%Y-%m-%d')\n",
    "ending_time = tomorrow_str\n",
    "tomorrow_str\n",
    "results = []\n",
    "predictionfrequency = []\n",
    "avgpredictionchange = []\n",
    "avgpredictionchangesd = []\n",
    "avgweightedpredictionchange = []\n",
    "avgweightedpredictionchangesd = []\n",
    "avgweightedbenchmarkchange = []\n",
    "avgweightedbenchmarkchangesd = []\n",
    "predictiondayslists7 = []\n",
    "pd.set_option('display.max_rows', 400)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "#setting up benchmark\n",
    "##\n",
    "benchmarkdata = yf.Ticker(benchmark)\n",
    "benchmarkdata = benchmarkdata.history(period=\"max\")\n",
    "### the way change in what time is recorded\n",
    "#change type\n",
    "benchmarkdata[\"BMChange\"] = ((benchmarkdata[\"Close\"] - benchmarkdata[\"Open\"]) / benchmarkdata[\"Open\"])\n",
    "if not isinstance(benchmarkdata.index, pd.DatetimeIndex):\n",
    "    benchmarkdata.index = pd.to_datetime(benchmarkdata.index)\n",
    "benchmarkdata.index = benchmarkdata.index.normalize()\n",
    "benchmarkdata.index = benchmarkdata.index.date\n",
    "benchmarkdata = benchmarkdata.drop(benchmarkdata.index[1])\n",
    "print(benchmarkdata)\n",
    "\n",
    "#data\n",
    "# Fetch S&P 500 data\n",
    "sp500 = yf.Ticker(theticker)\n",
    "sp500 = sp500.history(period=\"max\")\n",
    "\n",
    "# Ensure the index is a DatetimeIndex\n",
    "if not isinstance(sp500.index, pd.DatetimeIndex):\n",
    "    sp500.index = pd.to_datetime(sp500.index)\n",
    "\n",
    "# Normalize the dates (remove the time component)\n",
    "sp500.index = sp500.index.normalize()\n",
    "\n",
    "# Drop columns if needed\n",
    "if \"Dividends\" in sp500.columns:\n",
    "    del sp500[\"Dividends\"]\n",
    "if \"Stock Splits\" in sp500.columns:\n",
    "    del sp500[\"Stock Splits\"]\n",
    "sp500.index = sp500.index.date\n",
    "display(sp500)\n",
    "sp500[\"Tomorrow\"] = sp500[\"Close\"].shift(-1)\n",
    "######what change were predicting\n",
    "sp500[\"BMChange\"] = (benchmarkdata[\"Close\"] - benchmarkdata[\"Open\"])/benchmarkdata[\"Open\"]\n",
    "sp500[\"Target\"] = (sp500[\"BMChange\"] > ((sp500[\"Close\"] - sp500[\"Open\"])/sp500[\"Open\"])).astype(int).shift(-1)\n",
    "benchmarkdata[\"BMChange\"] = benchmarkdata[\"BMChange\"].shift(-1)\n",
    "sp500 = sp500.drop(columns=['BMChange'])\n",
    "#sp500[\"Target\"] = (sp500[\"Close\"] > sp500[\"Open\"]).astype(int).shift(-1)\n",
    "#####\n",
    "sp500 = sp500.loc[starting_time:].copy()\n",
    "display(sp500)\n",
    "volatility = yf.download(tickers = '^VIX', start = starting_time, end = ending_time)\n",
    "volatility = volatility[['Open', 'Adj Close', 'High', 'Low']]\n",
    "volatility = volatility.rename(columns={'Open': 'VOpen', 'Adj Close': 'VAdj Close', 'High': 'VHigh', 'Low': 'VLow'})\n",
    "volatility.index = volatility.index.normalize()\n",
    "volatility.index = volatility.index.date\n",
    "sp500 = pd.concat([sp500, volatility], axis=1)\n",
    "USD = yf.download(tickers = '^NYICDX', start = starting_time, end = ending_time)\n",
    "USD = USD[['Open', 'Adj Close', 'High', 'Low']]\n",
    "USD = USD.rename(columns={'Open': 'USDOpen', 'Adj Close': 'USDAdj Close', 'High': 'USDHigh', 'Low': 'USDLow'})\n",
    "USD.index = USD.index.normalize()\n",
    "USD.index = USD.index.date\n",
    "sp500 = pd.concat([sp500, USD], axis=1)\n",
    "#QQQ = yf.download(tickers = 'QQQ', start = starting_time, end = ending_time)\n",
    "#QQQ = QQQ[['Open', 'Adj Close', 'High', 'Low']]\n",
    "#QQQ = QQQ.rename(columns={'Open': 'QQQOpen', 'Adj Close': 'QQQAdj Close', 'High': 'QQQHigh', 'Low': 'QQQLow'})\n",
    "#QQQ.index = QQQ.index.normalize()\n",
    "#QQQ.index = QQQ.index.date\n",
    "#sp500 = pd.concat([sp500, QQQ], axis=1)\n",
    "tech = yf.download(tickers = 'XLK', start = starting_time, end = ending_time)\n",
    "tech = tech[['Open', 'Adj Close', 'High', 'Low']]\n",
    "tech = tech.rename(columns={'Open': 'techOpen', 'Adj Close': 'techAdj Close', 'High': 'techHigh', 'Low': 'techLow'})\n",
    "tech.index = tech.index.normalize()\n",
    "tech.index = tech.index.date\n",
    "sp500 = pd.concat([sp500, tech], axis=1)\n",
    "msusfn = yf.download(tickers = 'IXF', start = starting_time, end = ending_time)\n",
    "msusfn = msusfn[['Open', 'Adj Close', 'High', 'Low']]\n",
    "msusfn = msusfn.rename(columns={'Open': 'msusfnOpen', 'Adj Close': 'msusfnAdj Close', 'High': 'msusfnHigh', 'Low': 'msusfnLow'})\n",
    "msusfn.index = msusfn.index.normalize()\n",
    "msusfn.index = msusfn.index.date\n",
    "sp500 = pd.concat([sp500, msusfn], axis=1)\n",
    "new_row = pd.DataFrame([[0] * len(sp500.columns)], columns=sp500.columns, index=[pd.to_datetime(tomorrow_str)])\n",
    "new_row.index = new_row.index.normalize()\n",
    "new_row.index = new_row.index.date\n",
    "sp500 = pd.concat([sp500, new_row])\n",
    "sp500 = sp500.drop(sp500.index[-1])\n",
    "train = sp500.iloc[:-100]\n",
    "test = sp500.iloc[-100:]\n",
    "\n",
    "#old\n",
    "predictors = [\"Close\", \"Volume\", \"Open\", \"High\"]\n",
    "#, \"Low\", \"VOpen\", \"VAdj Close\", \"VHigh\", \"VLow\"\n",
    "#predictors = [\"Close\", \"Volume\", \"Open\", \"High\", \"Low\"]\n",
    "\n",
    "#The backtest function evaluates a model by training it on historical data \n",
    "#and testing it in increments. It iterates over the data, training the model\n",
    "#on a growing set of past data and testing it on the next segment. \n",
    "#The results are collected and combined into a single DataFrame for analysis.\n",
    "def backtest(data, model, predictors, start=startnumber, step=stepnumber):\n",
    "    all_predictions = []\n",
    "\n",
    "    for i in range(start, data.shape[0], step):\n",
    "        train = data.iloc[0:i].copy()\n",
    "        test = data.iloc[i:(i+step)].copy()\n",
    "        predictions = predict(train, test, predictors, model)\n",
    "        all_predictions.append(predictions)\n",
    "    \n",
    "    return pd.concat(all_predictions)\n",
    "\n",
    "horizons = [2,5,60,250,1000]\n",
    "new_predictors = []\n",
    "\n",
    "for horizon in horizons:\n",
    "    rolling_averages = sp500.rolling(horizon).mean()\n",
    "    \n",
    "    ratio_column = f\"Close_Ratio_{horizon}\"\n",
    "    sp500[ratio_column] = sp500[\"Close\"] / rolling_averages[\"Close\"]\n",
    "    \n",
    "    trend_column = f\"Trend_{horizon}\"\n",
    "    sp500[trend_column] = sp500.shift(1).rolling(horizon).sum()[\"Target\"]\n",
    "    \n",
    "    new_predictors+= [ratio_column, trend_column]\n",
    "\n",
    "sp500 = sp500.dropna(subset=sp500.columns[sp500.columns != \"Tomorrow\"])\n",
    "\n",
    "additional_predictors = [\"msusfnOpen\", \"msusfnAdj Close\"]# \"VOpen\", \"VAdj Close\"] #\"VAdj Close\"]#, \"QQQOpen\", \"QQQAdj Close\"] #[\"VAdj Close\", \"VOpen\", \"VHigh\", \"VLow\"]\n",
    "#\"USDHigh\", \"USDLow\", \"USDAdj Close\", \"USDOpen\", ,  \"techLow\",\"techHigh\", \n",
    "#additional_predictors = [\"Close\", \"Volume\", \"Open\", \"High\", \"Low\"] #[]#\n",
    "new_predictors.extend(additional_predictors)\n",
    "\n",
    "def predict(train, test, predictors, model):\n",
    "        model.fit(train[predictors], train[\"Target\"])\n",
    "        preds = model.predict_proba(test[predictors])[:,1]\n",
    "        preds[preds >=dt] = 1\n",
    "        preds[preds <dt] = 0\n",
    "        preds = pd.Series(preds, index=test.index, name=\"Predictions\")\n",
    "        combined = pd.concat([test[\"Target\"], preds], axis=1)\n",
    "        return combined\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "#\n",
    "for i in range (iterations):\n",
    "    predictionchange = []\n",
    "    weightedpredictionchange = []\n",
    "    weightedbmchange = []\n",
    "    model = RandomForestClassifier(n_estimators=number_of_estimators, max_features=maxfeatures, min_samples_split=minimum_of_samples_split, random_state=i)\n",
    "    predictions = backtest(sp500, model, new_predictors)\n",
    "    #display(f\"Iteration {i + 1}:\")\n",
    "    #display(predictions)\n",
    "    #display(predictions[\"Predictions\"].value_counts())\n",
    "    #display(predictions[\"Target\"].value_counts() / predictions.shape[0])\n",
    "    #display(np.round(precision_score(predictions[\"Target\"], predictions[\"Predictions\"]), decimals=4))\n",
    "    results.append(precision_score(predictions[\"Target\"], predictions[\"Predictions\"]))\n",
    "    \n",
    "    \n",
    "    #predictions already happened what happens after is obtaining data\n",
    "    #newdata\n",
    "    spy = yf.Ticker(theticker)\n",
    "    spy = spy.history(period=\"max\")\n",
    "    #\n",
    "    if not isinstance(spy.index, pd.DatetimeIndex):\n",
    "        spy.index = pd.to_datetime(spy.index)\n",
    "        #\n",
    "    spy.index = spy.index.normalize()\n",
    "    del spy[\"Dividends\"]\n",
    "    del spy[\"Stock Splits\"]\n",
    "    del spy[\"Volume\"]\n",
    "    del spy[\"High\"]\n",
    "    del spy[\"Low\"]\n",
    "    #### what time for change\n",
    "    spy[\"Change\"] = (spy[\"Close\"] - spy[\"Open\"]).shift(-1)\n",
    "    #####\n",
    "    spy['Open']\n",
    "    del spy['Close']\n",
    "    spy.index = spy.index.date\n",
    "    spy = spy.drop(spy.index[1])\n",
    "    display(spy)\n",
    "    total_sum = predictions[\"Predictions\"].value_counts().sum()\n",
    "    preddays7 = []\n",
    "    for idx, value in predictions[\"Predictions\"].items():\n",
    "        if value != 0:\n",
    "            preddays7.append(idx)\n",
    "    predictiondayslists7.append(preddays7)\n",
    "    preddays7 = []\n",
    "    display(total_sum)\n",
    "    spy = spy.iloc[-total_sum:]\n",
    "    predictionfrequency.append(predictions[\"Predictions\"].sum() / len(spy))\n",
    "    \n",
    "    predictions = predictions.iloc[-total_sum:]    #???total_sum\n",
    "    change_column = spy['Change'].tolist()\n",
    "    spy_column = spy['Open'].tolist()\n",
    "    #display(change_column)\n",
    "    predictions['Change'] = change_column\n",
    "    predictions['Weight'] = spy_column\n",
    "    display(predictions)\n",
    "    combination = predictions\n",
    "    plt.figure(figsize=(10, 6))  # Adjust the figure size as needed\n",
    "    plt.plot(predictions.index, predictions['Predictions'], marker='o', linestyle='-')\n",
    "    plt.title('Predictions Over Time')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Predictions')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    #display(spy)\n",
    "    def new_row(row):\n",
    "        if row['Predictions'] == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return row['Change']\n",
    "    predictions['UpdatedChange'] = predictions.apply(new_row, axis=1)\n",
    "    predictions['WeightedChange'] = predictions['UpdatedChange'] / predictions['Weight']\n",
    "    display(predictions)\n",
    "    last_200_rows = predictions.tail(400)\n",
    "    #display(last_200_rows)\n",
    "    for value in predictions['UpdatedChange']:\n",
    "        if value != 0:\n",
    "            predictionchange.append(value)    \n",
    "    avgpredictionchange.append(np.mean(predictionchange))\n",
    "    avgpredictionchangesd.append(np.std(predictionchange))\n",
    "    for value in predictions['WeightedChange']:\n",
    "        if value != 0:\n",
    "            weightedpredictionchange.append(value)    \n",
    "    avgweightedpredictionchange.append(np.mean(weightedpredictionchange))\n",
    "    avgweightedpredictionchangesd.append(np.std(weightedpredictionchange))\n",
    "    ###benchmark stuff\n",
    "    display(combination)\n",
    "    combination[\"BMChange\"] = benchmarkdata[\"BMChange\"]\n",
    "    for idx, value in predictions[\"UpdatedChange\"].items():\n",
    "        if value != 0:\n",
    "            weightedbmchange.append(predictions[\"BMChange\"].loc[idx])\n",
    "        else:\n",
    "            predictions.at[idx, \"BMChange\"] = 0\n",
    "    # Filter out the non-zero elements\n",
    "    combination[\"BMChange\"] = combination[\"WeightedChange\"] - combination[\"BMChange\"]\n",
    "    weightedbmchange = combination[\"BMChange\"][combination[\"BMChange\"] != 0]\n",
    "\n",
    "# Convert the filtered series to a list\n",
    "    weightedbmchange = weightedbmchange.tolist()\n",
    "\n",
    "    avgweightedbenchmarkchange.append(np.mean(weightedbmchange))\n",
    "    avgweightedbenchmarkchangesd.append(np.std(weightedbmchange))\n",
    "    display(combination)\n",
    "\n",
    "\n",
    "print(type(combination))\n",
    "print(type(benchmarkdata))\n",
    "print(benchmarkdata)\n",
    "\n",
    "display(results)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "display(np.round((elapsed_time / 60), decimals=2))\n",
    "display(np.round(((elapsed_time / 60) / iterations), decimals=2))\n",
    "\n",
    "display(np.round(np.mean(avgpredictionchange), decimals=2))\n",
    "display(np.round(np.mean(avgpredictionchangesd), decimals=4))\n",
    "display(np.round(np.mean(avgweightedpredictionchange), decimals=6))\n",
    "display(np.round(np.mean(avgweightedpredictionchangesd), decimals=6))\n",
    "\n",
    "display(np.round(np.mean(predictionfrequency), decimals=4))\n",
    "display(np.round(np.mean(results), decimals=4))\n",
    "display(np.round(np.std(results), decimals=4))\n",
    "\n",
    "dates_when_predictions_is_1 = predictions.index[predictions['Predictions'] == 1].tolist()\n",
    "\n",
    "#print(f\"All dates when 'predictions' is equal to 1: {dates_when_predictions_is_1}\")\n",
    "\n",
    "\n",
    "# Load the CSV file\n",
    "filename = \"/Users/derek/Downloads/DT AI Transformed Data.csv\"\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "# Define your 24 variables (ordered list of values to add)\n",
    "new_data = [\n",
    "    notebook_name, number_of_estimators, startdateunformatted, dt, VIXvar, minimum_of_samples_split,\n",
    "    maxfeatures, GSPCvar, shrtcomment, iterations, NYICDXQQQvar, XLKvar,\n",
    "    startnumber, stepnumber, theticker, np.round(np.mean(results), decimals=4), np.round(np.std(results), decimals=4), np.round(((elapsed_time / 60) / iterations), decimals=2),\n",
    "    np.round((elapsed_time / 60), decimals=2), np.round(np.mean(predictionfrequency), decimals = 4), np.round(np.mean(avgpredictionchange), decimals=2), np.round(np.mean(avgpredictionchangesd), decimals=4), np.round(np.mean(avgweightedpredictionchange), decimals=6), np.round(np.mean(avgweightedpredictionchangesd), decimals=6), np.round(np.mean(avgweightedbenchmarkchange), decimals = 6), np.round(np.mean(avgweightedbenchmarkchangesd), decimals=6), longcomment\n",
    "]\n",
    "\n",
    "# Find the first empty row\n",
    "# Here we assume that the row is considered empty if all columns are NaN or empty strings\n",
    "empty_row_index = df.index[df.isnull().all(axis=1) | (df == '').all(axis=1)].tolist()\n",
    "if empty_row_index:\n",
    "    # If there are empty rows, use the first one\n",
    "    first_empty_row = empty_row_index[0]\n",
    "else:\n",
    "    # If no empty rows, append a new row at the end\n",
    "    first_empty_row = len(df)\n",
    "\n",
    "# Ensure the length of new_data matches the number of columns\n",
    "if len(new_data) != len(df.columns):\n",
    "    raise ValueError(\"Number of new data values does not match number of columns.\")\n",
    "\n",
    "# Add the new data to the determined row\n",
    "df.loc[first_empty_row] = new_data\n",
    "\n",
    "# Save the updated DataFrame back to the CSV file\n",
    "df.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Data successfully added to row {first_empty_row + 1} of the CSV file.\")\n",
    "\n",
    "print(f\"Number of columns in DataFrame: {len(df.columns)}\")\n",
    "print(f\"Number of values in new_data: {len(new_data)}\")\n",
    "\n",
    "print(benchmarkdata)\n",
    "\n",
    "print(np.mean(avgweightedbenchmarkchange))\n",
    "print(np.mean(avgweightedbenchmarkchangesd)) \n",
    "\n",
    "\n",
    "\n",
    "#imports\n",
    "import time\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#manually set data\n",
    "notebook_name = \"Decision Tree Testing Consistency Open Close 4 (BM Target)\"\n",
    "VIXvar = ''\n",
    "maxfeatures = 5 #'sqrt'\n",
    "GSPCvar = 'all - og'\n",
    "#all predictors - the original\n",
    "shrtcomment = 'HealthOCL'\n",
    "NYICDXQQQvar = ''\n",
    "XLKvar = ''\n",
    "benchmark = '^GSPC'\n",
    "theticker = 'IYH' #^GSPC\n",
    "predictionsnum = 'N/A'\n",
    "longcomment = ''\n",
    "\n",
    "#settings\n",
    "dt = 0.625\n",
    "\n",
    "number_of_estimators = 1600\n",
    "minimum_of_samples_split = 150\n",
    "startnumber = 3000\n",
    "stepnumber = 220\n",
    "startdateunformatted = '1950-01-01'\n",
    "\n",
    "#prep\n",
    "starting_time = pd.to_datetime(startdateunformatted).date()\n",
    "today = datetime.now()\n",
    "tomorrow = today + timedelta(days=1)\n",
    "tomorrow_str = tomorrow.strftime('%Y-%m-%d')\n",
    "ending_time = tomorrow_str\n",
    "tomorrow_str\n",
    "results = []\n",
    "predictionfrequency = []\n",
    "avgpredictionchange = []\n",
    "avgpredictionchangesd = []\n",
    "avgweightedpredictionchange = []\n",
    "avgweightedpredictionchangesd = []\n",
    "avgweightedbenchmarkchange = []\n",
    "avgweightedbenchmarkchangesd = []\n",
    "predictiondayslists8 = []\n",
    "pd.set_option('display.max_rows', 400)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "#setting up benchmark\n",
    "##\n",
    "benchmarkdata = yf.Ticker(benchmark)\n",
    "benchmarkdata = benchmarkdata.history(period=\"max\")\n",
    "### the way change in what time is recorded\n",
    "#change type\n",
    "benchmarkdata[\"BMChange\"] = ((benchmarkdata[\"Close\"] - benchmarkdata[\"Open\"]) / benchmarkdata[\"Open\"])\n",
    "if not isinstance(benchmarkdata.index, pd.DatetimeIndex):\n",
    "    benchmarkdata.index = pd.to_datetime(benchmarkdata.index)\n",
    "benchmarkdata.index = benchmarkdata.index.normalize()\n",
    "benchmarkdata.index = benchmarkdata.index.date\n",
    "benchmarkdata = benchmarkdata.drop(benchmarkdata.index[1])\n",
    "print(benchmarkdata)\n",
    "\n",
    "#data\n",
    "# Fetch S&P 500 data\n",
    "sp500 = yf.Ticker(theticker)\n",
    "sp500 = sp500.history(period=\"max\")\n",
    "\n",
    "# Ensure the index is a DatetimeIndex\n",
    "if not isinstance(sp500.index, pd.DatetimeIndex):\n",
    "    sp500.index = pd.to_datetime(sp500.index)\n",
    "\n",
    "# Normalize the dates (remove the time component)\n",
    "sp500.index = sp500.index.normalize()\n",
    "\n",
    "# Drop columns if needed\n",
    "if \"Dividends\" in sp500.columns:\n",
    "    del sp500[\"Dividends\"]\n",
    "if \"Stock Splits\" in sp500.columns:\n",
    "    del sp500[\"Stock Splits\"]\n",
    "sp500.index = sp500.index.date\n",
    "display(sp500)\n",
    "sp500[\"Tomorrow\"] = sp500[\"Close\"].shift(-1)\n",
    "######what change were predicting\n",
    "sp500[\"BMChange\"] = (benchmarkdata[\"Close\"] - benchmarkdata[\"Open\"])/benchmarkdata[\"Open\"]\n",
    "sp500[\"Target\"] = (sp500[\"BMChange\"] > ((sp500[\"Close\"] - sp500[\"Open\"])/sp500[\"Open\"])).astype(int).shift(-1)\n",
    "benchmarkdata[\"BMChange\"] = benchmarkdata[\"BMChange\"].shift(-1)\n",
    "sp500 = sp500.drop(columns=['BMChange'])\n",
    "#sp500[\"Target\"] = (sp500[\"Close\"] > sp500[\"Open\"]).astype(int).shift(-1)\n",
    "#####\n",
    "sp500 = sp500.loc[starting_time:].copy()\n",
    "display(sp500)\n",
    "volatility = yf.download(tickers = '^VIX', start = starting_time, end = ending_time)\n",
    "volatility = volatility[['Open', 'Adj Close', 'High', 'Low']]\n",
    "volatility = volatility.rename(columns={'Open': 'VOpen', 'Adj Close': 'VAdj Close', 'High': 'VHigh', 'Low': 'VLow'})\n",
    "volatility.index = volatility.index.normalize()\n",
    "volatility.index = volatility.index.date\n",
    "sp500 = pd.concat([sp500, volatility], axis=1)\n",
    "USD = yf.download(tickers = '^NYICDX', start = starting_time, end = ending_time)\n",
    "USD = USD[['Open', 'Adj Close', 'High', 'Low']]\n",
    "USD = USD.rename(columns={'Open': 'USDOpen', 'Adj Close': 'USDAdj Close', 'High': 'USDHigh', 'Low': 'USDLow'})\n",
    "USD.index = USD.index.normalize()\n",
    "USD.index = USD.index.date\n",
    "sp500 = pd.concat([sp500, USD], axis=1)\n",
    "#QQQ = yf.download(tickers = 'QQQ', start = starting_time, end = ending_time)\n",
    "#QQQ = QQQ[['Open', 'Adj Close', 'High', 'Low']]\n",
    "#QQQ = QQQ.rename(columns={'Open': 'QQQOpen', 'Adj Close': 'QQQAdj Close', 'High': 'QQQHigh', 'Low': 'QQQLow'})\n",
    "#QQQ.index = QQQ.index.normalize()\n",
    "#QQQ.index = QQQ.index.date\n",
    "#sp500 = pd.concat([sp500, QQQ], axis=1)\n",
    "Health = yf.download(tickers = '^SP500-35', start = starting_time, end = ending_time)\n",
    "Health = Health[['Open', 'Adj Close', 'High', 'Low']]\n",
    "Health = Health.rename(columns={'Open': 'HealthOpen', 'Adj Close': 'HealthAdj Close', 'High': 'HealthHigh', 'Low': 'HealthLow'})\n",
    "Health.index = Health.index.normalize()\n",
    "Health.index = Health.index.date\n",
    "sp500 = pd.concat([sp500, Health], axis=1)\n",
    "tech = yf.download(tickers = 'XLK', start = starting_time, end = ending_time)\n",
    "tech = tech[['Open', 'Adj Close', 'High', 'Low']]\n",
    "tech = tech.rename(columns={'Open': 'techOpen', 'Adj Close': 'techAdj Close', 'High': 'techHigh', 'Low': 'techLow'})\n",
    "tech.index = tech.index.normalize()\n",
    "tech.index = tech.index.date\n",
    "sp500 = pd.concat([sp500, tech], axis=1)\n",
    "msusfn = yf.download(tickers = 'IXF', start = starting_time, end = ending_time)\n",
    "msusfn = msusfn[['Open', 'Adj Close', 'High', 'Low']]\n",
    "msusfn = msusfn.rename(columns={'Open': 'msusfnOpen', 'Adj Close': 'msusfnAdj Close', 'High': 'msusfnHigh', 'Low': 'msusfnLow'})\n",
    "msusfn.index = msusfn.index.normalize()\n",
    "msusfn.index = msusfn.index.date\n",
    "sp500 = pd.concat([sp500, msusfn], axis=1)\n",
    "new_row = pd.DataFrame([[0] * len(sp500.columns)], columns=sp500.columns, index=[pd.to_datetime(tomorrow_str)])\n",
    "new_row.index = new_row.index.normalize()\n",
    "new_row.index = new_row.index.date\n",
    "sp500 = pd.concat([sp500, new_row])\n",
    "sp500 = sp500.drop(sp500.index[-1])\n",
    "train = sp500.iloc[:-100]\n",
    "test = sp500.iloc[-100:]\n",
    "\n",
    "#old\n",
    "predictors = [\"Close\", \"Volume\", \"Open\", \"High\"]\n",
    "#, \"Low\", \"VOpen\", \"VAdj Close\", \"VHigh\", \"VLow\"\n",
    "#predictors = [\"Close\", \"Volume\", \"Open\", \"High\", \"Low\"]\n",
    "\n",
    "#The backtest function evaluates a model by training it on historical data \n",
    "#and testing it in increments. It iterates over the data, training the model\n",
    "#on a growing set of past data and testing it on the next segment. \n",
    "#The results are collected and combined into a single DataFrame for analysis.\n",
    "def backtest(data, model, predictors, start=startnumber, step=stepnumber):\n",
    "    all_predictions = []\n",
    "\n",
    "    for i in range(start, data.shape[0], step):\n",
    "        train = data.iloc[0:i].copy()\n",
    "        test = data.iloc[i:(i+step)].copy()\n",
    "        predictions = predict(train, test, predictors, model)\n",
    "        all_predictions.append(predictions)\n",
    "    \n",
    "    return pd.concat(all_predictions)\n",
    "\n",
    "horizons = [2,5,60,250,1000]\n",
    "new_predictors = []\n",
    "\n",
    "for horizon in horizons:\n",
    "    rolling_averages = sp500.rolling(horizon).mean()\n",
    "    \n",
    "    ratio_column = f\"Close_Ratio_{horizon}\"\n",
    "    sp500[ratio_column] = sp500[\"Close\"] / rolling_averages[\"Close\"]\n",
    "    \n",
    "    trend_column = f\"Trend_{horizon}\"\n",
    "    sp500[trend_column] = sp500.shift(1).rolling(horizon).sum()[\"Target\"]\n",
    "    \n",
    "    new_predictors+= [ratio_column, trend_column]\n",
    "\n",
    "sp500 = sp500.dropna(subset=sp500.columns[sp500.columns != \"Tomorrow\"])\n",
    "\n",
    "additional_predictors = [\"HealthOpen\", \"HealthAdj Close\", \"HealthLow\"]\n",
    "    #\"msusfnOpen\", \"msusfnAdj Close\"]# \"VOpen\", \"VAdj Close\"] #\"VAdj Close\"]#, \"QQQOpen\", \"QQQAdj Close\"] #[\"VAdj Close\", \"VOpen\", \"VHigh\", \"VLow\"]\n",
    "#\"USDHigh\", \"USDLow\", \"USDAdj Close\", \"USDOpen\", ,\"techOpen\", \"techAdj Close\"  \"techLow\",\"techHigh\", \n",
    "#additional_predictors = [\"Close\", \"Volume\", \"Open\", \"High\", \"Low\"] #[]#\n",
    "new_predictors.extend(additional_predictors)\n",
    "\n",
    "def predict(train, test, predictors, model):\n",
    "        model.fit(train[predictors], train[\"Target\"])\n",
    "        preds = model.predict_proba(test[predictors])[:,1]\n",
    "        preds[preds >=dt] = 1\n",
    "        preds[preds <dt] = 0\n",
    "        preds = pd.Series(preds, index=test.index, name=\"Predictions\")\n",
    "        combined = pd.concat([test[\"Target\"], preds], axis=1)\n",
    "        return combined\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "#\n",
    "for i in range (iterations):\n",
    "    predictionchange = []\n",
    "    weightedpredictionchange = []\n",
    "    weightedbmchange = []\n",
    "    model = RandomForestClassifier(n_estimators=number_of_estimators, max_features=maxfeatures, min_samples_split=minimum_of_samples_split, random_state=i)\n",
    "    predictions = backtest(sp500, model, new_predictors)\n",
    "    #display(f\"Iteration {i + 1}:\")\n",
    "    #display(predictions)\n",
    "    #display(predictions[\"Predictions\"].value_counts())\n",
    "    #display(predictions[\"Target\"].value_counts() / predictions.shape[0])\n",
    "    #display(np.round(precision_score(predictions[\"Target\"], predictions[\"Predictions\"]), decimals=4))\n",
    "    results.append(precision_score(predictions[\"Target\"], predictions[\"Predictions\"]))\n",
    "    \n",
    "    \n",
    "    #predictions already happened what happens after is obtaining data\n",
    "    #newdata\n",
    "    spy = yf.Ticker(theticker)\n",
    "    spy = spy.history(period=\"max\")\n",
    "    #\n",
    "    if not isinstance(spy.index, pd.DatetimeIndex):\n",
    "        spy.index = pd.to_datetime(spy.index)\n",
    "        #\n",
    "    spy.index = spy.index.normalize()\n",
    "    del spy[\"Dividends\"]\n",
    "    del spy[\"Stock Splits\"]\n",
    "    del spy[\"Volume\"]\n",
    "    del spy[\"High\"]\n",
    "    del spy[\"Low\"]\n",
    "    #### what time for change\n",
    "    spy[\"Change\"] = (spy[\"Close\"] - spy[\"Open\"]).shift(-1)\n",
    "    #####\n",
    "    spy['Open']\n",
    "    del spy['Close']\n",
    "    spy.index = spy.index.date\n",
    "    spy = spy.drop(spy.index[1])\n",
    "    display(spy)\n",
    "    total_sum = predictions[\"Predictions\"].value_counts().sum()\n",
    "    preddays8 = []\n",
    "    for idx, value in predictions[\"Predictions\"].items():\n",
    "        if value != 0:\n",
    "            preddays8.append(idx)\n",
    "    predictiondayslists8.append(preddays8)\n",
    "    preddays8 = []\n",
    "    display(total_sum)\n",
    "    spy = spy.iloc[-total_sum:]\n",
    "    predictionfrequency.append(predictions[\"Predictions\"].sum() / len(spy))\n",
    "    \n",
    "    predictions = predictions.iloc[-total_sum:]    #???total_sum\n",
    "    change_column = spy['Change'].tolist()\n",
    "    spy_column = spy['Open'].tolist()\n",
    "    #display(change_column)\n",
    "    predictions['Change'] = change_column\n",
    "    predictions['Weight'] = spy_column\n",
    "    display(predictions)\n",
    "    combination = predictions\n",
    "    plt.figure(figsize=(10, 6))  # Adjust the figure size as needed\n",
    "    plt.plot(predictions.index, predictions['Predictions'], marker='o', linestyle='-')\n",
    "    plt.title('Predictions Over Time')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Predictions')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    #display(spy)\n",
    "    def new_row(row):\n",
    "        if row['Predictions'] == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return row['Change']\n",
    "    predictions['UpdatedChange'] = predictions.apply(new_row, axis=1)\n",
    "    predictions['WeightedChange'] = predictions['UpdatedChange'] / predictions['Weight']\n",
    "    display(predictions)\n",
    "    last_200_rows = predictions.tail(400)\n",
    "    #display(last_200_rows)\n",
    "    for value in predictions['UpdatedChange']:\n",
    "        if value != 0:\n",
    "            predictionchange.append(value)    \n",
    "    avgpredictionchange.append(np.mean(predictionchange))\n",
    "    avgpredictionchangesd.append(np.std(predictionchange))\n",
    "    for value in predictions['WeightedChange']:\n",
    "        if value != 0:\n",
    "            weightedpredictionchange.append(value)    \n",
    "    avgweightedpredictionchange.append(np.mean(weightedpredictionchange))\n",
    "    avgweightedpredictionchangesd.append(np.std(weightedpredictionchange))\n",
    "    ###benchmark stuff\n",
    "    display(combination)\n",
    "    combination[\"BMChange\"] = benchmarkdata[\"BMChange\"]\n",
    "    for idx, value in predictions[\"UpdatedChange\"].items():\n",
    "        if value != 0:\n",
    "            weightedbmchange.append(predictions[\"BMChange\"].loc[idx])\n",
    "        else:\n",
    "            predictions.at[idx, \"BMChange\"] = 0\n",
    "    # Filter out the non-zero elements\n",
    "    combination[\"BMChange\"] = combination[\"WeightedChange\"] - combination[\"BMChange\"]\n",
    "    weightedbmchange = combination[\"BMChange\"][combination[\"BMChange\"] != 0]\n",
    "\n",
    "# Convert the filtered series to a list\n",
    "    weightedbmchange = weightedbmchange.tolist()\n",
    "\n",
    "    avgweightedbenchmarkchange.append(np.mean(weightedbmchange))\n",
    "    avgweightedbenchmarkchangesd.append(np.std(weightedbmchange))\n",
    "    display(combination)\n",
    "\n",
    "\n",
    "print(type(combination))\n",
    "print(type(benchmarkdata))\n",
    "print(benchmarkdata)\n",
    "\n",
    "display(results)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "display(np.round((elapsed_time / 60), decimals=2))\n",
    "display(np.round(((elapsed_time / 60) / iterations), decimals=2))\n",
    "\n",
    "display(np.round(np.mean(avgpredictionchange), decimals=2))\n",
    "display(np.round(np.mean(avgpredictionchangesd), decimals=4))\n",
    "display(np.round(np.mean(avgweightedpredictionchange), decimals=6))\n",
    "display(np.round(np.mean(avgweightedpredictionchangesd), decimals=6))\n",
    "\n",
    "display(np.round(np.mean(predictionfrequency), decimals=4))\n",
    "display(np.round(np.mean(results), decimals=4))\n",
    "display(np.round(np.std(results), decimals=4))\n",
    "\n",
    "dates_when_predictions_is_1 = predictions.index[predictions['Predictions'] == 1].tolist()\n",
    "\n",
    "#print(f\"All dates when 'predictions' is equal to 1: {dates_when_predictions_is_1}\")\n",
    "\n",
    "\n",
    "# Load the CSV file\n",
    "filename = \"/Users/derek/Downloads/DT AI Transformed Data.csv\"\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "# Define your 24 variables (ordered list of values to add)\n",
    "new_data = [\n",
    "    notebook_name, number_of_estimators, startdateunformatted, dt, VIXvar, minimum_of_samples_split,\n",
    "    maxfeatures, GSPCvar, shrtcomment, iterations, NYICDXQQQvar, XLKvar,\n",
    "    startnumber, stepnumber, theticker, np.round(np.mean(results), decimals=4), np.round(np.std(results), decimals=4), np.round(((elapsed_time / 60) / iterations), decimals=2),\n",
    "    np.round((elapsed_time / 60), decimals=2), np.round(np.mean(predictionfrequency), decimals = 4), np.round(np.mean(avgpredictionchange), decimals=2), np.round(np.mean(avgpredictionchangesd), decimals=4), np.round(np.mean(avgweightedpredictionchange), decimals=6), np.round(np.mean(avgweightedpredictionchangesd), decimals=6), np.round(np.mean(avgweightedbenchmarkchange), decimals = 6), np.round(np.mean(avgweightedbenchmarkchangesd), decimals=6), longcomment\n",
    "]\n",
    "\n",
    "# Find the first empty row\n",
    "# Here we assume that the row is considered empty if all columns are NaN or empty strings\n",
    "empty_row_index = df.index[df.isnull().all(axis=1) | (df == '').all(axis=1)].tolist()\n",
    "if empty_row_index:\n",
    "    # If there are empty rows, use the first one\n",
    "    first_empty_row = empty_row_index[0]\n",
    "else:\n",
    "    # If no empty rows, append a new row at the end\n",
    "    first_empty_row = len(df)\n",
    "\n",
    "# Ensure the length of new_data matches the number of columns\n",
    "if len(new_data) != len(df.columns):\n",
    "    raise ValueError(\"Number of new data values does not match number of columns.\")\n",
    "\n",
    "# Add the new data to the determined row\n",
    "df.loc[first_empty_row] = new_data\n",
    "\n",
    "# Save the updated DataFrame back to the CSV file\n",
    "df.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Data successfully added to row {first_empty_row + 1} of the CSV file.\")\n",
    "\n",
    "print(f\"Number of columns in DataFrame: {len(df.columns)}\")\n",
    "print(f\"Number of values in new_data: {len(new_data)}\")\n",
    "\n",
    "print(benchmarkdata)\n",
    "\n",
    "print(np.mean(avgweightedbenchmarkchange))\n",
    "print(np.mean(avgweightedbenchmarkchangesd)) \n",
    "\n",
    "\n",
    "\n",
    "#imports\n",
    "import time\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#manually set data\n",
    "notebook_name = \"Decision Tree Testing Consistency Open Close 4 (BM Target)\"\n",
    "VIXvar = ''\n",
    "maxfeatures = 5 #'sqrt'\n",
    "GSPCvar = 'all - og'\n",
    "#all predictors - the original\n",
    "shrtcomment = 'HealthOC'\n",
    "NYICDXQQQvar = ''\n",
    "XLKvar = ''\n",
    "benchmark = '^GSPC'\n",
    "theticker = 'XLV' #^GSPC\n",
    "predictionsnum = 'N/A'\n",
    "longcomment = ''\n",
    "\n",
    "#settings\n",
    "dt = 0.6\n",
    "\n",
    "number_of_estimators = 1600\n",
    "minimum_of_samples_split = 150\n",
    "startnumber = 3000\n",
    "stepnumber = 220\n",
    "startdateunformatted = '1950-01-01'\n",
    "\n",
    "#prep\n",
    "starting_time = pd.to_datetime(startdateunformatted).date()\n",
    "today = datetime.now()\n",
    "tomorrow = today + timedelta(days=1)\n",
    "tomorrow_str = tomorrow.strftime('%Y-%m-%d')\n",
    "ending_time = tomorrow_str\n",
    "tomorrow_str\n",
    "results = []\n",
    "predictionfrequency = []\n",
    "avgpredictionchange = []\n",
    "avgpredictionchangesd = []\n",
    "avgweightedpredictionchange = []\n",
    "avgweightedpredictionchangesd = []\n",
    "avgweightedbenchmarkchange = []\n",
    "avgweightedbenchmarkchangesd = []\n",
    "predictiondayslists9 = []\n",
    "pd.set_option('display.max_rows', 400)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "#setting up benchmark\n",
    "##\n",
    "benchmarkdata = yf.Ticker(benchmark)\n",
    "benchmarkdata = benchmarkdata.history(period=\"max\")\n",
    "### the way change in what time is recorded\n",
    "#change type\n",
    "benchmarkdata[\"BMChange\"] = ((benchmarkdata[\"Close\"] - benchmarkdata[\"Open\"]) / benchmarkdata[\"Open\"])\n",
    "if not isinstance(benchmarkdata.index, pd.DatetimeIndex):\n",
    "    benchmarkdata.index = pd.to_datetime(benchmarkdata.index)\n",
    "benchmarkdata.index = benchmarkdata.index.normalize()\n",
    "benchmarkdata.index = benchmarkdata.index.date\n",
    "benchmarkdata = benchmarkdata.drop(benchmarkdata.index[1])\n",
    "print(benchmarkdata)\n",
    "\n",
    "#data\n",
    "# Fetch S&P 500 data\n",
    "sp500 = yf.Ticker(theticker)\n",
    "sp500 = sp500.history(period=\"max\")\n",
    "\n",
    "# Ensure the index is a DatetimeIndex\n",
    "if not isinstance(sp500.index, pd.DatetimeIndex):\n",
    "    sp500.index = pd.to_datetime(sp500.index)\n",
    "\n",
    "# Normalize the dates (remove the time component)\n",
    "sp500.index = sp500.index.normalize()\n",
    "\n",
    "# Drop columns if needed\n",
    "if \"Dividends\" in sp500.columns:\n",
    "    del sp500[\"Dividends\"]\n",
    "if \"Stock Splits\" in sp500.columns:\n",
    "    del sp500[\"Stock Splits\"]\n",
    "sp500.index = sp500.index.date\n",
    "display(sp500)\n",
    "sp500[\"Tomorrow\"] = sp500[\"Close\"].shift(-1)\n",
    "######what change were predicting\n",
    "sp500[\"BMChange\"] = (benchmarkdata[\"Close\"] - benchmarkdata[\"Open\"])/benchmarkdata[\"Open\"]\n",
    "sp500[\"Target\"] = (sp500[\"BMChange\"] > ((sp500[\"Close\"] - sp500[\"Open\"])/sp500[\"Open\"])).astype(int).shift(-1)\n",
    "benchmarkdata[\"BMChange\"] = benchmarkdata[\"BMChange\"].shift(-1)\n",
    "sp500 = sp500.drop(columns=['BMChange'])\n",
    "#sp500[\"Target\"] = (sp500[\"Close\"] > sp500[\"Open\"]).astype(int).shift(-1)\n",
    "#####\n",
    "sp500 = sp500.loc[starting_time:].copy()\n",
    "display(sp500)\n",
    "volatility = yf.download(tickers = '^VIX', start = starting_time, end = ending_time)\n",
    "volatility = volatility[['Open', 'Adj Close', 'High', 'Low']]\n",
    "volatility = volatility.rename(columns={'Open': 'VOpen', 'Adj Close': 'VAdj Close', 'High': 'VHigh', 'Low': 'VLow'})\n",
    "volatility.index = volatility.index.normalize()\n",
    "volatility.index = volatility.index.date\n",
    "sp500 = pd.concat([sp500, volatility], axis=1)\n",
    "USD = yf.download(tickers = '^NYICDX', start = starting_time, end = ending_time)\n",
    "USD = USD[['Open', 'Adj Close', 'High', 'Low']]\n",
    "USD = USD.rename(columns={'Open': 'USDOpen', 'Adj Close': 'USDAdj Close', 'High': 'USDHigh', 'Low': 'USDLow'})\n",
    "USD.index = USD.index.normalize()\n",
    "USD.index = USD.index.date\n",
    "sp500 = pd.concat([sp500, USD], axis=1)\n",
    "#QQQ = yf.download(tickers = 'QQQ', start = starting_time, end = ending_time)\n",
    "#QQQ = QQQ[['Open', 'Adj Close', 'High', 'Low']]\n",
    "#QQQ = QQQ.rename(columns={'Open': 'QQQOpen', 'Adj Close': 'QQQAdj Close', 'High': 'QQQHigh', 'Low': 'QQQLow'})\n",
    "#QQQ.index = QQQ.index.normalize()\n",
    "#QQQ.index = QQQ.index.date\n",
    "#sp500 = pd.concat([sp500, QQQ], axis=1)\n",
    "Health = yf.download(tickers = '^SP500-35', start = starting_time, end = ending_time)\n",
    "Health = Health[['Open', 'Adj Close', 'High', 'Low']]\n",
    "Health = Health.rename(columns={'Open': 'HealthOpen', 'Adj Close': 'HealthAdj Close', 'High': 'HealthHigh', 'Low': 'HealthLow'})\n",
    "Health.index = Health.index.normalize()\n",
    "Health.index = Health.index.date\n",
    "sp500 = pd.concat([sp500, Health], axis=1)\n",
    "tech = yf.download(tickers = 'XLK', start = starting_time, end = ending_time)\n",
    "tech = tech[['Open', 'Adj Close', 'High', 'Low']]\n",
    "tech = tech.rename(columns={'Open': 'techOpen', 'Adj Close': 'techAdj Close', 'High': 'techHigh', 'Low': 'techLow'})\n",
    "tech.index = tech.index.normalize()\n",
    "tech.index = tech.index.date\n",
    "sp500 = pd.concat([sp500, tech], axis=1)\n",
    "msusfn = yf.download(tickers = 'IXF', start = starting_time, end = ending_time)\n",
    "msusfn = msusfn[['Open', 'Adj Close', 'High', 'Low']]\n",
    "msusfn = msusfn.rename(columns={'Open': 'msusfnOpen', 'Adj Close': 'msusfnAdj Close', 'High': 'msusfnHigh', 'Low': 'msusfnLow'})\n",
    "msusfn.index = msusfn.index.normalize()\n",
    "msusfn.index = msusfn.index.date\n",
    "sp500 = pd.concat([sp500, msusfn], axis=1)\n",
    "new_row = pd.DataFrame([[0] * len(sp500.columns)], columns=sp500.columns, index=[pd.to_datetime(tomorrow_str)])\n",
    "new_row.index = new_row.index.normalize()\n",
    "new_row.index = new_row.index.date\n",
    "sp500 = pd.concat([sp500, new_row])\n",
    "sp500 = sp500.drop(sp500.index[-1])\n",
    "train = sp500.iloc[:-100]\n",
    "test = sp500.iloc[-100:]\n",
    "\n",
    "#old\n",
    "predictors = [\"Close\", \"Volume\", \"Open\", \"High\"]\n",
    "#, \"Low\", \"VOpen\", \"VAdj Close\", \"VHigh\", \"VLow\"\n",
    "#predictors = [\"Close\", \"Volume\", \"Open\", \"High\", \"Low\"]\n",
    "\n",
    "#The backtest function evaluates a model by training it on historical data \n",
    "#and testing it in increments. It iterates over the data, training the model\n",
    "#on a growing set of past data and testing it on the next segment. \n",
    "#The results are collected and combined into a single DataFrame for analysis.\n",
    "def backtest(data, model, predictors, start=startnumber, step=stepnumber):\n",
    "    all_predictions = []\n",
    "\n",
    "    for i in range(start, data.shape[0], step):\n",
    "        train = data.iloc[0:i].copy()\n",
    "        test = data.iloc[i:(i+step)].copy()\n",
    "        predictions = predict(train, test, predictors, model)\n",
    "        all_predictions.append(predictions)\n",
    "    \n",
    "    return pd.concat(all_predictions)\n",
    "\n",
    "horizons = [2,5,60,250,1000]\n",
    "new_predictors = []\n",
    "\n",
    "for horizon in horizons:\n",
    "    rolling_averages = sp500.rolling(horizon).mean()\n",
    "    \n",
    "    ratio_column = f\"Close_Ratio_{horizon}\"\n",
    "    sp500[ratio_column] = sp500[\"Close\"] / rolling_averages[\"Close\"]\n",
    "    \n",
    "    trend_column = f\"Trend_{horizon}\"\n",
    "    sp500[trend_column] = sp500.shift(1).rolling(horizon).sum()[\"Target\"]\n",
    "    \n",
    "    new_predictors+= [ratio_column, trend_column]\n",
    "\n",
    "sp500 = sp500.dropna(subset=sp500.columns[sp500.columns != \"Tomorrow\"])\n",
    "\n",
    "additional_predictors = [\"HealthOpen\", \"HealthAdj Close\"]\n",
    "    #\"msusfnOpen\", \"msusfnAdj Close\"]# \"VOpen\", \"VAdj Close\"] #\"VAdj Close\"]#, \"QQQOpen\", \"QQQAdj Close\"] #[\"VAdj Close\", \"VOpen\", \"VHigh\", \"VLow\"]\n",
    "#\"USDHigh\", \"USDLow\", \"USDAdj Close\", \"USDOpen\", ,\"techOpen\", \"techAdj Close\"  \"techLow\",\"techHigh\", \n",
    "#additional_predictors = [\"Close\", \"Volume\", \"Open\", \"High\", \"Low\"] #[]#\n",
    "new_predictors.extend(additional_predictors)\n",
    "\n",
    "def predict(train, test, predictors, model):\n",
    "        model.fit(train[predictors], train[\"Target\"])\n",
    "        preds = model.predict_proba(test[predictors])[:,1]\n",
    "        preds[preds >=dt] = 1\n",
    "        preds[preds <dt] = 0\n",
    "        preds = pd.Series(preds, index=test.index, name=\"Predictions\")\n",
    "        combined = pd.concat([test[\"Target\"], preds], axis=1)\n",
    "        return combined\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "#\n",
    "for i in range (iterations):\n",
    "    predictionchange = []\n",
    "    weightedpredictionchange = []\n",
    "    weightedbmchange = []\n",
    "    model = RandomForestClassifier(n_estimators=number_of_estimators, max_features=maxfeatures, min_samples_split=minimum_of_samples_split, random_state=i)\n",
    "    predictions = backtest(sp500, model, new_predictors)\n",
    "    #display(f\"Iteration {i + 1}:\")\n",
    "    #display(predictions)\n",
    "    #display(predictions[\"Predictions\"].value_counts())\n",
    "    #display(predictions[\"Target\"].value_counts() / predictions.shape[0])\n",
    "    #display(np.round(precision_score(predictions[\"Target\"], predictions[\"Predictions\"]), decimals=4))\n",
    "    results.append(precision_score(predictions[\"Target\"], predictions[\"Predictions\"]))\n",
    "    \n",
    "    \n",
    "    #predictions already happened what happens after is obtaining data\n",
    "    #newdata\n",
    "    spy = yf.Ticker(theticker)\n",
    "    spy = spy.history(period=\"max\")\n",
    "    #\n",
    "    if not isinstance(spy.index, pd.DatetimeIndex):\n",
    "        spy.index = pd.to_datetime(spy.index)\n",
    "        #\n",
    "    spy.index = spy.index.normalize()\n",
    "    del spy[\"Dividends\"]\n",
    "    del spy[\"Stock Splits\"]\n",
    "    del spy[\"Volume\"]\n",
    "    del spy[\"High\"]\n",
    "    del spy[\"Low\"]\n",
    "    #### what time for change\n",
    "    spy[\"Change\"] = (spy[\"Close\"] - spy[\"Open\"]).shift(-1)\n",
    "    #####\n",
    "    spy['Open']\n",
    "    del spy['Close']\n",
    "    spy.index = spy.index.date\n",
    "    spy = spy.drop(spy.index[1])\n",
    "    display(spy)\n",
    "    total_sum = predictions[\"Predictions\"].value_counts().sum()\n",
    "    preddays9 = []\n",
    "    for idx, value in predictions[\"Predictions\"].items():\n",
    "        if value != 0:\n",
    "            preddays9.append(idx)\n",
    "    predictiondayslists9.append(preddays9)\n",
    "    preddays9 = []\n",
    "    display(total_sum)\n",
    "    spy = spy.iloc[-total_sum:]\n",
    "    predictionfrequency.append(predictions[\"Predictions\"].sum() / len(spy))\n",
    "    \n",
    "    predictions = predictions.iloc[-total_sum:]    #???total_sum\n",
    "    change_column = spy['Change'].tolist()\n",
    "    spy_column = spy['Open'].tolist()\n",
    "    #display(change_column)\n",
    "    predictions['Change'] = change_column\n",
    "    predictions['Weight'] = spy_column\n",
    "    display(predictions)\n",
    "    combination = predictions\n",
    "    plt.figure(figsize=(10, 6))  # Adjust the figure size as needed\n",
    "    plt.plot(predictions.index, predictions['Predictions'], marker='o', linestyle='-')\n",
    "    plt.title('Predictions Over Time')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Predictions')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    #display(spy)\n",
    "    def new_row(row):\n",
    "        if row['Predictions'] == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return row['Change']\n",
    "    predictions['UpdatedChange'] = predictions.apply(new_row, axis=1)\n",
    "    predictions['WeightedChange'] = predictions['UpdatedChange'] / predictions['Weight']\n",
    "    display(predictions)\n",
    "    last_200_rows = predictions.tail(400)\n",
    "    #display(last_200_rows)\n",
    "    for value in predictions['UpdatedChange']:\n",
    "        if value != 0:\n",
    "            predictionchange.append(value)    \n",
    "    avgpredictionchange.append(np.mean(predictionchange))\n",
    "    avgpredictionchangesd.append(np.std(predictionchange))\n",
    "    for value in predictions['WeightedChange']:\n",
    "        if value != 0:\n",
    "            weightedpredictionchange.append(value)    \n",
    "    avgweightedpredictionchange.append(np.mean(weightedpredictionchange))\n",
    "    avgweightedpredictionchangesd.append(np.std(weightedpredictionchange))\n",
    "    ###benchmark stuff\n",
    "    display(combination)\n",
    "    combination[\"BMChange\"] = benchmarkdata[\"BMChange\"]\n",
    "    for idx, value in predictions[\"UpdatedChange\"].items():\n",
    "        if value != 0:\n",
    "            weightedbmchange.append(predictions[\"BMChange\"].loc[idx])\n",
    "        else:\n",
    "            predictions.at[idx, \"BMChange\"] = 0\n",
    "    # Filter out the non-zero elements\n",
    "    combination[\"BMChange\"] = combination[\"WeightedChange\"] - combination[\"BMChange\"]\n",
    "    weightedbmchange = combination[\"BMChange\"][combination[\"BMChange\"] != 0]\n",
    "\n",
    "# Convert the filtered series to a list\n",
    "    weightedbmchange = weightedbmchange.tolist()\n",
    "\n",
    "    avgweightedbenchmarkchange.append(np.mean(weightedbmchange))\n",
    "    avgweightedbenchmarkchangesd.append(np.std(weightedbmchange))\n",
    "    display(combination)\n",
    "\n",
    "\n",
    "print(type(combination))\n",
    "print(type(benchmarkdata))\n",
    "print(benchmarkdata)\n",
    "\n",
    "display(results)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "display(np.round((elapsed_time / 60), decimals=2))\n",
    "display(np.round(((elapsed_time / 60) / iterations), decimals=2))\n",
    "\n",
    "display(np.round(np.mean(avgpredictionchange), decimals=2))\n",
    "display(np.round(np.mean(avgpredictionchangesd), decimals=4))\n",
    "display(np.round(np.mean(avgweightedpredictionchange), decimals=6))\n",
    "display(np.round(np.mean(avgweightedpredictionchangesd), decimals=6))\n",
    "\n",
    "display(np.round(np.mean(predictionfrequency), decimals=4))\n",
    "display(np.round(np.mean(results), decimals=4))\n",
    "display(np.round(np.std(results), decimals=4))\n",
    "\n",
    "dates_when_predictions_is_1 = predictions.index[predictions['Predictions'] == 1].tolist()\n",
    "\n",
    "#print(f\"All dates when 'predictions' is equal to 1: {dates_when_predictions_is_1}\")\n",
    "\n",
    "\n",
    "# Load the CSV file\n",
    "filename = \"/Users/derek/Downloads/DT AI Transformed Data.csv\"\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "# Define your 24 variables (ordered list of values to add)\n",
    "new_data = [\n",
    "    notebook_name, number_of_estimators, startdateunformatted, dt, VIXvar, minimum_of_samples_split,\n",
    "    maxfeatures, GSPCvar, shrtcomment, iterations, NYICDXQQQvar, XLKvar,\n",
    "    startnumber, stepnumber, theticker, np.round(np.mean(results), decimals=4), np.round(np.std(results), decimals=4), np.round(((elapsed_time / 60) / iterations), decimals=2),\n",
    "    np.round((elapsed_time / 60), decimals=2), np.round(np.mean(predictionfrequency), decimals = 4), np.round(np.mean(avgpredictionchange), decimals=2), np.round(np.mean(avgpredictionchangesd), decimals=4), np.round(np.mean(avgweightedpredictionchange), decimals=6), np.round(np.mean(avgweightedpredictionchangesd), decimals=6), np.round(np.mean(avgweightedbenchmarkchange), decimals = 6), np.round(np.mean(avgweightedbenchmarkchangesd), decimals=6), longcomment\n",
    "]\n",
    "\n",
    "# Find the first empty row\n",
    "# Here we assume that the row is considered empty if all columns are NaN or empty strings\n",
    "empty_row_index = df.index[df.isnull().all(axis=1) | (df == '').all(axis=1)].tolist()\n",
    "if empty_row_index:\n",
    "    # If there are empty rows, use the first one\n",
    "    first_empty_row = empty_row_index[0]\n",
    "else:\n",
    "    # If no empty rows, append a new row at the end\n",
    "    first_empty_row = len(df)\n",
    "\n",
    "# Ensure the length of new_data matches the number of columns\n",
    "if len(new_data) != len(df.columns):\n",
    "    raise ValueError(\"Number of new data values does not match number of columns.\")\n",
    "\n",
    "# Add the new data to the determined row\n",
    "df.loc[first_empty_row] = new_data\n",
    "\n",
    "# Save the updated DataFrame back to the CSV file\n",
    "df.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Data successfully added to row {first_empty_row + 1} of the CSV file.\")\n",
    "\n",
    "print(f\"Number of columns in DataFrame: {len(df.columns)}\")\n",
    "print(f\"Number of values in new_data: {len(new_data)}\")\n",
    "\n",
    "print(benchmarkdata)\n",
    "\n",
    "print(np.mean(avgweightedbenchmarkchange))\n",
    "print(np.mean(avgweightedbenchmarkchangesd)) \n",
    "\n",
    "\n",
    "\n",
    "#imports\n",
    "import time\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#manually set data\n",
    "notebook_name = \"Decision Tree Testing Consistency Open Close 4 (BM Target)\"\n",
    "VIXvar = ''\n",
    "maxfeatures = 5 #'sqrt'\n",
    "GSPCvar = 'all - og'\n",
    "#all predictors - the original\n",
    "shrtcomment = 'NYEIDOC'\n",
    "NYICDXQQQvar = ''\n",
    "XLKvar = ''\n",
    "benchmark = '^GSPC'\n",
    "theticker = 'XLE' #^GSPC\n",
    "predictionsnum = 'N/A'\n",
    "longcomment = ''\n",
    "\n",
    "#settings\n",
    "dt = 0.625\n",
    "\n",
    "number_of_estimators = 1600\n",
    "minimum_of_samples_split = 150\n",
    "startnumber = 3000\n",
    "stepnumber = 220\n",
    "startdateunformatted = '1950-01-01'\n",
    "\n",
    "#prep\n",
    "starting_time = pd.to_datetime(startdateunformatted).date()\n",
    "today = datetime.now()\n",
    "tomorrow = today + timedelta(days=1)\n",
    "tomorrow_str = tomorrow.strftime('%Y-%m-%d')\n",
    "ending_time = tomorrow_str\n",
    "tomorrow_str\n",
    "results = []\n",
    "predictionfrequency = []\n",
    "avgpredictionchange = []\n",
    "avgpredictionchangesd = []\n",
    "avgweightedpredictionchange = []\n",
    "avgweightedpredictionchangesd = []\n",
    "avgweightedbenchmarkchange = []\n",
    "avgweightedbenchmarkchangesd = []\n",
    "predictiondayslists10 = []\n",
    "pd.set_option('display.max_rows', 400)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "#setting up benchmark\n",
    "##\n",
    "benchmarkdata = yf.Ticker(benchmark)\n",
    "benchmarkdata = benchmarkdata.history(period=\"max\")\n",
    "### the way change in what time is recorded\n",
    "#change type\n",
    "benchmarkdata[\"BMChange\"] = ((benchmarkdata[\"Close\"] - benchmarkdata[\"Open\"]) / benchmarkdata[\"Open\"])\n",
    "if not isinstance(benchmarkdata.index, pd.DatetimeIndex):\n",
    "    benchmarkdata.index = pd.to_datetime(benchmarkdata.index)\n",
    "benchmarkdata.index = benchmarkdata.index.normalize()\n",
    "benchmarkdata.index = benchmarkdata.index.date\n",
    "benchmarkdata = benchmarkdata.drop(benchmarkdata.index[1])\n",
    "print(benchmarkdata)\n",
    "\n",
    "#data\n",
    "# Fetch S&P 500 data\n",
    "sp500 = yf.Ticker(theticker)\n",
    "sp500 = sp500.history(period=\"max\")\n",
    "\n",
    "# Ensure the index is a DatetimeIndex\n",
    "if not isinstance(sp500.index, pd.DatetimeIndex):\n",
    "    sp500.index = pd.to_datetime(sp500.index)\n",
    "\n",
    "# Normalize the dates (remove the time component)\n",
    "sp500.index = sp500.index.normalize()\n",
    "\n",
    "# Drop columns if needed\n",
    "if \"Dividends\" in sp500.columns:\n",
    "    del sp500[\"Dividends\"]\n",
    "if \"Stock Splits\" in sp500.columns:\n",
    "    del sp500[\"Stock Splits\"]\n",
    "sp500.index = sp500.index.date\n",
    "display(sp500)\n",
    "sp500[\"Tomorrow\"] = sp500[\"Close\"].shift(-1)\n",
    "######what change were predicting\n",
    "sp500[\"BMChange\"] = (benchmarkdata[\"Close\"] - benchmarkdata[\"Open\"])/benchmarkdata[\"Open\"]\n",
    "sp500[\"Target\"] = (sp500[\"BMChange\"] > ((sp500[\"Close\"] - sp500[\"Open\"])/sp500[\"Open\"])).astype(int).shift(-1)\n",
    "benchmarkdata[\"BMChange\"] = benchmarkdata[\"BMChange\"].shift(-1)\n",
    "sp500 = sp500.drop(columns=['BMChange'])\n",
    "#sp500[\"Target\"] = (sp500[\"Close\"] > sp500[\"Open\"]).astype(int).shift(-1)\n",
    "#####\n",
    "sp500 = sp500.loc[starting_time:].copy()\n",
    "display(sp500)\n",
    "volatility = yf.download(tickers = '^VIX', start = starting_time, end = ending_time)\n",
    "volatility = volatility[['Open', 'Adj Close', 'High', 'Low']]\n",
    "volatility = volatility.rename(columns={'Open': 'VOpen', 'Adj Close': 'VAdj Close', 'High': 'VHigh', 'Low': 'VLow'})\n",
    "volatility.index = volatility.index.normalize()\n",
    "volatility.index = volatility.index.date\n",
    "sp500 = pd.concat([sp500, volatility], axis=1)\n",
    "USD = yf.download(tickers = '^NYICDX', start = starting_time, end = ending_time)\n",
    "USD = USD[['Open', 'Adj Close', 'High', 'Low']]\n",
    "USD = USD.rename(columns={'Open': 'USDOpen', 'Adj Close': 'USDAdj Close', 'High': 'USDHigh', 'Low': 'USDLow'})\n",
    "USD.index = USD.index.normalize()\n",
    "USD.index = USD.index.date\n",
    "sp500 = pd.concat([sp500, USD], axis=1)\n",
    "#QQQ = yf.download(tickers = 'QQQ', start = starting_time, end = ending_time)\n",
    "#QQQ = QQQ[['Open', 'Adj Close', 'High', 'Low']]\n",
    "#QQQ = QQQ.rename(columns={'Open': 'QQQOpen', 'Adj Close': 'QQQAdj Close', 'High': 'QQQHigh', 'Low': 'QQQLow'})\n",
    "#QQQ.index = QQQ.index.normalize()\n",
    "#QQQ.index = QQQ.index.date\n",
    "#sp500 = pd.concat([sp500, QQQ], axis=1)\n",
    "Health = yf.download(tickers = '^SP500-35', start = starting_time, end = ending_time)\n",
    "Health = Health[['Open', 'Adj Close', 'High', 'Low']]\n",
    "Health = Health.rename(columns={'Open': 'HealthOpen', 'Adj Close': 'HealthAdj Close', 'High': 'HealthHigh', 'Low': 'HealthLow'})\n",
    "Health.index = Health.index.normalize()\n",
    "Health.index = Health.index.date\n",
    "sp500 = pd.concat([sp500, Health], axis=1)\n",
    "Consume = yf.download(tickers = '^SP500-25', start = starting_time, end = ending_time)\n",
    "Consume = Consume[['Open', 'Adj Close', 'High', 'Low']]\n",
    "Consume = Consume.rename(columns={'Open': 'ConsumeOpen', 'Adj Close': 'ConsumeAdj Close', 'High': 'ConsumeHigh', 'Low': 'ConsumeLow'})\n",
    "Consume.index = Consume.index.normalize()\n",
    "Consume.index = Consume.index.date\n",
    "sp500 = pd.concat([sp500, Consume], axis=1)\n",
    "tech = yf.download(tickers = 'XLK', start = starting_time, end = ending_time)\n",
    "tech = tech[['Open', 'Adj Close', 'High', 'Low']]\n",
    "tech = tech.rename(columns={'Open': 'techOpen', 'Adj Close': 'techAdj Close', 'High': 'techHigh', 'Low': 'techLow'})\n",
    "tech.index = tech.index.normalize()\n",
    "tech.index = tech.index.date\n",
    "sp500 = pd.concat([sp500, tech], axis=1)\n",
    "msusfn = yf.download(tickers = 'NYEID', start = starting_time, end = ending_time)\n",
    "msusfn = msusfn[['Open', 'Adj Close', 'High', 'Low']]\n",
    "msusfn = msusfn.rename(columns={'Open': 'msusfnOpen', 'Adj Close': 'msusfnAdj Close', 'High': 'msusfnHigh', 'Low': 'msusfnLow'})\n",
    "msusfn.index = msusfn.index.normalize()\n",
    "msusfn.index = msusfn.index.date\n",
    "sp500 = pd.concat([sp500, msusfn], axis=1)\n",
    "new_row = pd.DataFrame([[0] * len(sp500.columns)], columns=sp500.columns, index=[pd.to_datetime(tomorrow_str)])\n",
    "new_row.index = new_row.index.normalize()\n",
    "new_row.index = new_row.index.date\n",
    "sp500 = pd.concat([sp500, new_row])\n",
    "sp500 = sp500.drop(sp500.index[-1])\n",
    "train = sp500.iloc[:-100]\n",
    "test = sp500.iloc[-100:]\n",
    "\n",
    "#old\n",
    "predictors = [\"Close\", \"Volume\", \"Open\", \"High\"]\n",
    "#, \"Low\", \"VOpen\", \"VAdj Close\", \"VHigh\", \"VLow\"\n",
    "#predictors = [\"Close\", \"Volume\", \"Open\", \"High\", \"Low\"]\n",
    "\n",
    "#The backtest function evaluates a model by training it on historical data \n",
    "#and testing it in increments. It iterates over the data, training the model\n",
    "#on a growing set of past data and testing it on the next segment. \n",
    "#The results are collected and combined into a single DataFrame for analysis.\n",
    "def backtest(data, model, predictors, start=startnumber, step=stepnumber):\n",
    "    all_predictions = []\n",
    "\n",
    "    for i in range(start, data.shape[0], step):\n",
    "        train = data.iloc[0:i].copy()\n",
    "        test = data.iloc[i:(i+step)].copy()\n",
    "        predictions = predict(train, test, predictors, model)\n",
    "        all_predictions.append(predictions)\n",
    "    \n",
    "    return pd.concat(all_predictions)\n",
    "\n",
    "horizons = [2,5,60,250,1000]\n",
    "new_predictors = []\n",
    "\n",
    "for horizon in horizons:\n",
    "    rolling_averages = sp500.rolling(horizon).mean()\n",
    "    \n",
    "    ratio_column = f\"Close_Ratio_{horizon}\"\n",
    "    sp500[ratio_column] = sp500[\"Close\"] / rolling_averages[\"Close\"]\n",
    "    \n",
    "    trend_column = f\"Trend_{horizon}\"\n",
    "    sp500[trend_column] = sp500.shift(1).rolling(horizon).sum()[\"Target\"]\n",
    "    \n",
    "    new_predictors+= [ratio_column, trend_column]\n",
    "\n",
    "sp500 = sp500.dropna(subset=sp500.columns[sp500.columns != \"Tomorrow\"])\n",
    "\n",
    "additional_predictors = [\"msusfnOpen\", \"msusfnAdj Close\"]\n",
    "    #\"HealthOpen\", \"HealthAdj Close\"]\n",
    "    #\"msusfnOpen\", \"msusfnAdj Close\"]# \"VOpen\", \"VAdj Close\"] #\"VAdj Close\"]#, \"QQQOpen\", \"QQQAdj Close\"] #[\"VAdj Close\", \"VOpen\", \"VHigh\", \"VLow\"]\n",
    "#\"USDHigh\", \"USDLow\", \"USDAdj Close\", \"USDOpen\", ,\"techOpen\", \"techAdj Close\"  \"techLow\",\"techHigh\", \n",
    "#additional_predictors = [\"Close\", \"Volume\", \"Open\", \"High\", \"Low\"] #[]#\n",
    "new_predictors.extend(additional_predictors)\n",
    "\n",
    "def predict(train, test, predictors, model):\n",
    "        model.fit(train[predictors], train[\"Target\"])\n",
    "        preds = model.predict_proba(test[predictors])[:,1]\n",
    "        preds[preds >=dt] = 1\n",
    "        preds[preds <dt] = 0\n",
    "        preds = pd.Series(preds, index=test.index, name=\"Predictions\")\n",
    "        combined = pd.concat([test[\"Target\"], preds], axis=1)\n",
    "        return combined\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "#\n",
    "for i in range (iterations):\n",
    "    predictionchange = []\n",
    "    weightedpredictionchange = []\n",
    "    weightedbmchange = []\n",
    "    model = RandomForestClassifier(n_estimators=number_of_estimators, max_features=maxfeatures, min_samples_split=minimum_of_samples_split, random_state=i)\n",
    "    predictions = backtest(sp500, model, new_predictors)\n",
    "    #display(f\"Iteration {i + 1}:\")\n",
    "    #display(predictions)\n",
    "    #display(predictions[\"Predictions\"].value_counts())\n",
    "    #display(predictions[\"Target\"].value_counts() / predictions.shape[0])\n",
    "    #display(np.round(precision_score(predictions[\"Target\"], predictions[\"Predictions\"]), decimals=4))\n",
    "    results.append(precision_score(predictions[\"Target\"], predictions[\"Predictions\"]))\n",
    "    \n",
    "    \n",
    "    #predictions already happened what happens after is obtaining data\n",
    "    #newdata\n",
    "    spy = yf.Ticker(theticker)\n",
    "    spy = spy.history(period=\"max\")\n",
    "    #\n",
    "    if not isinstance(spy.index, pd.DatetimeIndex):\n",
    "        spy.index = pd.to_datetime(spy.index)\n",
    "        #\n",
    "    spy.index = spy.index.normalize()\n",
    "    del spy[\"Dividends\"]\n",
    "    del spy[\"Stock Splits\"]\n",
    "    del spy[\"Volume\"]\n",
    "    del spy[\"High\"]\n",
    "    del spy[\"Low\"]\n",
    "    #### what time for change\n",
    "    spy[\"Change\"] = (spy[\"Close\"] - spy[\"Open\"]).shift(-1)\n",
    "    #####\n",
    "    spy['Open']\n",
    "    del spy['Close']\n",
    "    spy.index = spy.index.date\n",
    "    spy = spy.drop(spy.index[1])\n",
    "    display(spy)\n",
    "    total_sum = predictions[\"Predictions\"].value_counts().sum()\n",
    "    preddays10 = []\n",
    "    for idx, value in predictions[\"Predictions\"].items():\n",
    "        if value != 0:\n",
    "            preddays10.append(idx)\n",
    "    predictiondayslists10.append(preddays10)\n",
    "    preddays10 = []\n",
    "    display(total_sum)\n",
    "    spy = spy.iloc[-total_sum:]\n",
    "    predictionfrequency.append(predictions[\"Predictions\"].sum() / len(spy))\n",
    "    \n",
    "    predictions = predictions.iloc[-total_sum:]    #???total_sum\n",
    "    change_column = spy['Change'].tolist()\n",
    "    spy_column = spy['Open'].tolist()\n",
    "    #display(change_column)\n",
    "    predictions['Change'] = change_column\n",
    "    predictions['Weight'] = spy_column\n",
    "    display(predictions)\n",
    "    combination = predictions\n",
    "    plt.figure(figsize=(10, 6))  # Adjust the figure size as needed\n",
    "    plt.plot(predictions.index, predictions['Predictions'], marker='o', linestyle='-')\n",
    "    plt.title('Predictions Over Time')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Predictions')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    #display(spy)\n",
    "    def new_row(row):\n",
    "        if row['Predictions'] == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return row['Change']\n",
    "    predictions['UpdatedChange'] = predictions.apply(new_row, axis=1)\n",
    "    predictions['WeightedChange'] = predictions['UpdatedChange'] / predictions['Weight']\n",
    "    display(predictions)\n",
    "    last_200_rows = predictions.tail(400)\n",
    "    #display(last_200_rows)\n",
    "    for value in predictions['UpdatedChange']:\n",
    "        if value != 0:\n",
    "            predictionchange.append(value)    \n",
    "    avgpredictionchange.append(np.mean(predictionchange))\n",
    "    avgpredictionchangesd.append(np.std(predictionchange))\n",
    "    for value in predictions['WeightedChange']:\n",
    "        if value != 0:\n",
    "            weightedpredictionchange.append(value)    \n",
    "    avgweightedpredictionchange.append(np.mean(weightedpredictionchange))\n",
    "    avgweightedpredictionchangesd.append(np.std(weightedpredictionchange))\n",
    "    ###benchmark stuff\n",
    "    display(combination)\n",
    "    combination[\"BMChange\"] = benchmarkdata[\"BMChange\"]\n",
    "    for idx, value in predictions[\"UpdatedChange\"].items():\n",
    "        if value != 0:\n",
    "            weightedbmchange.append(predictions[\"BMChange\"].loc[idx])\n",
    "        else:\n",
    "            predictions.at[idx, \"BMChange\"] = 0\n",
    "    # Filter out the non-zero elements\n",
    "    combination[\"BMChange\"] = combination[\"WeightedChange\"] - combination[\"BMChange\"]\n",
    "    weightedbmchange = combination[\"BMChange\"][combination[\"BMChange\"] != 0]\n",
    "\n",
    "# Convert the filtered series to a list\n",
    "    weightedbmchange = weightedbmchange.tolist()\n",
    "\n",
    "    avgweightedbenchmarkchange.append(np.mean(weightedbmchange))\n",
    "    avgweightedbenchmarkchangesd.append(np.std(weightedbmchange))\n",
    "    display(combination)\n",
    "\n",
    "\n",
    "print(type(combination))\n",
    "print(type(benchmarkdata))\n",
    "print(benchmarkdata)\n",
    "\n",
    "display(results)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "display(np.round((elapsed_time / 60), decimals=2))\n",
    "display(np.round(((elapsed_time / 60) / iterations), decimals=2))\n",
    "\n",
    "display(np.round(np.mean(avgpredictionchange), decimals=2))\n",
    "display(np.round(np.mean(avgpredictionchangesd), decimals=4))\n",
    "display(np.round(np.mean(avgweightedpredictionchange), decimals=6))\n",
    "display(np.round(np.mean(avgweightedpredictionchangesd), decimals=6))\n",
    "\n",
    "display(np.round(np.mean(predictionfrequency), decimals=4))\n",
    "display(np.round(np.mean(results), decimals=4))\n",
    "display(np.round(np.std(results), decimals=4))\n",
    "\n",
    "dates_when_predictions_is_1 = predictions.index[predictions['Predictions'] == 1].tolist()\n",
    "\n",
    "#print(f\"All dates when 'predictions' is equal to 1: {dates_when_predictions_is_1}\")\n",
    "\n",
    "\n",
    "# Load the CSV file\n",
    "filename = \"/Users/derek/Downloads/DT AI Transformed Data.csv\"\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "# Define your 24 variables (ordered list of values to add)\n",
    "new_data = [\n",
    "    notebook_name, number_of_estimators, startdateunformatted, dt, VIXvar, minimum_of_samples_split,\n",
    "    maxfeatures, GSPCvar, shrtcomment, iterations, NYICDXQQQvar, XLKvar,\n",
    "    startnumber, stepnumber, theticker, np.round(np.mean(results), decimals=4), np.round(np.std(results), decimals=4), np.round(((elapsed_time / 60) / iterations), decimals=2),\n",
    "    np.round((elapsed_time / 60), decimals=2), np.round(np.mean(predictionfrequency), decimals = 4), np.round(np.mean(avgpredictionchange), decimals=2), np.round(np.mean(avgpredictionchangesd), decimals=4), np.round(np.mean(avgweightedpredictionchange), decimals=6), np.round(np.mean(avgweightedpredictionchangesd), decimals=6), np.round(np.mean(avgweightedbenchmarkchange), decimals = 6), np.round(np.mean(avgweightedbenchmarkchangesd), decimals=6), longcomment\n",
    "]\n",
    "\n",
    "# Find the first empty row\n",
    "# Here we assume that the row is considered empty if all columns are NaN or empty strings\n",
    "empty_row_index = df.index[df.isnull().all(axis=1) | (df == '').all(axis=1)].tolist()\n",
    "if empty_row_index:\n",
    "    # If there are empty rows, use the first one\n",
    "    first_empty_row = empty_row_index[0]\n",
    "else:\n",
    "    # If no empty rows, append a new row at the end\n",
    "    first_empty_row = len(df)\n",
    "\n",
    "# Ensure the length of new_data matches the number of columns\n",
    "if len(new_data) != len(df.columns):\n",
    "    raise ValueError(\"Number of new data values does not match number of columns.\")\n",
    "\n",
    "# Add the new data to the determined row\n",
    "df.loc[first_empty_row] = new_data\n",
    "\n",
    "# Save the updated DataFrame back to the CSV file\n",
    "df.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Data successfully added to row {first_empty_row + 1} of the CSV file.\")\n",
    "\n",
    "print(f\"Number of columns in DataFrame: {len(df.columns)}\")\n",
    "print(f\"Number of values in new_data: {len(new_data)}\")\n",
    "\n",
    "print(benchmarkdata)\n",
    "\n",
    "print(np.mean(avgweightedbenchmarkchange))\n",
    "print(np.mean(avgweightedbenchmarkchangesd)) \n",
    "\n",
    "\n",
    "\n",
    "#imports\n",
    "import time\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#manually set data\n",
    "notebook_name = \"Decision Tree Testing Consistency Open Close 4 (BM Target)\"\n",
    "VIXvar = ''\n",
    "maxfeatures = 5 #'sqrt'\n",
    "GSPCvar = 'all - og'\n",
    "#all predictors - the original\n",
    "shrtcomment = 'EnergyOC'\n",
    "NYICDXQQQvar = ''\n",
    "XLKvar = ''\n",
    "benchmark = '^GSPC'\n",
    "theticker = 'XLE' #^GSPC\n",
    "predictionsnum = 'N/A'\n",
    "longcomment = ''\n",
    "\n",
    "#settings\n",
    "dt = 0.625\n",
    "\n",
    "number_of_estimators = 1600\n",
    "minimum_of_samples_split = 150\n",
    "startnumber = 3000\n",
    "stepnumber = 220\n",
    "startdateunformatted = '1950-01-01'\n",
    "\n",
    "#prep\n",
    "starting_time = pd.to_datetime(startdateunformatted).date()\n",
    "today = datetime.now()\n",
    "tomorrow = today + timedelta(days=1)\n",
    "tomorrow_str = tomorrow.strftime('%Y-%m-%d')\n",
    "ending_time = tomorrow_str\n",
    "tomorrow_str\n",
    "results = []\n",
    "predictionfrequency = []\n",
    "avgpredictionchange = []\n",
    "avgpredictionchangesd = []\n",
    "avgweightedpredictionchange = []\n",
    "avgweightedpredictionchangesd = []\n",
    "avgweightedbenchmarkchange = []\n",
    "avgweightedbenchmarkchangesd = []\n",
    "predictiondayslists11 = []\n",
    "pd.set_option('display.max_rows', 400)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "#setting up benchmark\n",
    "##\n",
    "benchmarkdata = yf.Ticker(benchmark)\n",
    "benchmarkdata = benchmarkdata.history(period=\"max\")\n",
    "### the way change in what time is recorded\n",
    "#change type\n",
    "benchmarkdata[\"BMChange\"] = ((benchmarkdata[\"Close\"] - benchmarkdata[\"Open\"]) / benchmarkdata[\"Open\"])\n",
    "if not isinstance(benchmarkdata.index, pd.DatetimeIndex):\n",
    "    benchmarkdata.index = pd.to_datetime(benchmarkdata.index)\n",
    "benchmarkdata.index = benchmarkdata.index.normalize()\n",
    "benchmarkdata.index = benchmarkdata.index.date\n",
    "benchmarkdata = benchmarkdata.drop(benchmarkdata.index[1])\n",
    "print(benchmarkdata)\n",
    "\n",
    "#data\n",
    "# Fetch S&P 500 data\n",
    "sp500 = yf.Ticker(theticker)\n",
    "sp500 = sp500.history(period=\"max\")\n",
    "\n",
    "# Ensure the index is a DatetimeIndex\n",
    "if not isinstance(sp500.index, pd.DatetimeIndex):\n",
    "    sp500.index = pd.to_datetime(sp500.index)\n",
    "\n",
    "# Normalize the dates (remove the time component)\n",
    "sp500.index = sp500.index.normalize()\n",
    "\n",
    "# Drop columns if needed\n",
    "if \"Dividends\" in sp500.columns:\n",
    "    del sp500[\"Dividends\"]\n",
    "if \"Stock Splits\" in sp500.columns:\n",
    "    del sp500[\"Stock Splits\"]\n",
    "sp500.index = sp500.index.date\n",
    "display(sp500)\n",
    "sp500[\"Tomorrow\"] = sp500[\"Close\"].shift(-1)\n",
    "######what change were predicting\n",
    "sp500[\"BMChange\"] = (benchmarkdata[\"Close\"] - benchmarkdata[\"Open\"])/benchmarkdata[\"Open\"]\n",
    "sp500[\"Target\"] = (sp500[\"BMChange\"] > ((sp500[\"Close\"] - sp500[\"Open\"])/sp500[\"Open\"])).astype(int).shift(-1)\n",
    "benchmarkdata[\"BMChange\"] = benchmarkdata[\"BMChange\"].shift(-1)\n",
    "sp500 = sp500.drop(columns=['BMChange'])\n",
    "#sp500[\"Target\"] = (sp500[\"Close\"] > sp500[\"Open\"]).astype(int).shift(-1)\n",
    "#####\n",
    "sp500 = sp500.loc[starting_time:].copy()\n",
    "display(sp500)\n",
    "volatility = yf.download(tickers = '^VIX', start = starting_time, end = ending_time)\n",
    "volatility = volatility[['Open', 'Adj Close', 'High', 'Low']]\n",
    "volatility = volatility.rename(columns={'Open': 'VOpen', 'Adj Close': 'VAdj Close', 'High': 'VHigh', 'Low': 'VLow'})\n",
    "volatility.index = volatility.index.normalize()\n",
    "volatility.index = volatility.index.date\n",
    "sp500 = pd.concat([sp500, volatility], axis=1)\n",
    "USD = yf.download(tickers = '^NYICDX', start = starting_time, end = ending_time)\n",
    "USD = USD[['Open', 'Adj Close', 'High', 'Low']]\n",
    "USD = USD.rename(columns={'Open': 'USDOpen', 'Adj Close': 'USDAdj Close', 'High': 'USDHigh', 'Low': 'USDLow'})\n",
    "USD.index = USD.index.normalize()\n",
    "USD.index = USD.index.date\n",
    "sp500 = pd.concat([sp500, USD], axis=1)\n",
    "#QQQ = yf.download(tickers = 'QQQ', start = starting_time, end = ending_time)\n",
    "#QQQ = QQQ[['Open', 'Adj Close', 'High', 'Low']]\n",
    "#QQQ = QQQ.rename(columns={'Open': 'QQQOpen', 'Adj Close': 'QQQAdj Close', 'High': 'QQQHigh', 'Low': 'QQQLow'})\n",
    "#QQQ.index = QQQ.index.normalize()\n",
    "#QQQ.index = QQQ.index.date\n",
    "#sp500 = pd.concat([sp500, QQQ], axis=1)\n",
    "Health = yf.download(tickers = '^SP500-35', start = starting_time, end = ending_time)\n",
    "Health = Health[['Open', 'Adj Close', 'High', 'Low']]\n",
    "Health = Health.rename(columns={'Open': 'HealthOpen', 'Adj Close': 'HealthAdj Close', 'High': 'HealthHigh', 'Low': 'HealthLow'})\n",
    "Health.index = Health.index.normalize()\n",
    "Health.index = Health.index.date\n",
    "sp500 = pd.concat([sp500, Health], axis=1)\n",
    "Consume = yf.download(tickers = '^SP500-25', start = starting_time, end = ending_time)\n",
    "Consume = Consume[['Open', 'Adj Close', 'High', 'Low']]\n",
    "Consume = Consume.rename(columns={'Open': 'ConsumeOpen', 'Adj Close': 'ConsumeAdj Close', 'High': 'ConsumeHigh', 'Low': 'ConsumeLow'})\n",
    "Consume.index = Consume.index.normalize()\n",
    "Consume.index = Consume.index.date\n",
    "sp500 = pd.concat([sp500, Consume], axis=1)\n",
    "tech = yf.download(tickers = 'XLK', start = starting_time, end = ending_time)\n",
    "tech = tech[['Open', 'Adj Close', 'High', 'Low']]\n",
    "tech = tech.rename(columns={'Open': 'techOpen', 'Adj Close': 'techAdj Close', 'High': 'techHigh', 'Low': 'techLow'})\n",
    "tech.index = tech.index.normalize()\n",
    "tech.index = tech.index.date\n",
    "sp500 = pd.concat([sp500, tech], axis=1)\n",
    "msusfn = yf.download(tickers = 'NYEID', start = starting_time, end = ending_time)\n",
    "msusfn = msusfn[['Open', 'Adj Close', 'High', 'Low']]\n",
    "msusfn = msusfn.rename(columns={'Open': 'msusfnOpen', 'Adj Close': 'msusfnAdj Close', 'High': 'msusfnHigh', 'Low': 'msusfnLow'})\n",
    "msusfn.index = msusfn.index.normalize()\n",
    "msusfn.index = msusfn.index.date\n",
    "sp500 = pd.concat([sp500, msusfn], axis=1)\n",
    "new_row = pd.DataFrame([[0] * len(sp500.columns)], columns=sp500.columns, index=[pd.to_datetime(tomorrow_str)])\n",
    "new_row.index = new_row.index.normalize()\n",
    "new_row.index = new_row.index.date\n",
    "sp500 = pd.concat([sp500, new_row])\n",
    "sp500 = sp500.drop(sp500.index[-1])\n",
    "train = sp500.iloc[:-100]\n",
    "test = sp500.iloc[-100:]\n",
    "\n",
    "#old\n",
    "predictors = [\"Close\", \"Volume\", \"Open\", \"High\"]\n",
    "#, \"Low\", \"VOpen\", \"VAdj Close\", \"VHigh\", \"VLow\"\n",
    "#predictors = [\"Close\", \"Volume\", \"Open\", \"High\", \"Low\"]\n",
    "\n",
    "#The backtest function evaluates a model by training it on historical data \n",
    "#and testing it in increments. It iterates over the data, training the model\n",
    "#on a growing set of past data and testing it on the next segment. \n",
    "#The results are collected and combined into a single DataFrame for analysis.\n",
    "def backtest(data, model, predictors, start=startnumber, step=stepnumber):\n",
    "    all_predictions = []\n",
    "\n",
    "    for i in range(start, data.shape[0], step):\n",
    "        train = data.iloc[0:i].copy()\n",
    "        test = data.iloc[i:(i+step)].copy()\n",
    "        predictions = predict(train, test, predictors, model)\n",
    "        all_predictions.append(predictions)\n",
    "    \n",
    "    return pd.concat(all_predictions)\n",
    "\n",
    "horizons = [2,5,60,250,1000]\n",
    "new_predictors = []\n",
    "\n",
    "for horizon in horizons:\n",
    "    rolling_averages = sp500.rolling(horizon).mean()\n",
    "    \n",
    "    ratio_column = f\"Close_Ratio_{horizon}\"\n",
    "    sp500[ratio_column] = sp500[\"Close\"] / rolling_averages[\"Close\"]\n",
    "    \n",
    "    trend_column = f\"Trend_{horizon}\"\n",
    "    sp500[trend_column] = sp500.shift(1).rolling(horizon).sum()[\"Target\"]\n",
    "    \n",
    "    new_predictors+= [ratio_column, trend_column]\n",
    "\n",
    "sp500 = sp500.dropna(subset=sp500.columns[sp500.columns != \"Tomorrow\"])\n",
    "\n",
    "additional_predictors = [\"msusfnOpen\", \"msusfnAdj Close\"]\n",
    "    #\"HealthOpen\", \"HealthAdj Close\"]\n",
    "    #\"msusfnOpen\", \"msusfnAdj Close\"]# \"VOpen\", \"VAdj Close\"] #\"VAdj Close\"]#, \"QQQOpen\", \"QQQAdj Close\"] #[\"VAdj Close\", \"VOpen\", \"VHigh\", \"VLow\"]\n",
    "#\"USDHigh\", \"USDLow\", \"USDAdj Close\", \"USDOpen\", ,\"techOpen\", \"techAdj Close\"  \"techLow\",\"techHigh\", \n",
    "#additional_predictors = [\"Close\", \"Volume\", \"Open\", \"High\", \"Low\"] #[]#\n",
    "new_predictors.extend(additional_predictors)\n",
    "\n",
    "def predict(train, test, predictors, model):\n",
    "        model.fit(train[predictors], train[\"Target\"])\n",
    "        preds = model.predict_proba(test[predictors])[:,1]\n",
    "        preds[preds >=dt] = 1\n",
    "        preds[preds <dt] = 0\n",
    "        preds = pd.Series(preds, index=test.index, name=\"Predictions\")\n",
    "        combined = pd.concat([test[\"Target\"], preds], axis=1)\n",
    "        return combined\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "#\n",
    "for i in range (iterations):\n",
    "    predictionchange = []\n",
    "    weightedpredictionchange = []\n",
    "    weightedbmchange = []\n",
    "    model = RandomForestClassifier(n_estimators=number_of_estimators, max_features=maxfeatures, min_samples_split=minimum_of_samples_split, random_state=i)\n",
    "    predictions = backtest(sp500, model, new_predictors)\n",
    "    #display(f\"Iteration {i + 1}:\")\n",
    "    #display(predictions)\n",
    "    #display(predictions[\"Predictions\"].value_counts())\n",
    "    #display(predictions[\"Target\"].value_counts() / predictions.shape[0])\n",
    "    #display(np.round(precision_score(predictions[\"Target\"], predictions[\"Predictions\"]), decimals=4))\n",
    "    results.append(precision_score(predictions[\"Target\"], predictions[\"Predictions\"]))\n",
    "    \n",
    "    \n",
    "    #predictions already happened what happens after is obtaining data\n",
    "    #newdata\n",
    "    spy = yf.Ticker(theticker)\n",
    "    spy = spy.history(period=\"max\")\n",
    "    #\n",
    "    if not isinstance(spy.index, pd.DatetimeIndex):\n",
    "        spy.index = pd.to_datetime(spy.index)\n",
    "        #\n",
    "    spy.index = spy.index.normalize()\n",
    "    del spy[\"Dividends\"]\n",
    "    del spy[\"Stock Splits\"]\n",
    "    del spy[\"Volume\"]\n",
    "    del spy[\"High\"]\n",
    "    del spy[\"Low\"]\n",
    "    #### what time for change\n",
    "    spy[\"Change\"] = (spy[\"Close\"] - spy[\"Open\"]).shift(-1)\n",
    "    #####\n",
    "    spy['Open']\n",
    "    del spy['Close']\n",
    "    spy.index = spy.index.date\n",
    "    spy = spy.drop(spy.index[1])\n",
    "    display(spy)\n",
    "    total_sum = predictions[\"Predictions\"].value_counts().sum()\n",
    "    preddays11 = []\n",
    "    for idx, value in predictions[\"Predictions\"].items():\n",
    "        if value != 0:\n",
    "            preddays11.append(idx)\n",
    "    predictiondayslists11.append(preddays11)\n",
    "    preddays11 = []\n",
    "    display(total_sum)\n",
    "    spy = spy.iloc[-total_sum:]\n",
    "    predictionfrequency.append(predictions[\"Predictions\"].sum() / len(spy))\n",
    "    \n",
    "    predictions = predictions.iloc[-total_sum:]    #???total_sum\n",
    "    change_column = spy['Change'].tolist()\n",
    "    spy_column = spy['Open'].tolist()\n",
    "    #display(change_column)\n",
    "    predictions['Change'] = change_column\n",
    "    predictions['Weight'] = spy_column\n",
    "    display(predictions)\n",
    "    combination = predictions\n",
    "    plt.figure(figsize=(10, 6))  # Adjust the figure size as needed\n",
    "    plt.plot(predictions.index, predictions['Predictions'], marker='o', linestyle='-')\n",
    "    plt.title('Predictions Over Time')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Predictions')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    #display(spy)\n",
    "    def new_row(row):\n",
    "        if row['Predictions'] == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return row['Change']\n",
    "    predictions['UpdatedChange'] = predictions.apply(new_row, axis=1)\n",
    "    predictions['WeightedChange'] = predictions['UpdatedChange'] / predictions['Weight']\n",
    "    display(predictions)\n",
    "    last_200_rows = predictions.tail(400)\n",
    "    #display(last_200_rows)\n",
    "    for value in predictions['UpdatedChange']:\n",
    "        if value != 0:\n",
    "            predictionchange.append(value)    \n",
    "    avgpredictionchange.append(np.mean(predictionchange))\n",
    "    avgpredictionchangesd.append(np.std(predictionchange))\n",
    "    for value in predictions['WeightedChange']:\n",
    "        if value != 0:\n",
    "            weightedpredictionchange.append(value)    \n",
    "    avgweightedpredictionchange.append(np.mean(weightedpredictionchange))\n",
    "    avgweightedpredictionchangesd.append(np.std(weightedpredictionchange))\n",
    "    ###benchmark stuff\n",
    "    display(combination)\n",
    "    combination[\"BMChange\"] = benchmarkdata[\"BMChange\"]\n",
    "    for idx, value in predictions[\"UpdatedChange\"].items():\n",
    "        if value != 0:\n",
    "            weightedbmchange.append(predictions[\"BMChange\"].loc[idx])\n",
    "        else:\n",
    "            predictions.at[idx, \"BMChange\"] = 0\n",
    "    # Filter out the non-zero elements\n",
    "    combination[\"BMChange\"] = combination[\"WeightedChange\"] - combination[\"BMChange\"]\n",
    "    weightedbmchange = combination[\"BMChange\"][combination[\"BMChange\"] != 0]\n",
    "\n",
    "# Convert the filtered series to a list\n",
    "    weightedbmchange = weightedbmchange.tolist()\n",
    "\n",
    "    avgweightedbenchmarkchange.append(np.mean(weightedbmchange))\n",
    "    avgweightedbenchmarkchangesd.append(np.std(weightedbmchange))\n",
    "    display(combination)\n",
    "\n",
    "\n",
    "print(type(combination))\n",
    "print(type(benchmarkdata))\n",
    "print(benchmarkdata)\n",
    "\n",
    "display(results)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "display(np.round((elapsed_time / 60), decimals=2))\n",
    "display(np.round(((elapsed_time / 60) / iterations), decimals=2))\n",
    "\n",
    "display(np.round(np.mean(avgpredictionchange), decimals=2))\n",
    "display(np.round(np.mean(avgpredictionchangesd), decimals=4))\n",
    "display(np.round(np.mean(avgweightedpredictionchange), decimals=6))\n",
    "display(np.round(np.mean(avgweightedpredictionchangesd), decimals=6))\n",
    "\n",
    "display(np.round(np.mean(predictionfrequency), decimals=4))\n",
    "display(np.round(np.mean(results), decimals=4))\n",
    "display(np.round(np.std(results), decimals=4))\n",
    "\n",
    "dates_when_predictions_is_1 = predictions.index[predictions['Predictions'] == 1].tolist()\n",
    "\n",
    "#print(f\"All dates when 'predictions' is equal to 1: {dates_when_predictions_is_1}\")\n",
    "\n",
    "\n",
    "# Load the CSV file\n",
    "filename = \"/Users/derek/Downloads/DT AI Transformed Data.csv\"\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "# Define your 24 variables (ordered list of values to add)\n",
    "new_data = [\n",
    "    notebook_name, number_of_estimators, startdateunformatted, dt, VIXvar, minimum_of_samples_split,\n",
    "    maxfeatures, GSPCvar, shrtcomment, iterations, NYICDXQQQvar, XLKvar,\n",
    "    startnumber, stepnumber, theticker, np.round(np.mean(results), decimals=4), np.round(np.std(results), decimals=4), np.round(((elapsed_time / 60) / iterations), decimals=2),\n",
    "    np.round((elapsed_time / 60), decimals=2), np.round(np.mean(predictionfrequency), decimals = 4), np.round(np.mean(avgpredictionchange), decimals=2), np.round(np.mean(avgpredictionchangesd), decimals=4), np.round(np.mean(avgweightedpredictionchange), decimals=6), np.round(np.mean(avgweightedpredictionchangesd), decimals=6), np.round(np.mean(avgweightedbenchmarkchange), decimals = 6), np.round(np.mean(avgweightedbenchmarkchangesd), decimals=6), longcomment\n",
    "]\n",
    "\n",
    "# Find the first empty row\n",
    "# Here we assume that the row is considered empty if all columns are NaN or empty strings\n",
    "empty_row_index = df.index[df.isnull().all(axis=1) | (df == '').all(axis=1)].tolist()\n",
    "if empty_row_index:\n",
    "    # If there are empty rows, use the first one\n",
    "    first_empty_row = empty_row_index[0]\n",
    "else:\n",
    "    # If no empty rows, append a new row at the end\n",
    "    first_empty_row = len(df)\n",
    "\n",
    "# Ensure the length of new_data matches the number of columns\n",
    "if len(new_data) != len(df.columns):\n",
    "    raise ValueError(\"Number of new data values does not match number of columns.\")\n",
    "\n",
    "# Add the new data to the determined row\n",
    "df.loc[first_empty_row] = new_data\n",
    "\n",
    "# Save the updated DataFrame back to the CSV file\n",
    "df.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Data successfully added to row {first_empty_row + 1} of the CSV file.\")\n",
    "\n",
    "print(f\"Number of columns in DataFrame: {len(df.columns)}\")\n",
    "print(f\"Number of values in new_data: {len(new_data)}\")\n",
    "\n",
    "print(benchmarkdata)\n",
    "\n",
    "print(np.mean(avgweightedbenchmarkchange))\n",
    "print(np.mean(avgweightedbenchmarkchangesd)) \n",
    "\n",
    "\n",
    "\n",
    "def calculate_conditional_probabilities(*lists_of_sublists):\n",
    "    \"\"\"\n",
    "    Takes multiple lists of sublists and computes a conditional probability matrix.\n",
    "    The matrix will return the conditional probability of elements from each sublist in one list \n",
    "    being present in the corresponding sublists of other lists.\n",
    "    \n",
    "    Args:\n",
    "        *lists_of_sublists: Variable number of lists containing sublists.\n",
    "    \n",
    "    Returns:\n",
    "        cond_prob_matrix: A 3D numpy array where each element at (i, j, k) is the conditional \n",
    "        probability of elements from sublist `i` of `lists_of_sublists[j]` being present in sublist `i` of `lists_of_sublists[k]`.\n",
    "    \"\"\"\n",
    "    num_lists = len(lists_of_sublists)\n",
    "    \n",
    "    # Determine the maximum number of sublists across all lists\n",
    "    num_sublists = max(len(lst) for lst in lists_of_sublists)\n",
    "\n",
    "    # Initialize a 3D matrix to store conditional probabilities\n",
    "    cond_prob_matrix = np.zeros((num_sublists, num_lists, num_lists))\n",
    "\n",
    "    # Loop over each sublist index\n",
    "    for i in range(num_sublists):\n",
    "        # Compare sublist `i` from each list with sublist `i` from every other list\n",
    "        for j in range(num_lists):\n",
    "            # Ensure that the list has enough sublists before accessing it\n",
    "            if i < len(lists_of_sublists[j]):\n",
    "                set_j = set(lists_of_sublists[j][i])  # Convert to set for fast comparison\n",
    "            else:\n",
    "                set_j = set()  # If no sublist at index `i`, treat as empty\n",
    "            \n",
    "            for k in range(num_lists):\n",
    "                if i < len(lists_of_sublists[k]):\n",
    "                    set_k = set(lists_of_sublists[k][i])  # Convert to set for fast comparison\n",
    "                else:\n",
    "                    set_k = set()  # If no sublist at index `i`, treat as empty\n",
    "                \n",
    "                # Calculate conditional probability: P(set_j | set_k)\n",
    "                if len(set_j) == 0:\n",
    "                    cond_prob_matrix[i, j, k] = 0\n",
    "                else:\n",
    "                    intersection = set_j.intersection(set_k)\n",
    "                    cond_prob_matrix[i, j, k] = len(intersection) / len(set_j)\n",
    "    \n",
    "    return cond_prob_matrix\n",
    "\n",
    "# Example usage:\n",
    "# List of sublists for n = 3 (with some sublists empty or missing)\n",
    "\n",
    "\n",
    "# Calculate the conditional probability matrix\n",
    "cond_prob_matrix = calculate_conditional_probabilities(predictiondayslists1, predictiondayslists2, predictiondayslists3, predictiondayslists4, predictiondayslists5, predictiondayslists6, predictiondayslists7, predictiondayslists8, predictiondayslists9, predictiondayslists10, predictiondayslists11)\n",
    "\n",
    "print(\"Conditional Probability Matrix:\")\n",
    "print(cond_prob_matrix)\n",
    "\n",
    "cond_prob_matrix\n",
    "\n",
    "mean_matrix = np.mean(cond_prob_matrix, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=8, suppress=True)\n",
    "mean_matrix\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
