{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d361c43e-9516-4877-8adc-2cab5aff2891",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import time\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#manually set data\n",
    "notebook_name = \"Decision Tree Testing Consistency Open Close 3 past tech, finance.ipynb\"\n",
    "VIXMSUSFNvar = ''\n",
    "maxfeatures = 5 #'sqrt'\n",
    "GSPCvar = 'all - og'\n",
    "#all predictors - the original\n",
    "shrtcomment = 'ConsumeOC'\n",
    "NYICDXQQQvar = ''\n",
    "XLKvar = ''\n",
    "benchmark = '^GSPC'\n",
    "theticker = 'XLY' #^GSPC\n",
    "predictionsnum = 'N/A'\n",
    "longcomment = ''\n",
    "\n",
    "#settings\n",
    "dt = 0.6\n",
    "iterations = 10\n",
    "number_of_estimators = 1600\n",
    "minimum_of_samples_split = 150\n",
    "startnumber = 3000\n",
    "stepnumber = 220\n",
    "startdateunformatted = '1950-01-01'\n",
    "\n",
    "#prep\n",
    "starting_time = pd.to_datetime(startdateunformatted).date()\n",
    "today = datetime.now()\n",
    "tomorrow = today + timedelta(days=1)\n",
    "tomorrow_str = tomorrow.strftime('%Y-%m-%d')\n",
    "ending_time = tomorrow_str\n",
    "tomorrow_str\n",
    "results = []\n",
    "predictionfrequency = []\n",
    "avgpredictionchange = []\n",
    "avgpredictionchangesd = []\n",
    "avgweightedpredictionchange = []\n",
    "avgweightedpredictionchangesd = []\n",
    "avgweightedbenchmarkchange = []\n",
    "avgweightedbenchmarkchangesd = []\n",
    "pd.set_option('display.max_rows', 400)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "#data\n",
    "# Fetch S&P 500 data\n",
    "sp500 = yf.Ticker(theticker)\n",
    "sp500 = sp500.history(period=\"max\")\n",
    "\n",
    "# Ensure the index is a DatetimeIndex\n",
    "if not isinstance(sp500.index, pd.DatetimeIndex):\n",
    "    sp500.index = pd.to_datetime(sp500.index)\n",
    "\n",
    "# Normalize the dates (remove the time component)\n",
    "sp500.index = sp500.index.normalize()\n",
    "\n",
    "# Drop columns if needed\n",
    "if \"Dividends\" in sp500.columns:\n",
    "    del sp500[\"Dividends\"]\n",
    "if \"Stock Splits\" in sp500.columns:\n",
    "    del sp500[\"Stock Splits\"]\n",
    "sp500.index = sp500.index.date\n",
    "display(sp500)\n",
    "sp500[\"Tomorrow\"] = sp500[\"Close\"].shift(-1)\n",
    "######what change were predicting\n",
    "sp500[\"Target\"] = (sp500[\"Close\"] > sp500[\"Open\"]).astype(int).shift(-1)\n",
    "#####\n",
    "sp500 = sp500.loc[starting_time:].copy()\n",
    "display(sp500)\n",
    "volatility = yf.download(tickers = '^VIX', start = starting_time, end = ending_time)\n",
    "volatility = volatility[['Open', 'Adj Close', 'High', 'Low']]\n",
    "volatility = volatility.rename(columns={'Open': 'VOpen', 'Adj Close': 'VAdj Close', 'High': 'VHigh', 'Low': 'VLow'})\n",
    "volatility.index = volatility.index.normalize()\n",
    "volatility.index = volatility.index.date\n",
    "sp500 = pd.concat([sp500, volatility], axis=1)\n",
    "USD = yf.download(tickers = '^NYICDX', start = starting_time, end = ending_time)\n",
    "USD = USD[['Open', 'Adj Close', 'High', 'Low']]\n",
    "USD = USD.rename(columns={'Open': 'USDOpen', 'Adj Close': 'USDAdj Close', 'High': 'USDHigh', 'Low': 'USDLow'})\n",
    "USD.index = USD.index.normalize()\n",
    "USD.index = USD.index.date\n",
    "sp500 = pd.concat([sp500, USD], axis=1)\n",
    "#QQQ = yf.download(tickers = 'QQQ', start = starting_time, end = ending_time)\n",
    "#QQQ = QQQ[['Open', 'Adj Close', 'High', 'Low']]\n",
    "#QQQ = QQQ.rename(columns={'Open': 'QQQOpen', 'Adj Close': 'QQQAdj Close', 'High': 'QQQHigh', 'Low': 'QQQLow'})\n",
    "#QQQ.index = QQQ.index.normalize()\n",
    "#QQQ.index = QQQ.index.date\n",
    "#sp500 = pd.concat([sp500, QQQ], axis=1)\n",
    "tech = yf.download(tickers = 'XLK', start = starting_time, end = ending_time)\n",
    "tech = tech[['Open', 'Adj Close', 'High', 'Low']]\n",
    "tech = tech.rename(columns={'Open': 'techOpen', 'Adj Close': 'techAdj Close', 'High': 'techHigh', 'Low': 'techLow'})\n",
    "tech.index = tech.index.normalize()\n",
    "tech.index = tech.index.date\n",
    "sp500 = pd.concat([sp500, tech], axis=1)\n",
    "Consume = yf.download(tickers = '^SP500-25', start = starting_time, end = ending_time)\n",
    "Consume = Consume[['Open', 'Adj Close', 'High', 'Low']]\n",
    "Consume = Consume.rename(columns={'Open': 'ConsumeOpen', 'Adj Close': 'ConsumeAdj Close', 'High': 'ConsumeHigh', 'Low': 'ConsumeLow'})\n",
    "Consume.index = Consume.index.normalize()\n",
    "Consume.index = Consume.index.date\n",
    "sp500 = pd.concat([sp500, Consume], axis=1)\n",
    "msusfn = yf.download(tickers = 'IXF', start = starting_time, end = ending_time)\n",
    "msusfn = msusfn[['Open', 'Adj Close', 'High', 'Low']]\n",
    "msusfn = msusfn.rename(columns={'Open': 'msusfnOpen', 'Adj Close': 'msusfnAdj Close', 'High': 'msusfnHigh', 'Low': 'msusfnLow'})\n",
    "msusfn.index = msusfn.index.normalize()\n",
    "msusfn.index = msusfn.index.date\n",
    "sp500 = pd.concat([sp500, msusfn], axis=1)\n",
    "new_row = pd.DataFrame([[0] * len(sp500.columns)], columns=sp500.columns, index=[pd.to_datetime(tomorrow_str)])\n",
    "new_row.index = new_row.index.normalize()\n",
    "new_row.index = new_row.index.date\n",
    "sp500 = pd.concat([sp500, new_row])\n",
    "train = sp500.iloc[:-100]\n",
    "test = sp500.iloc[-100:]\n",
    "\n",
    "#old\n",
    "predictors = [\"Close\", \"Volume\", \"Open\", \"High\"]\n",
    "#, \"Low\", \"VOpen\", \"VAdj Close\", \"VHigh\", \"VLow\"\n",
    "#predictors = [\"Close\", \"Volume\", \"Open\", \"High\", \"Low\"]\n",
    "\n",
    "#The backtest function evaluates a model by training it on historical data \n",
    "#and testing it in increments. It iterates over the data, training the model\n",
    "#on a growing set of past data and testing it on the next segment. \n",
    "#The results are collected and combined into a single DataFrame for analysis.\n",
    "def backtest(data, model, predictors, start=startnumber, step=stepnumber):\n",
    "    all_predictions = []\n",
    "\n",
    "    for i in range(start, data.shape[0], step):\n",
    "        train = data.iloc[0:i].copy()\n",
    "        test = data.iloc[i:(i+step)].copy()\n",
    "        predictions = predict(train, test, predictors, model)\n",
    "        all_predictions.append(predictions)\n",
    "    \n",
    "    return pd.concat(all_predictions)\n",
    "\n",
    "horizons = [2,5,60,250,1000]\n",
    "new_predictors = []\n",
    "\n",
    "for horizon in horizons:\n",
    "    rolling_averages = sp500.rolling(horizon).mean()\n",
    "    \n",
    "    ratio_column = f\"Close_Ratio_{horizon}\"\n",
    "    sp500[ratio_column] = sp500[\"Close\"] / rolling_averages[\"Close\"]\n",
    "    \n",
    "    trend_column = f\"Trend_{horizon}\"\n",
    "    sp500[trend_column] = sp500.shift(1).rolling(horizon).sum()[\"Target\"]\n",
    "    \n",
    "    new_predictors+= [ratio_column, trend_column]\n",
    "\n",
    "sp500 = sp500.dropna(subset=sp500.columns[sp500.columns != \"Tomorrow\"])\n",
    "\n",
    "additional_predictors = [\"ConsumeAdj Close\", \"ConsumeOpen\"]\n",
    "#\"msusfnOpen\", \"msusfnAdj Close\"]\n",
    "    #\"USDAdj Close\", \"USDOpen\"] #  \"techAdj Close\", \"techOpen\"]# \"VOpen\", \"VAdj Close\"] #\"VAdj Close\"]#, \"QQQOpen\", \"QQQAdj Close\"] #[\"VAdj Close\", \"VOpen\", \"VHigh\", \"VLow\"]\n",
    "#\"USDHigh\", \"USDLow\", \"USDAdj Close\", \"USDOpen\", ,  \"techLow\",\"techHigh\", \n",
    "#additional_predictors = [\"Close\", \"Volume\", \"Open\", \"High\", \"Low\"] #[]#\n",
    "new_predictors.extend(additional_predictors)\n",
    "\n",
    "def predict(train, test, predictors, model):\n",
    "        model.fit(train[predictors], train[\"Target\"])\n",
    "        preds = model.predict_proba(test[predictors])[:,1]\n",
    "        preds[preds >=dt] = 1\n",
    "        preds[preds <dt] = 0\n",
    "        preds = pd.Series(preds, index=test.index, name=\"Predictions\")\n",
    "        combined = pd.concat([test[\"Target\"], preds], axis=1)\n",
    "        return combined\n",
    "\n",
    "start_time = time.time()\n",
    "##\n",
    "benchmarkdata = yf.Ticker(benchmark)\n",
    "benchmarkdata = benchmarkdata.history(period=\"max\")\n",
    "### the way change in what time is recorded\n",
    "benchmarkdata[\"BMChange\"] = ((benchmarkdata[\"Close\"] - benchmarkdata[\"Open\"])/benchmarkdata[\"Close\"]).shift(-1)\n",
    "###\n",
    "if not isinstance(benchmarkdata.index, pd.DatetimeIndex):\n",
    "    benchmarkdata.index = pd.to_datetime(benchmarkdata.index)\n",
    "\n",
    "benchmarkdata.index = benchmarkdata.index.normalize()\n",
    "benchmarkdata.index = benchmarkdata.index.date\n",
    "benchmarkdata = benchmarkdata.drop(benchmarkdata.index[1])\n",
    "print(benchmarkdata)\n",
    "#\n",
    "for i in range (iterations):\n",
    "    predictionchange = []\n",
    "    weightedpredictionchange = []\n",
    "    weightedbmchange = []\n",
    "    model = RandomForestClassifier(n_estimators=number_of_estimators, max_features=maxfeatures, min_samples_split=minimum_of_samples_split, random_state=i)\n",
    "    predictions = backtest(sp500, model, new_predictors)\n",
    "    #display(f\"Iteration {i + 1}:\")\n",
    "    #display(predictions)\n",
    "    #display(predictions[\"Predictions\"].value_counts())\n",
    "    #display(predictions[\"Target\"].value_counts() / predictions.shape[0])\n",
    "    #display(np.round(precision_score(predictions[\"Target\"], predictions[\"Predictions\"]), decimals=4))\n",
    "    results.append(precision_score(predictions[\"Target\"], predictions[\"Predictions\"]))\n",
    "    \n",
    "    \n",
    "    #predictions already happened what happens after is obtaining data\n",
    "    #newdata\n",
    "    spy = yf.Ticker(theticker)\n",
    "    spy = spy.history(period=\"max\")\n",
    "    #\n",
    "    if not isinstance(spy.index, pd.DatetimeIndex):\n",
    "        spy.index = pd.to_datetime(spy.index)\n",
    "        #\n",
    "    spy.index = spy.index.normalize()\n",
    "    del spy[\"Dividends\"]\n",
    "    del spy[\"Stock Splits\"]\n",
    "    del spy[\"Volume\"]\n",
    "    del spy[\"High\"]\n",
    "    del spy[\"Low\"]\n",
    "    #### what time for change\n",
    "    spy[\"Change\"] = (spy[\"Close\"] - spy[\"Open\"]).shift(-1)\n",
    "    #####\n",
    "    spy['Open']\n",
    "    del spy['Close']\n",
    "    spy.index = spy.index.date\n",
    "    spy = spy.drop(spy.index[1])\n",
    "    display(spy)\n",
    "    total_sum = predictions[\"Predictions\"].value_counts().sum()\n",
    "    display(total_sum)\n",
    "    spy = spy.iloc[-total_sum:]\n",
    "    predictionfrequency.append(predictions[\"Predictions\"].sum() / len(spy))\n",
    "    \n",
    "    predictions = predictions.iloc[-total_sum:]    #???total_sum\n",
    "    change_column = spy['Change'].tolist()\n",
    "    spy_column = spy['Open'].tolist()\n",
    "    #display(change_column)\n",
    "    predictions['Change'] = change_column\n",
    "    predictions['Weight'] = spy_column\n",
    "    display(predictions)\n",
    "    combination = predictions\n",
    "    plt.figure(figsize=(10, 6))  # Adjust the figure size as needed\n",
    "    plt.plot(predictions.index, predictions['Predictions'], marker='o', linestyle='-')\n",
    "    plt.title('Predictions Over Time')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Predictions')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    #display(spy)\n",
    "    def new_row(row):\n",
    "        if row['Predictions'] == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return row['Change']\n",
    "    predictions['UpdatedChange'] = predictions.apply(new_row, axis=1)\n",
    "    predictions['WeightedChange'] = predictions['UpdatedChange'] / predictions['Weight']\n",
    "    display(predictions)\n",
    "    last_200_rows = predictions.tail(400)\n",
    "    #display(last_200_rows)\n",
    "    for value in predictions['UpdatedChange']:\n",
    "        if value != 0:\n",
    "            predictionchange.append(value)    \n",
    "    avgpredictionchange.append(np.mean(predictionchange))\n",
    "    avgpredictionchangesd.append(np.std(predictionchange))\n",
    "    for value in predictions['WeightedChange']:\n",
    "        if value != 0:\n",
    "            weightedpredictionchange.append(value)    \n",
    "    avgweightedpredictionchange.append(np.mean(weightedpredictionchange))\n",
    "    avgweightedpredictionchangesd.append(np.std(weightedpredictionchange))\n",
    "    ###benchmark stuff\n",
    "    display(combination)\n",
    "    combination[\"BMChange\"] = benchmarkdata[\"BMChange\"]\n",
    "    for idx, value in predictions[\"UpdatedChange\"].items():\n",
    "        if value != 0:\n",
    "            weightedbmchange.append(predictions[\"BMChange\"].loc[idx])\n",
    "        else:\n",
    "            predictions.at[idx, \"BMChange\"] = 0\n",
    "    # Filter out the non-zero elements\n",
    "    combination[\"BMChange\"] = combination[\"WeightedChange\"] - combination[\"BMChange\"]\n",
    "    weightedbmchange = combination[\"BMChange\"][combination[\"BMChange\"] != 0]\n",
    "\n",
    "# Convert the filtered series to a list\n",
    "    weightedbmchange = weightedbmchange.tolist()\n",
    "\n",
    "    avgweightedbenchmarkchange.append(np.mean(weightedbmchange))\n",
    "    avgweightedbenchmarkchangesd.append(np.std(weightedbmchange))\n",
    "    display(combination)\n",
    "\n",
    "\n",
    "print(type(combination))\n",
    "print(type(benchmarkdata))\n",
    "print(benchmarkdata)\n",
    "\n",
    "display(results)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "display(np.round((elapsed_time / 60), decimals=2))\n",
    "display(np.round(((elapsed_time / 60) / iterations), decimals=2))\n",
    "\n",
    "display(np.round(np.mean(avgpredictionchange), decimals=2))\n",
    "display(np.round(np.mean(avgpredictionchangesd), decimals=4))\n",
    "display(np.round(np.mean(avgweightedpredictionchange), decimals=6))\n",
    "display(np.round(np.mean(avgweightedpredictionchangesd), decimals=6))\n",
    "\n",
    "display(np.round(np.mean(predictionfrequency), decimals=4))\n",
    "display(np.round(np.mean(results), decimals=4))\n",
    "display(np.round(np.std(results), decimals=4))\n",
    "\n",
    "dates_when_predictions_is_1 = predictions.index[predictions['Predictions'] == 1].tolist()\n",
    "\n",
    "#print(f\"All dates when 'predictions' is equal to 1: {dates_when_predictions_is_1}\")\n",
    "\n",
    "\n",
    "# Load the CSV file\n",
    "filename = \"/Users/derek/Downloads/DT AI Transformed Data.csv\"\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "# Define your 24 variables (ordered list of values to add)\n",
    "new_data = [\n",
    "    notebook_name, number_of_estimators, startdateunformatted, dt, VIXMSUSFNvar, minimum_of_samples_split,\n",
    "    maxfeatures, GSPCvar, shrtcomment, iterations, NYICDXQQQvar, XLKvar,\n",
    "    startnumber, stepnumber, theticker, np.round(np.mean(results), decimals=4), np.round(np.std(results), decimals=4), np.round(((elapsed_time / 60) / iterations), decimals=2),\n",
    "    np.round((elapsed_time / 60), decimals=2), np.round(np.mean(predictionfrequency), decimals = 4), np.round(np.mean(avgpredictionchange), decimals=2), np.round(np.mean(avgpredictionchangesd), decimals=4), np.round(np.mean(avgweightedpredictionchange), decimals=6), np.round(np.mean(avgweightedpredictionchangesd), decimals=6), np.round(np.mean(avgweightedbenchmarkchange), decimals = 6), np.round(np.mean(avgweightedbenchmarkchangesd), decimals=6), longcomment\n",
    "]\n",
    "\n",
    "# Find the first empty row\n",
    "# Here we assume that the row is considered empty if all columns are NaN or empty strings\n",
    "empty_row_index = df.index[df.isnull().all(axis=1) | (df == '').all(axis=1)].tolist()\n",
    "if empty_row_index:\n",
    "    # If there are empty rows, use the first one\n",
    "    first_empty_row = empty_row_index[0]\n",
    "else:\n",
    "    # If no empty rows, append a new row at the end\n",
    "    first_empty_row = len(df)\n",
    "\n",
    "# Ensure the length of new_data matches the number of columns\n",
    "if len(new_data) != len(df.columns):\n",
    "    raise ValueError(\"Number of new data values does not match number of columns.\")\n",
    "\n",
    "# Add the new data to the determined row\n",
    "df.loc[first_empty_row] = new_data\n",
    "\n",
    "# Save the updated DataFrame back to the CSV file\n",
    "df.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Data successfully added to row {first_empty_row + 1} of the CSV file.\")\n",
    "\n",
    "print(f\"Number of columns in DataFrame: {len(df.columns)}\")\n",
    "print(f\"Number of values in new_data: {len(new_data)}\")\n",
    "\n",
    "print(benchmarkdata)\n",
    "\n",
    "print(np.mean(avgweightedbenchmarkchange))\n",
    "print(np.mean(avgweightedbenchmarkchangesd)) \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
